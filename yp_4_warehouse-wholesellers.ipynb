{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# B2B Wholesaler/Warehouse Yellow Pages Scraper\n",
    "\n",
    "**Target:** B2B businesses that could benefit from custom NextJS web apps for order management, invoicing, and inventory systems.\n",
    "\n",
    "## Features\n",
    "- 40+ search terms targeting wholesalers, distributors, warehouses, manufacturers\n",
    "- 35+ NYC locations including industrial areas and nearby NJ\n",
    "- Auto-resume capability (saves progress after each listing)\n",
    "- Built-in deduplication across all files\n",
    "- Rotating user agents to avoid detection\n",
    "- Robust error handling with retries\n",
    "\n",
    "## How to Use\n",
    "1. Set the MODE in Cell 1 configuration\n",
    "2. Run Cell 1 to start scraping\n",
    "3. Run Cell 2 to merge all files when done\n",
    "4. Run Cell 3 to export leads with emails only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import re\n",
    "import random\n",
    "import os\n",
    "import json\n",
    "import hashlib\n",
    "from datetime import datetime\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "from openpyxl import load_workbook\n",
    "from openpyxl.worksheet.datavalidation import DataValidation\n",
    "\n",
    "# ============================================================================\n",
    "#                              CONFIGURATION\n",
    "# ============================================================================\n",
    "\n",
    "# === MODE: Choose how to run ===\n",
    "# \"single\"    - Run one search term + one location (set CURRENT_SEARCH_INDEX & CURRENT_LOCATION_INDEX)\n",
    "# \"batch\"     - Run one search term across ALL locations\n",
    "# \"all\"       - Run ALL search terms across ALL locations (comprehensive but slow)\n",
    "# \"resume\"    - Resume from saved progress file\n",
    "MODE = \"single\"\n",
    "\n",
    "# === B2B WHOLESALER/WAREHOUSE SEARCH TERMS ===\n",
    "# These businesses typically need order management, invoicing, inventory software\n",
    "SEARCHES = [\n",
    "    # Core wholesale/distribution\n",
    "    {\"term\": \"wholesale\", \"label\": \"Wholesale\"},\n",
    "    {\"term\": \"wholesalers\", \"label\": \"Wholesaler\"},\n",
    "    {\"term\": \"wholesale-distributors\", \"label\": \"Wholesale Distributor\"},\n",
    "    {\"term\": \"distributors\", \"label\": \"Distributor\"},\n",
    "    {\"term\": \"distribution-services\", \"label\": \"Distribution Services\"},\n",
    "    \n",
    "    # Warehousing\n",
    "    {\"term\": \"warehouses\", \"label\": \"Warehouse\"},\n",
    "    {\"term\": \"warehouse-storage\", \"label\": \"Warehouse Storage\"},\n",
    "    {\"term\": \"warehousing\", \"label\": \"Warehousing\"},\n",
    "    {\"term\": \"public-warehouses\", \"label\": \"Public Warehouse\"},\n",
    "    {\"term\": \"cold-storage\", \"label\": \"Cold Storage\"},\n",
    "    \n",
    "    # Import/Export (often need customs & invoicing)\n",
    "    {\"term\": \"importers\", \"label\": \"Importer\"},\n",
    "    {\"term\": \"exporters\", \"label\": \"Exporter\"},\n",
    "    {\"term\": \"import-export\", \"label\": \"Import/Export\"},\n",
    "    {\"term\": \"freight-forwarding\", \"label\": \"Freight Forwarding\"},\n",
    "    {\"term\": \"customs-brokers\", \"label\": \"Customs Broker\"},\n",
    "    \n",
    "    # Manufacturing & Industrial (need order systems)\n",
    "    {\"term\": \"manufacturers\", \"label\": \"Manufacturer\"},\n",
    "    {\"term\": \"manufacturing\", \"label\": \"Manufacturing\"},\n",
    "    {\"term\": \"industrial-equipment\", \"label\": \"Industrial Equipment\"},\n",
    "    {\"term\": \"packaging-materials-equipment\", \"label\": \"Packaging\"},\n",
    "    \n",
    "    # Food/Beverage Wholesale (high volume, need invoicing)\n",
    "    {\"term\": \"food-brokers\", \"label\": \"Food Broker\"},\n",
    "    {\"term\": \"food-products-wholesale\", \"label\": \"Food Wholesale\"},\n",
    "    {\"term\": \"beverage-distributors\", \"label\": \"Beverage Distributor\"},\n",
    "    {\"term\": \"grocery-wholesale\", \"label\": \"Grocery Wholesale\"},\n",
    "    {\"term\": \"meat-wholesale\", \"label\": \"Meat Wholesale\"},\n",
    "    {\"term\": \"produce-wholesale\", \"label\": \"Produce Wholesale\"},\n",
    "    {\"term\": \"seafood-wholesale\", \"label\": \"Seafood Wholesale\"},\n",
    "    \n",
    "    # Building/Construction Supplies (B2B heavy)\n",
    "    {\"term\": \"building-materials\", \"label\": \"Building Materials\"},\n",
    "    {\"term\": \"lumber-wholesale\", \"label\": \"Lumber Wholesale\"},\n",
    "    {\"term\": \"plumbing-supplies-wholesale\", \"label\": \"Plumbing Supplies\"},\n",
    "    {\"term\": \"electrical-supplies-wholesale\", \"label\": \"Electrical Supplies\"},\n",
    "    {\"term\": \"hardware-wholesale\", \"label\": \"Hardware Wholesale\"},\n",
    "    \n",
    "    # Other B2B\n",
    "    {\"term\": \"paper-products-wholesale\", \"label\": \"Paper Products\"},\n",
    "    {\"term\": \"janitorial-supplies\", \"label\": \"Janitorial Supplies\"},\n",
    "    {\"term\": \"restaurant-equipment-supplies\", \"label\": \"Restaurant Equipment\"},\n",
    "    {\"term\": \"beauty-supplies-wholesale\", \"label\": \"Beauty Supplies\"},\n",
    "    {\"term\": \"clothing-wholesale\", \"label\": \"Clothing Wholesale\"},\n",
    "    {\"term\": \"auto-parts-wholesale\", \"label\": \"Auto Parts Wholesale\"},\n",
    "    {\"term\": \"electronics-wholesale\", \"label\": \"Electronics Wholesale\"},\n",
    "    {\"term\": \"medical-equipment-supplies\", \"label\": \"Medical Supplies\"},\n",
    "    {\"term\": \"office-supplies-wholesale\", \"label\": \"Office Supplies\"},\n",
    "    \n",
    "    # Logistics (often need custom software)\n",
    "    {\"term\": \"logistics\", \"label\": \"Logistics\"},\n",
    "    {\"term\": \"fulfillment-services\", \"label\": \"Fulfillment Services\"},\n",
    "    {\"term\": \"third-party-logistics\", \"label\": \"3PL\"},\n",
    "    {\"term\": \"supply-chain\", \"label\": \"Supply Chain\"},\n",
    "]\n",
    "\n",
    "# === NYC LOCATIONS ===\n",
    "# Comprehensive coverage of NYC boroughs and industrial/commercial areas\n",
    "LOCATIONS = [\n",
    "    # Main boroughs\n",
    "    \"queens-ny\",\n",
    "    \"brooklyn-ny\",\n",
    "    \"bronx-ny\",\n",
    "    \"manhattan-ny\",\n",
    "    \"staten-island-ny\",\n",
    "    \n",
    "    # Queens industrial/commercial areas\n",
    "    \"long-island-city-ny\",\n",
    "    \"maspeth-ny\",\n",
    "    \"jamaica-ny\",\n",
    "    \"flushing-ny\",\n",
    "    \"astoria-ny\",\n",
    "    \"woodside-ny\",\n",
    "    \"ridgewood-ny\",\n",
    "    \"college-point-ny\",\n",
    "    \"ozone-park-ny\",\n",
    "    \n",
    "    # Brooklyn industrial areas\n",
    "    \"sunset-park-brooklyn-ny\",\n",
    "    \"red-hook-brooklyn-ny\",\n",
    "    \"bushwick-brooklyn-ny\",\n",
    "    \"east-new-york-brooklyn-ny\",\n",
    "    \"greenpoint-brooklyn-ny\",\n",
    "    \"williamsburg-brooklyn-ny\",\n",
    "    \"industry-city-brooklyn-ny\",\n",
    "    \"brooklyn-navy-yard-ny\",\n",
    "    \"canarsie-brooklyn-ny\",\n",
    "    \n",
    "    # Bronx industrial areas\n",
    "    \"hunts-point-bronx-ny\",\n",
    "    \"port-morris-bronx-ny\",\n",
    "    \"mott-haven-bronx-ny\",\n",
    "    \"south-bronx-ny\",\n",
    "    \"fordham-bronx-ny\",\n",
    "    \n",
    "    # Manhattan commercial\n",
    "    \"chelsea-ny\",\n",
    "    \"tribeca-ny\",\n",
    "    \"lower-manhattan-ny\",\n",
    "    \"garment-district-ny\",\n",
    "    \"meatpacking-district-ny\",\n",
    "    \n",
    "    # Nearby NJ (many warehouses serve NYC)\n",
    "    \"jersey-city-nj\",\n",
    "    \"newark-nj\",\n",
    "    \"elizabeth-nj\",\n",
    "    \"secaucus-nj\",\n",
    "    \"kearny-nj\",\n",
    "]\n",
    "\n",
    "# === CURRENT SELECTION (for \"single\" mode) ===\n",
    "CURRENT_SEARCH_INDEX = 0      # Which search term to use\n",
    "CURRENT_LOCATION_INDEX = 0    # Which location to use\n",
    "\n",
    "# === PAGINATION ===\n",
    "START_PAGE = 1\n",
    "MAX_PAGES = 10  # Safety limit (YP usually stops at 3-4)\n",
    "\n",
    "# === OUTPUT ===\n",
    "OUTPUT_DIR = \"exports_b2b_warehouse\"\n",
    "PROGRESS_FILE = \"scrape_progress.json\"\n",
    "\n",
    "# === SCRAPING SETTINGS ===\n",
    "FETCH_EMAILS = True           # Set False for faster scraping (no emails)\n",
    "DEBUG = False                 # Save debug HTML files\n",
    "HEADLESS = False              # False = visible browser (bypasses Cloudflare better)\n",
    "MIN_DELAY = 4                 # Min seconds between requests\n",
    "MAX_DELAY = 8                 # Max seconds between requests\n",
    "PAGE_DELAY = 12               # Seconds between pages\n",
    "LISTING_DELAY = 3             # Seconds between listing detail fetches\n",
    "RESTART_DRIVER_EACH_PAGE = True  # Fresh session each page\n",
    "MAX_RETRIES = 3               # Retries on failure\n",
    "\n",
    "# === USER AGENTS (rotated to avoid detection) ===\n",
    "USER_AGENTS = [\n",
    "    \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36\",\n",
    "    \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36\",\n",
    "    \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/17.0 Safari/605.1.15\",\n",
    "    \"Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:121.0) Gecko/20100101 Firefox/121.0\",\n",
    "    \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10.15; rv:121.0) Gecko/20100101 Firefox/121.0\",\n",
    "    \"Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "#                              HELPER FUNCTIONS\n",
    "# ============================================================================\n",
    "\n",
    "def ensure_output_dir():\n",
    "    \"\"\"Create output directory if it doesn't exist\"\"\"\n",
    "    if not os.path.exists(OUTPUT_DIR):\n",
    "        os.makedirs(OUTPUT_DIR)\n",
    "        print(f\"Created output directory: {OUTPUT_DIR}\")\n",
    "\n",
    "\n",
    "def get_output_filename(search_term, location):\n",
    "    \"\"\"Generate output filename for a search/location combo\"\"\"\n",
    "    return os.path.join(OUTPUT_DIR, f\"yp_b2b_{location}_{search_term}.xlsx\")\n",
    "\n",
    "\n",
    "def generate_lead_id(company_name, phone):\n",
    "    \"\"\"Generate unique ID for deduplication\"\"\"\n",
    "    key = f\"{company_name.lower().strip()}|{phone.strip()}\"\n",
    "    return hashlib.md5(key.encode()).hexdigest()[:12]\n",
    "\n",
    "\n",
    "def load_existing_leads(filepath):\n",
    "    \"\"\"Load existing leads from Excel file for deduplication\"\"\"\n",
    "    if os.path.exists(filepath):\n",
    "        try:\n",
    "            df = pd.read_excel(filepath)\n",
    "            return set(\n",
    "                generate_lead_id(row[\"Company Name\"], row.get(\"Phone Number\", \"\"))\n",
    "                for _, row in df.iterrows()\n",
    "            )\n",
    "        except:\n",
    "            return set()\n",
    "    return set()\n",
    "\n",
    "\n",
    "def load_all_existing_lead_ids():\n",
    "    \"\"\"Load all lead IDs from all existing files for global deduplication\"\"\"\n",
    "    all_ids = set()\n",
    "    if os.path.exists(OUTPUT_DIR):\n",
    "        for filename in os.listdir(OUTPUT_DIR):\n",
    "            if filename.endswith(\".xlsx\"):\n",
    "                filepath = os.path.join(OUTPUT_DIR, filename)\n",
    "                all_ids.update(load_existing_leads(filepath))\n",
    "    return all_ids\n",
    "\n",
    "\n",
    "def save_progress(search_idx, location_idx, page, status=\"in_progress\"):\n",
    "    \"\"\"Save scraping progress for resume capability\"\"\"\n",
    "    progress = {\n",
    "        \"search_index\": search_idx,\n",
    "        \"location_index\": location_idx,\n",
    "        \"page\": page,\n",
    "        \"status\": status,\n",
    "        \"timestamp\": datetime.now().isoformat()\n",
    "    }\n",
    "    with open(PROGRESS_FILE, \"w\") as f:\n",
    "        json.dump(progress, f)\n",
    "\n",
    "\n",
    "def load_progress():\n",
    "    \"\"\"Load saved progress\"\"\"\n",
    "    if os.path.exists(PROGRESS_FILE):\n",
    "        with open(PROGRESS_FILE, \"r\") as f:\n",
    "            return json.load(f)\n",
    "    return None\n",
    "\n",
    "\n",
    "def create_driver():\n",
    "    \"\"\"Create Selenium WebDriver with anti-detection settings\"\"\"\n",
    "    options = Options()\n",
    "    \n",
    "    if HEADLESS:\n",
    "        options.add_argument(\"--headless=new\")\n",
    "    \n",
    "    options.add_argument(\"--no-sandbox\")\n",
    "    options.add_argument(\"--disable-dev-shm-usage\")\n",
    "    options.add_argument(\"--disable-gpu\")\n",
    "    options.add_argument(\"--window-size=1920,1080\")\n",
    "    options.add_argument(\"--disable-blink-features=AutomationControlled\")\n",
    "    \n",
    "    # Random user agent\n",
    "    user_agent = random.choice(USER_AGENTS)\n",
    "    options.add_argument(f\"user-agent={user_agent}\")\n",
    "    \n",
    "    options.add_experimental_option(\"excludeSwitches\", [\"enable-automation\"])\n",
    "    options.add_experimental_option('useAutomationExtension', False)\n",
    "    \n",
    "    driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()), options=options)\n",
    "    driver.execute_script(\"Object.defineProperty(navigator, 'webdriver', {get: () => undefined})\")\n",
    "    \n",
    "    return driver\n",
    "\n",
    "\n",
    "def random_delay(min_sec=None, max_sec=None):\n",
    "    \"\"\"Human-like random delay\"\"\"\n",
    "    min_sec = min_sec or MIN_DELAY\n",
    "    max_sec = max_sec or MAX_DELAY\n",
    "    delay = random.uniform(min_sec, max_sec)\n",
    "    time.sleep(delay)\n",
    "    return delay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "#                           EMAIL EXTRACTION\n",
    "# ============================================================================\n",
    "\n",
    "# Email false positive filter\n",
    "EMAIL_BLACKLIST = [\n",
    "    'example.com', 'domain.com', 'email.com', 'yoursite', 'yourdomain',\n",
    "    'sentry.io', 'schema.org', 'json', 'wixpress', 'wix.com',\n",
    "    'googleapis', 'google.com', 'facebook', 'twitter', 'instagram',\n",
    "    '.png', '.jpg', '.gif', '.svg', '.css', '.js',\n",
    "    'yellowpages', 'yp.com', 'placeholder', 'test.com',\n",
    "    'wordpress', 'squarespace', 'shopify', 'godaddy'\n",
    "]\n",
    "\n",
    "\n",
    "def is_valid_email(email):\n",
    "    \"\"\"Check if email is likely valid (not a false positive)\"\"\"\n",
    "    if not email or '@' not in email:\n",
    "        return False\n",
    "    email_lower = email.lower()\n",
    "    return not any(x in email_lower for x in EMAIL_BLACKLIST)\n",
    "\n",
    "\n",
    "def extract_email_from_website(driver, website_url, timeout=15):\n",
    "    \"\"\"Extract email from company's own website\"\"\"\n",
    "    if not website_url or website_url == \"N/A\":\n",
    "        return \"\"\n",
    "    \n",
    "    try:\n",
    "        # Clean up URL\n",
    "        if not website_url.startswith(\"http\"):\n",
    "            website_url = \"https://\" + website_url\n",
    "        \n",
    "        random_delay(1, 2)\n",
    "        driver.set_page_load_timeout(timeout)\n",
    "        \n",
    "        try:\n",
    "            driver.get(website_url)\n",
    "        except:\n",
    "            return \"\"\n",
    "        \n",
    "        time.sleep(2)\n",
    "        page_source = driver.page_source\n",
    "        \n",
    "        # Check for error pages\n",
    "        if any(x in driver.title.lower() for x in [\"404\", \"not found\", \"error\", \"denied\"]):\n",
    "            return \"\"\n",
    "        \n",
    "        # Method 1: Find mailto links\n",
    "        mailto_match = re.search(r'href=[\"\\']mailto:([^\"\\'<>?\\s]+)', page_source, re.IGNORECASE)\n",
    "        if mailto_match:\n",
    "            email = mailto_match.group(1).strip()\n",
    "            if is_valid_email(email):\n",
    "                return email\n",
    "        \n",
    "        # Method 2: Find email patterns in page\n",
    "        email_matches = re.findall(r'[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\\.[a-zA-Z]{2,}', page_source)\n",
    "        for email in email_matches:\n",
    "            if is_valid_email(email):\n",
    "                return email\n",
    "        \n",
    "        # Method 3: Try contact pages\n",
    "        base_url = website_url.rstrip('/')\n",
    "        contact_paths = ['/contact', '/contact-us', '/about', '/about-us', '/contactus']\n",
    "        \n",
    "        for path in contact_paths:\n",
    "            try:\n",
    "                driver.get(base_url + path)\n",
    "                time.sleep(1.5)\n",
    "                contact_source = driver.page_source\n",
    "                \n",
    "                mailto_match = re.search(r'href=[\"\\']mailto:([^\"\\'<>?\\s]+)', contact_source, re.IGNORECASE)\n",
    "                if mailto_match:\n",
    "                    email = mailto_match.group(1).strip()\n",
    "                    if is_valid_email(email):\n",
    "                        return email\n",
    "                \n",
    "                email_matches = re.findall(r'[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\\.[a-zA-Z]{2,}', contact_source)\n",
    "                for email in email_matches:\n",
    "                    if is_valid_email(email):\n",
    "                        return email\n",
    "            except:\n",
    "                continue\n",
    "    \n",
    "    except Exception as e:\n",
    "        if DEBUG:\n",
    "            print(f\" [website error: {e}]\", end=\"\")\n",
    "    \n",
    "    return \"\"\n",
    "\n",
    "\n",
    "def extract_email_from_detail(driver, detail_url, website_url=\"\", debug_save=False):\n",
    "    \"\"\"Extract email from Yellow Pages detail page, fallback to company website\"\"\"\n",
    "    try:\n",
    "        random_delay(LISTING_DELAY, LISTING_DELAY + 2)\n",
    "        driver.get(detail_url)\n",
    "        \n",
    "        # Wait for page load\n",
    "        try:\n",
    "            WebDriverWait(driver, 10).until(\n",
    "                EC.presence_of_element_located((By.CSS_SELECTOR, \".business-info, .sales-info, #main-content, #cf-wrapper\"))\n",
    "            )\n",
    "        except:\n",
    "            pass\n",
    "        \n",
    "        time.sleep(2)\n",
    "        page_source = driver.page_source\n",
    "        \n",
    "        # Check for Cloudflare block\n",
    "        is_blocked = (\n",
    "            \"you have been blocked\" in page_source.lower() or\n",
    "            (\"cloudflare\" in page_source.lower() and \"ray id\" in page_source.lower())\n",
    "        )\n",
    "        \n",
    "        if is_blocked:\n",
    "            if website_url:\n",
    "                print(\" [YP blocked, trying website]\", end=\"\")\n",
    "                email = extract_email_from_website(driver, website_url)\n",
    "                if email:\n",
    "                    return email\n",
    "            return \"__BLOCKED__\"\n",
    "        \n",
    "        # Debug save\n",
    "        if debug_save:\n",
    "            with open(os.path.join(OUTPUT_DIR, \"debug_page_source.html\"), \"w\", encoding=\"utf-8\") as f:\n",
    "                f.write(page_source)\n",
    "        \n",
    "        # Scroll to load lazy content\n",
    "        driver.execute_script(\"window.scrollTo(0, 800);\")\n",
    "        time.sleep(1)\n",
    "        page_source = driver.page_source\n",
    "        \n",
    "        # Method 1: Mailto links in page source\n",
    "        mailto_match = re.search(r'href=[\"\\']mailto:([^\"\\'<>?\\s]+)', page_source, re.IGNORECASE)\n",
    "        if mailto_match:\n",
    "            email = mailto_match.group(1).strip()\n",
    "            if is_valid_email(email):\n",
    "                return email\n",
    "        \n",
    "        # Method 2: Selenium - email-business link\n",
    "        try:\n",
    "            email_elements = driver.find_elements(By.CSS_SELECTOR, \"a.email-business, a[class*='email']\")\n",
    "            for el in email_elements:\n",
    "                href = el.get_attribute(\"href\") or \"\"\n",
    "                if \"mailto:\" in href:\n",
    "                    email = href.replace(\"mailto:\", \"\").split(\"?\")[0].strip()\n",
    "                    if is_valid_email(email):\n",
    "                        return email\n",
    "        except:\n",
    "            pass\n",
    "        \n",
    "        # Method 3: Any mailto link\n",
    "        try:\n",
    "            mailto_links = driver.find_elements(By.CSS_SELECTOR, \"a[href*='mailto:']\")\n",
    "            for link in mailto_links:\n",
    "                href = link.get_attribute(\"href\") or \"\"\n",
    "                if \"mailto:\" in href:\n",
    "                    email = href.replace(\"mailto:\", \"\").split(\"?\")[0].strip()\n",
    "                    if is_valid_email(email):\n",
    "                        return email\n",
    "        except:\n",
    "            pass\n",
    "        \n",
    "        # Method 4: BeautifulSoup\n",
    "        soup = BeautifulSoup(page_source, \"html.parser\")\n",
    "        for link in soup.find_all(\"a\", href=True):\n",
    "            href = link[\"href\"]\n",
    "            if \"mailto:\" in href:\n",
    "                email = href.replace(\"mailto:\", \"\").split(\"?\")[0].strip()\n",
    "                if is_valid_email(email):\n",
    "                    return email\n",
    "        \n",
    "        # Method 5: Regex search\n",
    "        email_matches = re.findall(r'[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\\.[a-zA-Z]{2,}', page_source)\n",
    "        for email in email_matches:\n",
    "            if is_valid_email(email):\n",
    "                return email\n",
    "        \n",
    "        # Fallback: Try company website\n",
    "        if website_url:\n",
    "            print(\" [trying website]\", end=\"\")\n",
    "            email = extract_email_from_website(driver, website_url)\n",
    "            if email:\n",
    "                return email\n",
    "    \n",
    "    except Exception as e:\n",
    "        if DEBUG:\n",
    "            print(f\" [error: {e}]\", end=\"\")\n",
    "    \n",
    "    return \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "#                           LISTING PARSING\n",
    "# ============================================================================\n",
    "\n",
    "def parse_listing(listing, industry_label):\n",
    "    \"\"\"Parse a single listing into a lead dict\"\"\"\n",
    "    try:\n",
    "        # Company name\n",
    "        name_el = listing.select_one(\".business-name span\")\n",
    "        if not name_el:\n",
    "            name_el = listing.select_one(\".business-name\")\n",
    "        company = name_el.text.strip() if name_el else \"\"\n",
    "        \n",
    "        if not company:\n",
    "            return None\n",
    "        \n",
    "        # Phone\n",
    "        phone_el = listing.select_one(\".phones\")\n",
    "        phone = phone_el.text.strip() if phone_el else \"\"\n",
    "        \n",
    "        # Address\n",
    "        street = listing.select_one(\".street-address\")\n",
    "        locality = listing.select_one(\".locality\")\n",
    "        address = \" \".join(filter(None, [\n",
    "            street.text.strip() if street else \"\",\n",
    "            locality.text.strip() if locality else \"\"\n",
    "        ]))\n",
    "        \n",
    "        # Website\n",
    "        website_el = listing.select_one(\".track-visit-website\")\n",
    "        website = website_el[\"href\"] if website_el else \"\"\n",
    "        \n",
    "        # Detail link\n",
    "        detail_el = listing.select_one(\".business-name\")\n",
    "        detail_link = \"\"\n",
    "        if detail_el and detail_el.get(\"href\"):\n",
    "            detail_link = \"https://www.yellowpages.com\" + detail_el[\"href\"]\n",
    "        \n",
    "        # Categories/services (useful context)\n",
    "        categories_el = listing.select_one(\".categories\")\n",
    "        categories = categories_el.text.strip() if categories_el else \"\"\n",
    "        \n",
    "        return {\n",
    "            \"#\": None,\n",
    "            \"Company Name\": company,\n",
    "            \"Industry\": industry_label,\n",
    "            \"Category\": categories,\n",
    "            \"Contact Name\": \"\",\n",
    "            \"Email Address\": \"\",\n",
    "            \"Phone Number\": phone,\n",
    "            \"Website URL\": website,\n",
    "            \"Address\": address,\n",
    "            \"Date Added\": datetime.now().strftime(\"%-m/%-d/%y\"),\n",
    "            \"Date Contacted\": \"\",\n",
    "            \"Source\": detail_link,\n",
    "            \"Notes\": \"\",\n",
    "            \"Called\": \"\",\n",
    "            \"Followed Up\": \"\",\n",
    "            \"Closed\": \"\",\n",
    "            \"_lead_id\": generate_lead_id(company, phone)\n",
    "        }\n",
    "    except Exception as e:\n",
    "        if DEBUG:\n",
    "            print(f\"  Parse error: {e}\")\n",
    "        return None\n",
    "\n",
    "\n",
    "def get_listings_from_page(driver, industry_label):\n",
    "    \"\"\"Extract all listings from current search results page\"\"\"\n",
    "    # Scroll to load all content\n",
    "    driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "    time.sleep(2)\n",
    "    \n",
    "    try:\n",
    "        WebDriverWait(driver, 10).until(\n",
    "            EC.presence_of_element_located((By.CSS_SELECTOR, \".result, .search-results\"))\n",
    "        )\n",
    "    except:\n",
    "        return []\n",
    "    \n",
    "    listings = driver.find_elements(By.CSS_SELECTOR, \".result\")\n",
    "    page_data = []\n",
    "    \n",
    "    for listing in listings:\n",
    "        try:\n",
    "            html = listing.get_attribute(\"outerHTML\")\n",
    "            soup = BeautifulSoup(html, \"html.parser\")\n",
    "            parsed = parse_listing(soup, industry_label)\n",
    "            if parsed:\n",
    "                page_data.append(parsed)\n",
    "        except:\n",
    "            continue\n",
    "    \n",
    "    return page_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "#                           EXCEL OUTPUT\n",
    "# ============================================================================\n",
    "\n",
    "def add_checkboxes(filepath):\n",
    "    \"\"\"Add checkbox dropdowns to tracking columns\"\"\"\n",
    "    try:\n",
    "        wb = load_workbook(filepath)\n",
    "        ws = wb.active\n",
    "        \n",
    "        checkbox_validation = DataValidation(type=\"list\", formula1='\"☐,☑\"', allow_blank=True)\n",
    "        ws.add_data_validation(checkbox_validation)\n",
    "        \n",
    "        headers = {cell.value: cell.column for cell in ws[1]}\n",
    "        \n",
    "        for col_name in [\"Called\", \"Followed Up\", \"Closed\"]:\n",
    "            if col_name in headers:\n",
    "                col_idx = headers[col_name]\n",
    "                for row in range(2, ws.max_row + 1):\n",
    "                    cell = ws.cell(row=row, column=col_idx)\n",
    "                    if not cell.value:\n",
    "                        cell.value = \"☐\"\n",
    "                    checkbox_validation.add(cell)\n",
    "        \n",
    "        wb.save(filepath)\n",
    "    except Exception as e:\n",
    "        print(f\"  Warning: Could not add checkboxes: {e}\")\n",
    "\n",
    "\n",
    "def save_leads_to_excel(leads, filepath):\n",
    "    \"\"\"Save leads to Excel with proper formatting\"\"\"\n",
    "    if not leads:\n",
    "        return\n",
    "    \n",
    "    # Remove internal _lead_id column for output\n",
    "    clean_leads = []\n",
    "    for lead in leads:\n",
    "        clean_lead = {k: v for k, v in lead.items() if not k.startswith(\"_\")}\n",
    "        clean_leads.append(clean_lead)\n",
    "    \n",
    "    # Renumber\n",
    "    for i, lead in enumerate(clean_leads, 1):\n",
    "        lead[\"#\"] = i\n",
    "    \n",
    "    df = pd.DataFrame(clean_leads)\n",
    "    df.to_excel(filepath, index=False)\n",
    "    add_checkboxes(filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "#                           MAIN SCRAPER\n",
    "# ============================================================================\n",
    "\n",
    "def scrape_search(search_term, search_label, location, existing_ids=None):\n",
    "    \"\"\"Scrape a single search term + location combination\"\"\"\n",
    "    existing_ids = existing_ids or set()\n",
    "    base_url = f\"https://www.yellowpages.com/{location}/{search_term}\"\n",
    "    output_file = get_output_filename(search_term, location)\n",
    "    \n",
    "    # Load any existing leads for this file\n",
    "    existing_file_leads = []\n",
    "    if os.path.exists(output_file):\n",
    "        try:\n",
    "            df = pd.read_excel(output_file)\n",
    "            existing_file_leads = df.to_dict('records')\n",
    "            for lead in existing_file_leads:\n",
    "                lead[\"_lead_id\"] = generate_lead_id(\n",
    "                    lead.get(\"Company Name\", \"\"),\n",
    "                    lead.get(\"Phone Number\", \"\")\n",
    "                )\n",
    "        except:\n",
    "            pass\n",
    "    \n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"SCRAPING: {search_label}\")\n",
    "    print(f\"Location: {location}\")\n",
    "    print(f\"URL: {base_url}\")\n",
    "    print(f\"Output: {output_file}\")\n",
    "    print(f\"Existing leads in file: {len(existing_file_leads)}\")\n",
    "    print(f\"Global dedup pool: {len(existing_ids)} IDs\")\n",
    "    print(f\"{'='*70}\\n\")\n",
    "    \n",
    "    driver = create_driver()\n",
    "    all_leads = list(existing_file_leads)\n",
    "    local_ids = {lead[\"_lead_id\"] for lead in all_leads}\n",
    "    new_leads_count = 0\n",
    "    blocked_count = 0\n",
    "    \n",
    "    try:\n",
    "        for page in range(START_PAGE, MAX_PAGES + 1):\n",
    "            url = base_url if page == 1 else f\"{base_url}?page={page}\"\n",
    "            \n",
    "            print(f\"[Page {page}] Loading...\")\n",
    "            \n",
    "            for attempt in range(MAX_RETRIES):\n",
    "                try:\n",
    "                    driver.get(url)\n",
    "                    time.sleep(2)\n",
    "                    break\n",
    "                except Exception as e:\n",
    "                    if attempt < MAX_RETRIES - 1:\n",
    "                        print(f\"  Retry {attempt + 1}...\")\n",
    "                        time.sleep(5)\n",
    "                    else:\n",
    "                        print(f\"  Failed to load page: {e}\")\n",
    "                        continue\n",
    "            \n",
    "            # Get listings\n",
    "            page_listings = get_listings_from_page(driver, search_label)\n",
    "            \n",
    "            if not page_listings:\n",
    "                print(f\"  No listings found - end of results\")\n",
    "                break\n",
    "            \n",
    "            # Filter duplicates\n",
    "            new_listings = []\n",
    "            for listing in page_listings:\n",
    "                lead_id = listing[\"_lead_id\"]\n",
    "                if lead_id not in existing_ids and lead_id not in local_ids:\n",
    "                    new_listings.append(listing)\n",
    "                    local_ids.add(lead_id)\n",
    "            \n",
    "            print(f\"  Found {len(page_listings)} listings, {len(new_listings)} new\")\n",
    "            \n",
    "            if not new_listings:\n",
    "                print(f\"  All duplicates - skipping page\")\n",
    "                if page < MAX_PAGES:\n",
    "                    delay = random.uniform(PAGE_DELAY/2, PAGE_DELAY)\n",
    "                    print(f\"  Waiting {delay:.1f}s...\")\n",
    "                    time.sleep(delay)\n",
    "                continue\n",
    "            \n",
    "            # Fetch emails for new listings\n",
    "            if FETCH_EMAILS:\n",
    "                emails_found = 0\n",
    "                for i, lead in enumerate(new_listings):\n",
    "                    company_short = lead['Company Name'][:40].ljust(40)\n",
    "                    print(f\"  [{i+1:2}/{len(new_listings)}] {company_short}\", end=\"\", flush=True)\n",
    "                    \n",
    "                    email = extract_email_from_detail(\n",
    "                        driver,\n",
    "                        lead[\"Source\"],\n",
    "                        website_url=lead.get(\"Website URL\", \"\"),\n",
    "                        debug_save=(DEBUG and page == 1 and i == 0)\n",
    "                    )\n",
    "                    \n",
    "                    if email == \"__BLOCKED__\":\n",
    "                        print(f\" -> BLOCKED\")\n",
    "                        blocked_count += 1\n",
    "                        if blocked_count >= 5:\n",
    "                            print(\"\\n  Too many blocks - restarting browser...\")\n",
    "                            try:\n",
    "                                driver.quit()\n",
    "                            except:\n",
    "                                pass\n",
    "                            time.sleep(10)\n",
    "                            driver = create_driver()\n",
    "                            blocked_count = 0\n",
    "                    elif email:\n",
    "                        lead[\"Email Address\"] = email\n",
    "                        emails_found += 1\n",
    "                        print(f\" -> {email}\")\n",
    "                    else:\n",
    "                        print(f\" -> (no email)\")\n",
    "                \n",
    "                print(f\"\\n  Page {page}: {emails_found}/{len(new_listings)} emails found\")\n",
    "            \n",
    "            # Add to results\n",
    "            all_leads.extend(new_listings)\n",
    "            new_leads_count += len(new_listings)\n",
    "            \n",
    "            # Save progress\n",
    "            save_leads_to_excel(all_leads, output_file)\n",
    "            print(f\"  Saved {len(all_leads)} total leads to {output_file}\")\n",
    "            \n",
    "            # Delay before next page\n",
    "            if page < MAX_PAGES:\n",
    "                if RESTART_DRIVER_EACH_PAGE:\n",
    "                    print(f\"  Restarting browser...\")\n",
    "                    try:\n",
    "                        driver.quit()\n",
    "                    except:\n",
    "                        pass\n",
    "                    time.sleep(3)\n",
    "                    driver = create_driver()\n",
    "                \n",
    "                delay = random.uniform(PAGE_DELAY, PAGE_DELAY + 5)\n",
    "                print(f\"  Waiting {delay:.1f}s before next page...\\n\")\n",
    "                time.sleep(delay)\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"\\nError: {e}\")\n",
    "        # Save what we have\n",
    "        if all_leads:\n",
    "            save_leads_to_excel(all_leads, output_file)\n",
    "            print(f\"Saved {len(all_leads)} leads before error\")\n",
    "    \n",
    "    finally:\n",
    "        try:\n",
    "            driver.quit()\n",
    "        except:\n",
    "            pass\n",
    "    \n",
    "    # Final summary\n",
    "    email_count = sum(1 for lead in all_leads if lead.get(\"Email Address\"))\n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"COMPLETED: {search_label} in {location}\")\n",
    "    print(f\"New leads this run: {new_leads_count}\")\n",
    "    print(f\"Total leads in file: {len(all_leads)}\")\n",
    "    print(f\"With emails: {email_count}\")\n",
    "    print(f\"{'='*70}\")\n",
    "    \n",
    "    return all_leads, new_leads_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "#                           RUN MODES\n",
    "# ============================================================================\n",
    "\n",
    "def run_single_search():\n",
    "    \"\"\"Run a single search term + location\"\"\"\n",
    "    ensure_output_dir()\n",
    "    \n",
    "    search = SEARCHES[CURRENT_SEARCH_INDEX]\n",
    "    location = LOCATIONS[CURRENT_LOCATION_INDEX]\n",
    "    \n",
    "    print(f\"\\nMODE: Single Search\")\n",
    "    print(f\"Search: {search['term']} ({search['label']})\")\n",
    "    print(f\"Location: {location}\")\n",
    "    \n",
    "    existing_ids = load_all_existing_lead_ids()\n",
    "    scrape_search(search[\"term\"], search[\"label\"], location, existing_ids)\n",
    "\n",
    "\n",
    "def run_batch_search():\n",
    "    \"\"\"Run one search term across all locations\"\"\"\n",
    "    ensure_output_dir()\n",
    "    \n",
    "    search = SEARCHES[CURRENT_SEARCH_INDEX]\n",
    "    \n",
    "    print(f\"\\nMODE: Batch Search (all locations)\")\n",
    "    print(f\"Search: {search['term']} ({search['label']})\")\n",
    "    print(f\"Locations: {len(LOCATIONS)}\")\n",
    "    \n",
    "    total_new = 0\n",
    "    existing_ids = load_all_existing_lead_ids()\n",
    "    \n",
    "    for i, location in enumerate(LOCATIONS):\n",
    "        print(f\"\\n>>> Location {i+1}/{len(LOCATIONS)}: {location}\")\n",
    "        save_progress(CURRENT_SEARCH_INDEX, i, 0)\n",
    "        \n",
    "        _, new_count = scrape_search(search[\"term\"], search[\"label\"], location, existing_ids)\n",
    "        total_new += new_count\n",
    "        \n",
    "        # Update global dedup pool\n",
    "        existing_ids = load_all_existing_lead_ids()\n",
    "        \n",
    "        # Long delay between locations\n",
    "        if i < len(LOCATIONS) - 1:\n",
    "            delay = random.uniform(30, 60)\n",
    "            print(f\"\\nWaiting {delay:.0f}s before next location...\\n\")\n",
    "            time.sleep(delay)\n",
    "    \n",
    "    save_progress(CURRENT_SEARCH_INDEX, len(LOCATIONS) - 1, 0, \"completed\")\n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"BATCH COMPLETE!\")\n",
    "    print(f\"Total new leads: {total_new}\")\n",
    "    print(f\"{'='*70}\")\n",
    "\n",
    "\n",
    "def run_all_searches():\n",
    "    \"\"\"Run ALL search terms across ALL locations (comprehensive)\"\"\"\n",
    "    ensure_output_dir()\n",
    "    \n",
    "    print(f\"\\nMODE: Full Scrape (all searches x all locations)\")\n",
    "    print(f\"Searches: {len(SEARCHES)}\")\n",
    "    print(f\"Locations: {len(LOCATIONS)}\")\n",
    "    print(f\"Total combinations: {len(SEARCHES) * len(LOCATIONS)}\")\n",
    "    \n",
    "    total_new = 0\n",
    "    existing_ids = load_all_existing_lead_ids()\n",
    "    combo_count = 0\n",
    "    total_combos = len(SEARCHES) * len(LOCATIONS)\n",
    "    \n",
    "    for si, search in enumerate(SEARCHES):\n",
    "        for li, location in enumerate(LOCATIONS):\n",
    "            combo_count += 1\n",
    "            print(f\"\\n>>> Combo {combo_count}/{total_combos}: {search['term']} @ {location}\")\n",
    "            save_progress(si, li, 0)\n",
    "            \n",
    "            _, new_count = scrape_search(search[\"term\"], search[\"label\"], location, existing_ids)\n",
    "            total_new += new_count\n",
    "            \n",
    "            # Update global dedup pool\n",
    "            existing_ids = load_all_existing_lead_ids()\n",
    "            \n",
    "            # Delay between combos\n",
    "            if combo_count < total_combos:\n",
    "                delay = random.uniform(20, 40)\n",
    "                print(f\"\\nWaiting {delay:.0f}s before next combo...\\n\")\n",
    "                time.sleep(delay)\n",
    "    \n",
    "    save_progress(len(SEARCHES) - 1, len(LOCATIONS) - 1, 0, \"completed\")\n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"FULL SCRAPE COMPLETE!\")\n",
    "    print(f\"Total new leads: {total_new}\")\n",
    "    print(f\"{'='*70}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "#                       RUN SCRAPER (Main Cell)\n",
    "# ============================================================================\n",
    "\n",
    "print(f\"\\n{'='*70}\")\n",
    "print(\"B2B WHOLESALER/WAREHOUSE YELLOW PAGES SCRAPER\")\n",
    "print(f\"{'='*70}\")\n",
    "print(f\"Mode: {MODE}\")\n",
    "print(f\"Fetch emails: {FETCH_EMAILS}\")\n",
    "print(f\"Headless: {HEADLESS}\")\n",
    "print(f\"Output directory: {OUTPUT_DIR}\")\n",
    "print(f\"{'='*70}\\n\")\n",
    "\n",
    "if MODE == \"single\":\n",
    "    run_single_search()\n",
    "elif MODE == \"batch\":\n",
    "    run_batch_search()\n",
    "elif MODE == \"all\":\n",
    "    run_all_searches()\n",
    "else:\n",
    "    print(f\"Unknown mode: {MODE}\")\n",
    "    print(\"Valid modes: single, batch, all\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "#                    MERGE ALL FILES (Run after scraping)\n",
    "# ============================================================================\n",
    "\n",
    "import glob\n",
    "\n",
    "def merge_all_files():\n",
    "    \"\"\"Merge all scraped files into one master file\"\"\"\n",
    "    ensure_output_dir()\n",
    "    \n",
    "    files = glob.glob(os.path.join(OUTPUT_DIR, \"yp_b2b_*.xlsx\"))\n",
    "    \n",
    "    if not files:\n",
    "        print(\"No files to merge!\")\n",
    "        return None\n",
    "    \n",
    "    print(f\"Merging {len(files)} files...\")\n",
    "    \n",
    "    all_leads = []\n",
    "    for f in files:\n",
    "        try:\n",
    "            df = pd.read_excel(f)\n",
    "            all_leads.extend(df.to_dict('records'))\n",
    "        except Exception as e:\n",
    "            print(f\"  Error reading {f}: {e}\")\n",
    "    \n",
    "    if not all_leads:\n",
    "        print(\"No leads found!\")\n",
    "        return None\n",
    "    \n",
    "    # Deduplicate by company name + phone\n",
    "    seen = set()\n",
    "    unique_leads = []\n",
    "    for lead in all_leads:\n",
    "        key = generate_lead_id(lead.get(\"Company Name\", \"\"), lead.get(\"Phone Number\", \"\"))\n",
    "        if key not in seen:\n",
    "            seen.add(key)\n",
    "            unique_leads.append(lead)\n",
    "    \n",
    "    # Renumber\n",
    "    for i, lead in enumerate(unique_leads, 1):\n",
    "        lead[\"#\"] = i\n",
    "    \n",
    "    # Save\n",
    "    output_path = os.path.join(OUTPUT_DIR, \"yp_b2b_ALL_LEADS_MERGED.xlsx\")\n",
    "    df = pd.DataFrame(unique_leads)\n",
    "    df.to_excel(output_path, index=False)\n",
    "    add_checkboxes(output_path)\n",
    "    \n",
    "    email_count = sum(1 for lead in unique_leads if lead.get(\"Email Address\"))\n",
    "    \n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"MERGE COMPLETE!\")\n",
    "    print(f\"Files merged: {len(files)}\")\n",
    "    print(f\"Total unique leads: {len(unique_leads)}\")\n",
    "    print(f\"With emails: {email_count}\")\n",
    "    print(f\"Saved to: {output_path}\")\n",
    "    print(f\"{'='*70}\")\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Uncomment to run:\n",
    "# df = merge_all_files()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "#                    EXPORT LEADS WITH EMAILS ONLY\n",
    "# ============================================================================\n",
    "\n",
    "def export_with_emails_only():\n",
    "    \"\"\"Export only leads that have emails\"\"\"\n",
    "    merged_path = os.path.join(OUTPUT_DIR, \"yp_b2b_ALL_LEADS_MERGED.xlsx\")\n",
    "    \n",
    "    if not os.path.exists(merged_path):\n",
    "        print(\"Run merge_all_files() first!\")\n",
    "        return None\n",
    "    \n",
    "    df = pd.read_excel(merged_path)\n",
    "    df_emails = df[df[\"Email Address\"].notna() & (df[\"Email Address\"] != \"\")]\n",
    "    \n",
    "    # Renumber\n",
    "    df_emails = df_emails.copy()\n",
    "    df_emails[\"#\"] = range(1, len(df_emails) + 1)\n",
    "    \n",
    "    output_path = os.path.join(OUTPUT_DIR, \"yp_b2b_LEADS_WITH_EMAILS.xlsx\")\n",
    "    df_emails.to_excel(output_path, index=False)\n",
    "    add_checkboxes(output_path)\n",
    "    \n",
    "    print(f\"Exported {len(df_emails)} leads with emails to: {output_path}\")\n",
    "    return df_emails\n",
    "\n",
    "# Uncomment to run:\n",
    "# df_emails = export_with_emails_only()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "#                         VIEW STATISTICS\n",
    "# ============================================================================\n",
    "\n",
    "def print_stats():\n",
    "    \"\"\"Print statistics about collected leads\"\"\"\n",
    "    merged_path = os.path.join(OUTPUT_DIR, \"yp_b2b_ALL_LEADS_MERGED.xlsx\")\n",
    "    \n",
    "    if not os.path.exists(merged_path):\n",
    "        print(\"Run merge_all_files() first!\")\n",
    "        return\n",
    "    \n",
    "    df = pd.read_excel(merged_path)\n",
    "    \n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(\"LEAD STATISTICS\")\n",
    "    print(f\"{'='*70}\")\n",
    "    print(f\"Total leads: {len(df)}\")\n",
    "    print(f\"With emails: {df['Email Address'].notna().sum()}\")\n",
    "    print(f\"With websites: {df['Website URL'].notna().sum()}\")\n",
    "    print(f\"With phones: {df['Phone Number'].notna().sum()}\")\n",
    "    print(f\"\\nBy Industry:\")\n",
    "    print(df[\"Industry\"].value_counts().to_string())\n",
    "    print(f\"{'='*70}\")\n",
    "\n",
    "# Uncomment to run:\n",
    "# print_stats()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "#                      EXPORT BY INDUSTRY\n",
    "# ============================================================================\n",
    "\n",
    "def export_by_industry():\n",
    "    \"\"\"Export leads grouped by industry type\"\"\"\n",
    "    merged_path = os.path.join(OUTPUT_DIR, \"yp_b2b_ALL_LEADS_MERGED.xlsx\")\n",
    "    \n",
    "    if not os.path.exists(merged_path):\n",
    "        print(\"Run merge_all_files() first!\")\n",
    "        return\n",
    "    \n",
    "    df = pd.read_excel(merged_path)\n",
    "    \n",
    "    for industry in df[\"Industry\"].unique():\n",
    "        industry_df = df[df[\"Industry\"] == industry].copy()\n",
    "        industry_df[\"#\"] = range(1, len(industry_df) + 1)\n",
    "        \n",
    "        safe_name = industry.replace(\"/\", \"-\").replace(\" \", \"_\").lower()\n",
    "        output_path = os.path.join(OUTPUT_DIR, f\"yp_b2b_by_industry_{safe_name}.xlsx\")\n",
    "        industry_df.to_excel(output_path, index=False)\n",
    "        add_checkboxes(output_path)\n",
    "        \n",
    "        print(f\"  {industry}: {len(industry_df)} leads -> {output_path}\")\n",
    "\n",
    "# Uncomment to run:\n",
    "# export_by_industry()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
