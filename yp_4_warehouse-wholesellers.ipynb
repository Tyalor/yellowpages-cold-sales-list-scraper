{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# B2B Wholesaler/Warehouse Yellow Pages Scraper\n",
    "\n",
    "**Target:** B2B businesses that could benefit from custom NextJS web apps for order management, invoicing, and inventory systems.\n",
    "\n",
    "## Features\n",
    "- 40+ search terms targeting wholesalers, distributors, warehouses, manufacturers\n",
    "- 35+ NYC locations including industrial areas and nearby NJ\n",
    "- Auto-resume capability (saves progress after each listing)\n",
    "- Built-in deduplication across all files\n",
    "- Rotating user agents to avoid detection\n",
    "- Robust error handling with retries\n",
    "\n",
    "## How to Use\n",
    "1. Set the MODE in Cell 1 configuration\n",
    "2. Run Cell 1 to start scraping\n",
    "3. Run Cell 2 to merge all files when done\n",
    "4. Run Cell 3 to export leads with emails only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import re\n",
    "import random\n",
    "import os\n",
    "import json\n",
    "import hashlib\n",
    "from datetime import datetime\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "from openpyxl import load_workbook\n",
    "from openpyxl.worksheet.datavalidation import DataValidation\n",
    "\n",
    "# ============================================================================\n",
    "#                              CONFIGURATION\n",
    "# ============================================================================\n",
    "\n",
    "# === MODE: Choose how to run ===\n",
    "# \"single\"    - Run one search term + one location (set CURRENT_SEARCH_INDEX & CURRENT_LOCATION_INDEX)\n",
    "# \"batch\"     - Run one search term across ALL locations\n",
    "# \"all\"       - Run ALL search terms across ALL locations (comprehensive but slow)\n",
    "# \"resume\"    - Resume from saved progress file\n",
    "MODE = \"single\"\n",
    "\n",
    "# === B2B WHOLESALER/WAREHOUSE SEARCH TERMS ===\n",
    "# These businesses typically need order management, invoicing, inventory software\n",
    "SEARCHES = [\n",
    "    # Core wholesale/distribution\n",
    "    {\"term\": \"wholesale\", \"label\": \"Wholesale\"},\n",
    "    {\"term\": \"wholesalers\", \"label\": \"Wholesaler\"},\n",
    "    {\"term\": \"wholesale-distributors\", \"label\": \"Wholesale Distributor\"},\n",
    "    {\"term\": \"distributors\", \"label\": \"Distributor\"},\n",
    "    {\"term\": \"distribution-services\", \"label\": \"Distribution Services\"},\n",
    "    \n",
    "    # Warehousing\n",
    "    {\"term\": \"warehouses\", \"label\": \"Warehouse\"},\n",
    "    {\"term\": \"warehouse-storage\", \"label\": \"Warehouse Storage\"},\n",
    "    {\"term\": \"warehousing\", \"label\": \"Warehousing\"},\n",
    "    {\"term\": \"public-warehouses\", \"label\": \"Public Warehouse\"},\n",
    "    {\"term\": \"cold-storage\", \"label\": \"Cold Storage\"},\n",
    "    \n",
    "    # Import/Export (often need customs & invoicing)\n",
    "    {\"term\": \"importers\", \"label\": \"Importer\"},\n",
    "    {\"term\": \"exporters\", \"label\": \"Exporter\"},\n",
    "    {\"term\": \"import-export\", \"label\": \"Import/Export\"},\n",
    "    {\"term\": \"freight-forwarding\", \"label\": \"Freight Forwarding\"},\n",
    "    {\"term\": \"customs-brokers\", \"label\": \"Customs Broker\"},\n",
    "    \n",
    "    # Manufacturing & Industrial (need order systems)\n",
    "    {\"term\": \"manufacturers\", \"label\": \"Manufacturer\"},\n",
    "    {\"term\": \"manufacturing\", \"label\": \"Manufacturing\"},\n",
    "    {\"term\": \"industrial-equipment\", \"label\": \"Industrial Equipment\"},\n",
    "    {\"term\": \"packaging-materials-equipment\", \"label\": \"Packaging\"},\n",
    "    \n",
    "    # Food/Beverage Wholesale (high volume, need invoicing)\n",
    "    {\"term\": \"food-brokers\", \"label\": \"Food Broker\"},\n",
    "    {\"term\": \"food-products-wholesale\", \"label\": \"Food Wholesale\"},\n",
    "    {\"term\": \"beverage-distributors\", \"label\": \"Beverage Distributor\"},\n",
    "    {\"term\": \"grocery-wholesale\", \"label\": \"Grocery Wholesale\"},\n",
    "    {\"term\": \"meat-wholesale\", \"label\": \"Meat Wholesale\"},\n",
    "    {\"term\": \"produce-wholesale\", \"label\": \"Produce Wholesale\"},\n",
    "    {\"term\": \"seafood-wholesale\", \"label\": \"Seafood Wholesale\"},\n",
    "    \n",
    "    # Building/Construction Supplies (B2B heavy)\n",
    "    {\"term\": \"building-materials\", \"label\": \"Building Materials\"},\n",
    "    {\"term\": \"lumber-wholesale\", \"label\": \"Lumber Wholesale\"},\n",
    "    {\"term\": \"plumbing-supplies-wholesale\", \"label\": \"Plumbing Supplies\"},\n",
    "    {\"term\": \"electrical-supplies-wholesale\", \"label\": \"Electrical Supplies\"},\n",
    "    {\"term\": \"hardware-wholesale\", \"label\": \"Hardware Wholesale\"},\n",
    "    \n",
    "    # Other B2B\n",
    "    {\"term\": \"paper-products-wholesale\", \"label\": \"Paper Products\"},\n",
    "    {\"term\": \"janitorial-supplies\", \"label\": \"Janitorial Supplies\"},\n",
    "    {\"term\": \"restaurant-equipment-supplies\", \"label\": \"Restaurant Equipment\"},\n",
    "    {\"term\": \"beauty-supplies-wholesale\", \"label\": \"Beauty Supplies\"},\n",
    "    {\"term\": \"clothing-wholesale\", \"label\": \"Clothing Wholesale\"},\n",
    "    {\"term\": \"auto-parts-wholesale\", \"label\": \"Auto Parts Wholesale\"},\n",
    "    {\"term\": \"electronics-wholesale\", \"label\": \"Electronics Wholesale\"},\n",
    "    {\"term\": \"medical-equipment-supplies\", \"label\": \"Medical Supplies\"},\n",
    "    {\"term\": \"office-supplies-wholesale\", \"label\": \"Office Supplies\"},\n",
    "    \n",
    "    # Logistics (often need custom software)\n",
    "    {\"term\": \"logistics\", \"label\": \"Logistics\"},\n",
    "    {\"term\": \"fulfillment-services\", \"label\": \"Fulfillment Services\"},\n",
    "    {\"term\": \"third-party-logistics\", \"label\": \"3PL\"},\n",
    "    {\"term\": \"supply-chain\", \"label\": \"Supply Chain\"},\n",
    "]\n",
    "\n",
    "# === NYC LOCATIONS ===\n",
    "# Comprehensive coverage of NYC boroughs and industrial/commercial areas\n",
    "LOCATIONS = [\n",
    "    # Main boroughs\n",
    "    \"queens-ny\",\n",
    "    \"brooklyn-ny\",\n",
    "    \"bronx-ny\",\n",
    "    \"manhattan-ny\",\n",
    "    \"staten-island-ny\",\n",
    "    \n",
    "    # Queens industrial/commercial areas\n",
    "    \"long-island-city-ny\",\n",
    "    \"maspeth-ny\",\n",
    "    \"jamaica-ny\",\n",
    "    \"flushing-ny\",\n",
    "    \"astoria-ny\",\n",
    "    \"woodside-ny\",\n",
    "    \"ridgewood-ny\",\n",
    "    \"college-point-ny\",\n",
    "    \"ozone-park-ny\",\n",
    "    \n",
    "    # Brooklyn industrial areas\n",
    "    \"sunset-park-brooklyn-ny\",\n",
    "    \"red-hook-brooklyn-ny\",\n",
    "    \"bushwick-brooklyn-ny\",\n",
    "    \"east-new-york-brooklyn-ny\",\n",
    "    \"greenpoint-brooklyn-ny\",\n",
    "    \"williamsburg-brooklyn-ny\",\n",
    "    \"industry-city-brooklyn-ny\",\n",
    "    \"brooklyn-navy-yard-ny\",\n",
    "    \"canarsie-brooklyn-ny\",\n",
    "    \n",
    "    # Bronx industrial areas\n",
    "    \"hunts-point-bronx-ny\",\n",
    "    \"port-morris-bronx-ny\",\n",
    "    \"mott-haven-bronx-ny\",\n",
    "    \"south-bronx-ny\",\n",
    "    \"fordham-bronx-ny\",\n",
    "    \n",
    "    # Manhattan commercial\n",
    "    \"chelsea-ny\",\n",
    "    \"tribeca-ny\",\n",
    "    \"lower-manhattan-ny\",\n",
    "    \"garment-district-ny\",\n",
    "    \"meatpacking-district-ny\",\n",
    "    \n",
    "    # Nearby NJ (many warehouses serve NYC)\n",
    "    \"jersey-city-nj\",\n",
    "    \"newark-nj\",\n",
    "    \"elizabeth-nj\",\n",
    "    \"secaucus-nj\",\n",
    "    \"kearny-nj\",\n",
    "]\n",
    "\n",
    "# === CURRENT SELECTION (for \"single\" mode) ===\n",
    "CURRENT_SEARCH_INDEX = 0      # Which search term to use\n",
    "CURRENT_LOCATION_INDEX = 0    # Which location to use\n",
    "\n",
    "# === PAGINATION ===\n",
    "START_PAGE = 1\n",
    "MAX_PAGES = 10  # Safety limit (YP usually stops at 3-4)\n",
    "\n",
    "# === OUTPUT ===\n",
    "OUTPUT_DIR = \"exports_b2b_warehouse\"\n",
    "PROGRESS_FILE = \"scrape_progress.json\"\n",
    "\n",
    "# === SCRAPING SETTINGS ===\n",
    "FETCH_EMAILS = True           # Set False for faster scraping (no emails)\n",
    "DEBUG = False                 # Save debug HTML files\n",
    "HEADLESS = False              # False = visible browser (bypasses Cloudflare better)\n",
    "MIN_DELAY = 4                 # Min seconds between requests\n",
    "MAX_DELAY = 8                 # Max seconds between requests\n",
    "PAGE_DELAY = 12               # Seconds between pages\n",
    "LISTING_DELAY = 3             # Seconds between listing detail fetches\n",
    "RESTART_DRIVER_EACH_PAGE = True  # Fresh session each page\n",
    "MAX_RETRIES = 3               # Retries on failure\n",
    "\n",
    "# === USER AGENTS (rotated to avoid detection) ===\n",
    "USER_AGENTS = [\n",
    "    \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36\",\n",
    "    \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36\",\n",
    "    \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/17.0 Safari/605.1.15\",\n",
    "    \"Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:121.0) Gecko/20100101 Firefox/121.0\",\n",
    "    \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10.15; rv:121.0) Gecko/20100101 Firefox/121.0\",\n",
    "    \"Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "#                              HELPER FUNCTIONS\n",
    "# ============================================================================\n",
    "\n",
    "def ensure_output_dir():\n",
    "    \"\"\"Create output directory if it doesn't exist\"\"\"\n",
    "    if not os.path.exists(OUTPUT_DIR):\n",
    "        os.makedirs(OUTPUT_DIR)\n",
    "        print(f\"Created output directory: {OUTPUT_DIR}\")\n",
    "\n",
    "\n",
    "def get_output_filename(search_term, location):\n",
    "    \"\"\"Generate output filename for a search/location combo\"\"\"\n",
    "    return os.path.join(OUTPUT_DIR, f\"yp_b2b_{location}_{search_term}.xlsx\")\n",
    "\n",
    "\n",
    "def generate_lead_id(company_name, phone):\n",
    "    \"\"\"Generate unique ID for deduplication\"\"\"\n",
    "    key = f\"{company_name.lower().strip()}|{phone.strip()}\"\n",
    "    return hashlib.md5(key.encode()).hexdigest()[:12]\n",
    "\n",
    "\n",
    "def load_existing_leads(filepath):\n",
    "    \"\"\"Load existing leads from Excel file for deduplication\"\"\"\n",
    "    if os.path.exists(filepath):\n",
    "        try:\n",
    "            df = pd.read_excel(filepath)\n",
    "            return set(\n",
    "                generate_lead_id(row[\"Company Name\"], row.get(\"Phone Number\", \"\"))\n",
    "                for _, row in df.iterrows()\n",
    "            )\n",
    "        except:\n",
    "            return set()\n",
    "    return set()\n",
    "\n",
    "\n",
    "def load_all_existing_lead_ids():\n",
    "    \"\"\"Load all lead IDs from all existing files for global deduplication\"\"\"\n",
    "    all_ids = set()\n",
    "    if os.path.exists(OUTPUT_DIR):\n",
    "        for filename in os.listdir(OUTPUT_DIR):\n",
    "            if filename.endswith(\".xlsx\"):\n",
    "                filepath = os.path.join(OUTPUT_DIR, filename)\n",
    "                all_ids.update(load_existing_leads(filepath))\n",
    "    return all_ids\n",
    "\n",
    "\n",
    "def save_progress(search_idx, location_idx, page, status=\"in_progress\"):\n",
    "    \"\"\"Save scraping progress for resume capability\"\"\"\n",
    "    progress = {\n",
    "        \"search_index\": search_idx,\n",
    "        \"location_index\": location_idx,\n",
    "        \"page\": page,\n",
    "        \"status\": status,\n",
    "        \"timestamp\": datetime.now().isoformat()\n",
    "    }\n",
    "    with open(PROGRESS_FILE, \"w\") as f:\n",
    "        json.dump(progress, f)\n",
    "\n",
    "\n",
    "def load_progress():\n",
    "    \"\"\"Load saved progress\"\"\"\n",
    "    if os.path.exists(PROGRESS_FILE):\n",
    "        with open(PROGRESS_FILE, \"r\") as f:\n",
    "            return json.load(f)\n",
    "    return None\n",
    "\n",
    "\n",
    "def create_driver():\n",
    "    \"\"\"Create Selenium WebDriver with anti-detection settings\"\"\"\n",
    "    options = Options()\n",
    "    \n",
    "    if HEADLESS:\n",
    "        options.add_argument(\"--headless=new\")\n",
    "    \n",
    "    options.add_argument(\"--no-sandbox\")\n",
    "    options.add_argument(\"--disable-dev-shm-usage\")\n",
    "    options.add_argument(\"--disable-gpu\")\n",
    "    options.add_argument(\"--window-size=1920,1080\")\n",
    "    options.add_argument(\"--disable-blink-features=AutomationControlled\")\n",
    "    \n",
    "    # Random user agent\n",
    "    user_agent = random.choice(USER_AGENTS)\n",
    "    options.add_argument(f\"user-agent={user_agent}\")\n",
    "    \n",
    "    options.add_experimental_option(\"excludeSwitches\", [\"enable-automation\"])\n",
    "    options.add_experimental_option('useAutomationExtension', False)\n",
    "    \n",
    "    driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()), options=options)\n",
    "    driver.execute_script(\"Object.defineProperty(navigator, 'webdriver', {get: () => undefined})\")\n",
    "    \n",
    "    return driver\n",
    "\n",
    "\n",
    "def random_delay(min_sec=None, max_sec=None):\n",
    "    \"\"\"Human-like random delay\"\"\"\n",
    "    min_sec = min_sec or MIN_DELAY\n",
    "    max_sec = max_sec or MAX_DELAY\n",
    "    delay = random.uniform(min_sec, max_sec)\n",
    "    time.sleep(delay)\n",
    "    return delay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "#                           EMAIL EXTRACTION\n",
    "# ============================================================================\n",
    "\n",
    "# Email false positive filter\n",
    "EMAIL_BLACKLIST = [\n",
    "    'example.com', 'domain.com', 'email.com', 'yoursite', 'yourdomain',\n",
    "    'sentry.io', 'schema.org', 'json', 'wixpress', 'wix.com',\n",
    "    'googleapis', 'google.com', 'facebook', 'twitter', 'instagram',\n",
    "    '.png', '.jpg', '.gif', '.svg', '.css', '.js',\n",
    "    'yellowpages', 'yp.com', 'placeholder', 'test.com',\n",
    "    'wordpress', 'squarespace', 'shopify', 'godaddy'\n",
    "]\n",
    "\n",
    "\n",
    "def is_valid_email(email):\n",
    "    \"\"\"Check if email is likely valid (not a false positive)\"\"\"\n",
    "    if not email or '@' not in email:\n",
    "        return False\n",
    "    email_lower = email.lower()\n",
    "    return not any(x in email_lower for x in EMAIL_BLACKLIST)\n",
    "\n",
    "\n",
    "def extract_email_from_website(driver, website_url, timeout=15):\n",
    "    \"\"\"Extract email from company's own website\"\"\"\n",
    "    if not website_url or website_url == \"N/A\":\n",
    "        return \"\"\n",
    "    \n",
    "    try:\n",
    "        # Clean up URL\n",
    "        if not website_url.startswith(\"http\"):\n",
    "            website_url = \"https://\" + website_url\n",
    "        \n",
    "        random_delay(1, 2)\n",
    "        driver.set_page_load_timeout(timeout)\n",
    "        \n",
    "        try:\n",
    "            driver.get(website_url)\n",
    "        except:\n",
    "            return \"\"\n",
    "        \n",
    "        time.sleep(2)\n",
    "        page_source = driver.page_source\n",
    "        \n",
    "        # Check for error pages\n",
    "        if any(x in driver.title.lower() for x in [\"404\", \"not found\", \"error\", \"denied\"]):\n",
    "            return \"\"\n",
    "        \n",
    "        # Method 1: Find mailto links\n",
    "        mailto_match = re.search(r'href=[\"\\']mailto:([^\"\\'<>?\\s]+)', page_source, re.IGNORECASE)\n",
    "        if mailto_match:\n",
    "            email = mailto_match.group(1).strip()\n",
    "            if is_valid_email(email):\n",
    "                return email\n",
    "        \n",
    "        # Method 2: Find email patterns in page\n",
    "        email_matches = re.findall(r'[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\\.[a-zA-Z]{2,}', page_source)\n",
    "        for email in email_matches:\n",
    "            if is_valid_email(email):\n",
    "                return email\n",
    "        \n",
    "        # Method 3: Try contact pages\n",
    "        base_url = website_url.rstrip('/')\n",
    "        contact_paths = ['/contact', '/contact-us', '/about', '/about-us', '/contactus']\n",
    "        \n",
    "        for path in contact_paths:\n",
    "            try:\n",
    "                driver.get(base_url + path)\n",
    "                time.sleep(1.5)\n",
    "                contact_source = driver.page_source\n",
    "                \n",
    "                mailto_match = re.search(r'href=[\"\\']mailto:([^\"\\'<>?\\s]+)', contact_source, re.IGNORECASE)\n",
    "                if mailto_match:\n",
    "                    email = mailto_match.group(1).strip()\n",
    "                    if is_valid_email(email):\n",
    "                        return email\n",
    "                \n",
    "                email_matches = re.findall(r'[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\\.[a-zA-Z]{2,}', contact_source)\n",
    "                for email in email_matches:\n",
    "                    if is_valid_email(email):\n",
    "                        return email\n",
    "            except:\n",
    "                continue\n",
    "    \n",
    "    except Exception as e:\n",
    "        if DEBUG:\n",
    "            print(f\" [website error: {e}]\", end=\"\")\n",
    "    \n",
    "    return \"\"\n",
    "\n",
    "\n",
    "def extract_email_from_detail(driver, detail_url, website_url=\"\", debug_save=False):\n",
    "    \"\"\"Extract email from Yellow Pages detail page, fallback to company website\"\"\"\n",
    "    try:\n",
    "        random_delay(LISTING_DELAY, LISTING_DELAY + 2)\n",
    "        driver.get(detail_url)\n",
    "        \n",
    "        # Wait for page load\n",
    "        try:\n",
    "            WebDriverWait(driver, 10).until(\n",
    "                EC.presence_of_element_located((By.CSS_SELECTOR, \".business-info, .sales-info, #main-content, #cf-wrapper\"))\n",
    "            )\n",
    "        except:\n",
    "            pass\n",
    "        \n",
    "        time.sleep(2)\n",
    "        page_source = driver.page_source\n",
    "        \n",
    "        # Check for Cloudflare block\n",
    "        is_blocked = (\n",
    "            \"you have been blocked\" in page_source.lower() or\n",
    "            (\"cloudflare\" in page_source.lower() and \"ray id\" in page_source.lower())\n",
    "        )\n",
    "        \n",
    "        if is_blocked:\n",
    "            if website_url:\n",
    "                print(\" [YP blocked, trying website]\", end=\"\")\n",
    "                email = extract_email_from_website(driver, website_url)\n",
    "                if email:\n",
    "                    return email\n",
    "            return \"__BLOCKED__\"\n",
    "        \n",
    "        # Debug save\n",
    "        if debug_save:\n",
    "            with open(os.path.join(OUTPUT_DIR, \"debug_page_source.html\"), \"w\", encoding=\"utf-8\") as f:\n",
    "                f.write(page_source)\n",
    "        \n",
    "        # Scroll to load lazy content\n",
    "        driver.execute_script(\"window.scrollTo(0, 800);\")\n",
    "        time.sleep(1)\n",
    "        page_source = driver.page_source\n",
    "        \n",
    "        # Method 1: Mailto links in page source\n",
    "        mailto_match = re.search(r'href=[\"\\']mailto:([^\"\\'<>?\\s]+)', page_source, re.IGNORECASE)\n",
    "        if mailto_match:\n",
    "            email = mailto_match.group(1).strip()\n",
    "            if is_valid_email(email):\n",
    "                return email\n",
    "        \n",
    "        # Method 2: Selenium - email-business link\n",
    "        try:\n",
    "            email_elements = driver.find_elements(By.CSS_SELECTOR, \"a.email-business, a[class*='email']\")\n",
    "            for el in email_elements:\n",
    "                href = el.get_attribute(\"href\") or \"\"\n",
    "                if \"mailto:\" in href:\n",
    "                    email = href.replace(\"mailto:\", \"\").split(\"?\")[0].strip()\n",
    "                    if is_valid_email(email):\n",
    "                        return email\n",
    "        except:\n",
    "            pass\n",
    "        \n",
    "        # Method 3: Any mailto link\n",
    "        try:\n",
    "            mailto_links = driver.find_elements(By.CSS_SELECTOR, \"a[href*='mailto:']\")\n",
    "            for link in mailto_links:\n",
    "                href = link.get_attribute(\"href\") or \"\"\n",
    "                if \"mailto:\" in href:\n",
    "                    email = href.replace(\"mailto:\", \"\").split(\"?\")[0].strip()\n",
    "                    if is_valid_email(email):\n",
    "                        return email\n",
    "        except:\n",
    "            pass\n",
    "        \n",
    "        # Method 4: BeautifulSoup\n",
    "        soup = BeautifulSoup(page_source, \"html.parser\")\n",
    "        for link in soup.find_all(\"a\", href=True):\n",
    "            href = link[\"href\"]\n",
    "            if \"mailto:\" in href:\n",
    "                email = href.replace(\"mailto:\", \"\").split(\"?\")[0].strip()\n",
    "                if is_valid_email(email):\n",
    "                    return email\n",
    "        \n",
    "        # Method 5: Regex search\n",
    "        email_matches = re.findall(r'[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\\.[a-zA-Z]{2,}', page_source)\n",
    "        for email in email_matches:\n",
    "            if is_valid_email(email):\n",
    "                return email\n",
    "        \n",
    "        # Fallback: Try company website\n",
    "        if website_url:\n",
    "            print(\" [trying website]\", end=\"\")\n",
    "            email = extract_email_from_website(driver, website_url)\n",
    "            if email:\n",
    "                return email\n",
    "    \n",
    "    except Exception as e:\n",
    "        if DEBUG:\n",
    "            print(f\" [error: {e}]\", end=\"\")\n",
    "    \n",
    "    return \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "#                           LISTING PARSING\n",
    "# ============================================================================\n",
    "\n",
    "def parse_listing(listing, industry_label):\n",
    "    \"\"\"Parse a single listing into a lead dict\"\"\"\n",
    "    try:\n",
    "        # Company name\n",
    "        name_el = listing.select_one(\".business-name span\")\n",
    "        if not name_el:\n",
    "            name_el = listing.select_one(\".business-name\")\n",
    "        company = name_el.text.strip() if name_el else \"\"\n",
    "        \n",
    "        if not company:\n",
    "            return None\n",
    "        \n",
    "        # Phone\n",
    "        phone_el = listing.select_one(\".phones\")\n",
    "        phone = phone_el.text.strip() if phone_el else \"\"\n",
    "        \n",
    "        # Address\n",
    "        street = listing.select_one(\".street-address\")\n",
    "        locality = listing.select_one(\".locality\")\n",
    "        address = \" \".join(filter(None, [\n",
    "            street.text.strip() if street else \"\",\n",
    "            locality.text.strip() if locality else \"\"\n",
    "        ]))\n",
    "        \n",
    "        # Website\n",
    "        website_el = listing.select_one(\".track-visit-website\")\n",
    "        website = website_el[\"href\"] if website_el else \"\"\n",
    "        \n",
    "        # Detail link\n",
    "        detail_el = listing.select_one(\".business-name\")\n",
    "        detail_link = \"\"\n",
    "        if detail_el and detail_el.get(\"href\"):\n",
    "            detail_link = \"https://www.yellowpages.com\" + detail_el[\"href\"]\n",
    "        \n",
    "        # Categories/services (useful context)\n",
    "        categories_el = listing.select_one(\".categories\")\n",
    "        categories = categories_el.text.strip() if categories_el else \"\"\n",
    "        \n",
    "        return {\n",
    "            \"#\": None,\n",
    "            \"Company Name\": company,\n",
    "            \"Industry\": industry_label,\n",
    "            \"Category\": categories,\n",
    "            \"Contact Name\": \"\",\n",
    "            \"Email Address\": \"\",\n",
    "            \"Phone Number\": phone,\n",
    "            \"Website URL\": website,\n",
    "            \"Address\": address,\n",
    "            \"Date Added\": datetime.now().strftime(\"%-m/%-d/%y\"),\n",
    "            \"Date Contacted\": \"\",\n",
    "            \"Source\": detail_link,\n",
    "            \"Notes\": \"\",\n",
    "            \"Called\": \"\",\n",
    "            \"Followed Up\": \"\",\n",
    "            \"Closed\": \"\",\n",
    "            \"_lead_id\": generate_lead_id(company, phone)\n",
    "        }\n",
    "    except Exception as e:\n",
    "        if DEBUG:\n",
    "            print(f\"  Parse error: {e}\")\n",
    "        return None\n",
    "\n",
    "\n",
    "def get_listings_from_page(driver, industry_label):\n",
    "    \"\"\"Extract all listings from current search results page\"\"\"\n",
    "    # Scroll to load all content\n",
    "    driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "    time.sleep(2)\n",
    "    \n",
    "    try:\n",
    "        WebDriverWait(driver, 10).until(\n",
    "            EC.presence_of_element_located((By.CSS_SELECTOR, \".result, .search-results\"))\n",
    "        )\n",
    "    except:\n",
    "        return []\n",
    "    \n",
    "    listings = driver.find_elements(By.CSS_SELECTOR, \".result\")\n",
    "    page_data = []\n",
    "    \n",
    "    for listing in listings:\n",
    "        try:\n",
    "            html = listing.get_attribute(\"outerHTML\")\n",
    "            soup = BeautifulSoup(html, \"html.parser\")\n",
    "            parsed = parse_listing(soup, industry_label)\n",
    "            if parsed:\n",
    "                page_data.append(parsed)\n",
    "        except:\n",
    "            continue\n",
    "    \n",
    "    return page_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "#                           EXCEL OUTPUT\n",
    "# ============================================================================\n",
    "\n",
    "def add_checkboxes(filepath):\n",
    "    \"\"\"Add checkbox dropdowns to tracking columns\"\"\"\n",
    "    try:\n",
    "        wb = load_workbook(filepath)\n",
    "        ws = wb.active\n",
    "        \n",
    "        checkbox_validation = DataValidation(type=\"list\", formula1='\"☐,☑\"', allow_blank=True)\n",
    "        ws.add_data_validation(checkbox_validation)\n",
    "        \n",
    "        headers = {cell.value: cell.column for cell in ws[1]}\n",
    "        \n",
    "        for col_name in [\"Called\", \"Followed Up\", \"Closed\"]:\n",
    "            if col_name in headers:\n",
    "                col_idx = headers[col_name]\n",
    "                for row in range(2, ws.max_row + 1):\n",
    "                    cell = ws.cell(row=row, column=col_idx)\n",
    "                    if not cell.value:\n",
    "                        cell.value = \"☐\"\n",
    "                    checkbox_validation.add(cell)\n",
    "        \n",
    "        wb.save(filepath)\n",
    "    except Exception as e:\n",
    "        print(f\"  Warning: Could not add checkboxes: {e}\")\n",
    "\n",
    "\n",
    "def save_leads_to_excel(leads, filepath):\n",
    "    \"\"\"Save leads to Excel with proper formatting\"\"\"\n",
    "    if not leads:\n",
    "        return\n",
    "    \n",
    "    # Remove internal _lead_id column for output\n",
    "    clean_leads = []\n",
    "    for lead in leads:\n",
    "        clean_lead = {k: v for k, v in lead.items() if not k.startswith(\"_\")}\n",
    "        clean_leads.append(clean_lead)\n",
    "    \n",
    "    # Renumber\n",
    "    for i, lead in enumerate(clean_leads, 1):\n",
    "        lead[\"#\"] = i\n",
    "    \n",
    "    df = pd.DataFrame(clean_leads)\n",
    "    df.to_excel(filepath, index=False)\n",
    "    add_checkboxes(filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "#                           MAIN SCRAPER\n",
    "# ============================================================================\n",
    "\n",
    "def scrape_search(search_term, search_label, location, existing_ids=None):\n",
    "    \"\"\"Scrape a single search term + location combination\"\"\"\n",
    "    existing_ids = existing_ids or set()\n",
    "    base_url = f\"https://www.yellowpages.com/{location}/{search_term}\"\n",
    "    output_file = get_output_filename(search_term, location)\n",
    "    \n",
    "    # Load any existing leads for this file\n",
    "    existing_file_leads = []\n",
    "    if os.path.exists(output_file):\n",
    "        try:\n",
    "            df = pd.read_excel(output_file)\n",
    "            existing_file_leads = df.to_dict('records')\n",
    "            for lead in existing_file_leads:\n",
    "                lead[\"_lead_id\"] = generate_lead_id(\n",
    "                    lead.get(\"Company Name\", \"\"),\n",
    "                    lead.get(\"Phone Number\", \"\")\n",
    "                )\n",
    "        except:\n",
    "            pass\n",
    "    \n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"SCRAPING: {search_label}\")\n",
    "    print(f\"Location: {location}\")\n",
    "    print(f\"URL: {base_url}\")\n",
    "    print(f\"Output: {output_file}\")\n",
    "    print(f\"Existing leads in file: {len(existing_file_leads)}\")\n",
    "    print(f\"Global dedup pool: {len(existing_ids)} IDs\")\n",
    "    print(f\"{'='*70}\\n\")\n",
    "    \n",
    "    driver = create_driver()\n",
    "    all_leads = list(existing_file_leads)\n",
    "    local_ids = {lead[\"_lead_id\"] for lead in all_leads}\n",
    "    new_leads_count = 0\n",
    "    blocked_count = 0\n",
    "    \n",
    "    try:\n",
    "        for page in range(START_PAGE, MAX_PAGES + 1):\n",
    "            url = base_url if page == 1 else f\"{base_url}?page={page}\"\n",
    "            \n",
    "            print(f\"[Page {page}] Loading...\")\n",
    "            \n",
    "            for attempt in range(MAX_RETRIES):\n",
    "                try:\n",
    "                    driver.get(url)\n",
    "                    time.sleep(2)\n",
    "                    break\n",
    "                except Exception as e:\n",
    "                    if attempt < MAX_RETRIES - 1:\n",
    "                        print(f\"  Retry {attempt + 1}...\")\n",
    "                        time.sleep(5)\n",
    "                    else:\n",
    "                        print(f\"  Failed to load page: {e}\")\n",
    "                        continue\n",
    "            \n",
    "            # Get listings\n",
    "            page_listings = get_listings_from_page(driver, search_label)\n",
    "            \n",
    "            if not page_listings:\n",
    "                print(f\"  No listings found - end of results\")\n",
    "                break\n",
    "            \n",
    "            # Filter duplicates\n",
    "            new_listings = []\n",
    "            for listing in page_listings:\n",
    "                lead_id = listing[\"_lead_id\"]\n",
    "                if lead_id not in existing_ids and lead_id not in local_ids:\n",
    "                    new_listings.append(listing)\n",
    "                    local_ids.add(lead_id)\n",
    "            \n",
    "            print(f\"  Found {len(page_listings)} listings, {len(new_listings)} new\")\n",
    "            \n",
    "            if not new_listings:\n",
    "                print(f\"  All duplicates - skipping page\")\n",
    "                if page < MAX_PAGES:\n",
    "                    delay = random.uniform(PAGE_DELAY/2, PAGE_DELAY)\n",
    "                    print(f\"  Waiting {delay:.1f}s...\")\n",
    "                    time.sleep(delay)\n",
    "                continue\n",
    "            \n",
    "            # Fetch emails for new listings\n",
    "            if FETCH_EMAILS:\n",
    "                emails_found = 0\n",
    "                for i, lead in enumerate(new_listings):\n",
    "                    company_short = lead['Company Name'][:40].ljust(40)\n",
    "                    print(f\"  [{i+1:2}/{len(new_listings)}] {company_short}\", end=\"\", flush=True)\n",
    "                    \n",
    "                    email = extract_email_from_detail(\n",
    "                        driver,\n",
    "                        lead[\"Source\"],\n",
    "                        website_url=lead.get(\"Website URL\", \"\"),\n",
    "                        debug_save=(DEBUG and page == 1 and i == 0)\n",
    "                    )\n",
    "                    \n",
    "                    if email == \"__BLOCKED__\":\n",
    "                        print(f\" -> BLOCKED\")\n",
    "                        blocked_count += 1\n",
    "                        if blocked_count >= 5:\n",
    "                            print(\"\\n  Too many blocks - restarting browser...\")\n",
    "                            try:\n",
    "                                driver.quit()\n",
    "                            except:\n",
    "                                pass\n",
    "                            time.sleep(10)\n",
    "                            driver = create_driver()\n",
    "                            blocked_count = 0\n",
    "                    elif email:\n",
    "                        lead[\"Email Address\"] = email\n",
    "                        emails_found += 1\n",
    "                        print(f\" -> {email}\")\n",
    "                    else:\n",
    "                        print(f\" -> (no email)\")\n",
    "                \n",
    "                print(f\"\\n  Page {page}: {emails_found}/{len(new_listings)} emails found\")\n",
    "            \n",
    "            # Add to results\n",
    "            all_leads.extend(new_listings)\n",
    "            new_leads_count += len(new_listings)\n",
    "            \n",
    "            # Save progress\n",
    "            save_leads_to_excel(all_leads, output_file)\n",
    "            print(f\"  Saved {len(all_leads)} total leads to {output_file}\")\n",
    "            \n",
    "            # Delay before next page\n",
    "            if page < MAX_PAGES:\n",
    "                if RESTART_DRIVER_EACH_PAGE:\n",
    "                    print(f\"  Restarting browser...\")\n",
    "                    try:\n",
    "                        driver.quit()\n",
    "                    except:\n",
    "                        pass\n",
    "                    time.sleep(3)\n",
    "                    driver = create_driver()\n",
    "                \n",
    "                delay = random.uniform(PAGE_DELAY, PAGE_DELAY + 5)\n",
    "                print(f\"  Waiting {delay:.1f}s before next page...\\n\")\n",
    "                time.sleep(delay)\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"\\nError: {e}\")\n",
    "        # Save what we have\n",
    "        if all_leads:\n",
    "            save_leads_to_excel(all_leads, output_file)\n",
    "            print(f\"Saved {len(all_leads)} leads before error\")\n",
    "    \n",
    "    finally:\n",
    "        try:\n",
    "            driver.quit()\n",
    "        except:\n",
    "            pass\n",
    "    \n",
    "    # Final summary\n",
    "    email_count = sum(1 for lead in all_leads if lead.get(\"Email Address\"))\n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"COMPLETED: {search_label} in {location}\")\n",
    "    print(f\"New leads this run: {new_leads_count}\")\n",
    "    print(f\"Total leads in file: {len(all_leads)}\")\n",
    "    print(f\"With emails: {email_count}\")\n",
    "    print(f\"{'='*70}\")\n",
    "    \n",
    "    return all_leads, new_leads_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "#                           RUN MODES\n",
    "# ============================================================================\n",
    "\n",
    "def run_single_search():\n",
    "    \"\"\"Run a single search term + location\"\"\"\n",
    "    ensure_output_dir()\n",
    "    \n",
    "    search = SEARCHES[CURRENT_SEARCH_INDEX]\n",
    "    location = LOCATIONS[CURRENT_LOCATION_INDEX]\n",
    "    \n",
    "    print(f\"\\nMODE: Single Search\")\n",
    "    print(f\"Search: {search['term']} ({search['label']})\")\n",
    "    print(f\"Location: {location}\")\n",
    "    \n",
    "    existing_ids = load_all_existing_lead_ids()\n",
    "    scrape_search(search[\"term\"], search[\"label\"], location, existing_ids)\n",
    "\n",
    "\n",
    "def run_batch_search():\n",
    "    \"\"\"Run one search term across all locations\"\"\"\n",
    "    ensure_output_dir()\n",
    "    \n",
    "    search = SEARCHES[CURRENT_SEARCH_INDEX]\n",
    "    \n",
    "    print(f\"\\nMODE: Batch Search (all locations)\")\n",
    "    print(f\"Search: {search['term']} ({search['label']})\")\n",
    "    print(f\"Locations: {len(LOCATIONS)}\")\n",
    "    \n",
    "    total_new = 0\n",
    "    existing_ids = load_all_existing_lead_ids()\n",
    "    \n",
    "    for i, location in enumerate(LOCATIONS):\n",
    "        print(f\"\\n>>> Location {i+1}/{len(LOCATIONS)}: {location}\")\n",
    "        save_progress(CURRENT_SEARCH_INDEX, i, 0)\n",
    "        \n",
    "        _, new_count = scrape_search(search[\"term\"], search[\"label\"], location, existing_ids)\n",
    "        total_new += new_count\n",
    "        \n",
    "        # Update global dedup pool\n",
    "        existing_ids = load_all_existing_lead_ids()\n",
    "        \n",
    "        # Long delay between locations\n",
    "        if i < len(LOCATIONS) - 1:\n",
    "            delay = random.uniform(30, 60)\n",
    "            print(f\"\\nWaiting {delay:.0f}s before next location...\\n\")\n",
    "            time.sleep(delay)\n",
    "    \n",
    "    save_progress(CURRENT_SEARCH_INDEX, len(LOCATIONS) - 1, 0, \"completed\")\n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"BATCH COMPLETE!\")\n",
    "    print(f\"Total new leads: {total_new}\")\n",
    "    print(f\"{'='*70}\")\n",
    "\n",
    "\n",
    "def run_all_searches():\n",
    "    \"\"\"Run ALL search terms across ALL locations (comprehensive)\"\"\"\n",
    "    ensure_output_dir()\n",
    "    \n",
    "    print(f\"\\nMODE: Full Scrape (all searches x all locations)\")\n",
    "    print(f\"Searches: {len(SEARCHES)}\")\n",
    "    print(f\"Locations: {len(LOCATIONS)}\")\n",
    "    print(f\"Total combinations: {len(SEARCHES) * len(LOCATIONS)}\")\n",
    "    \n",
    "    total_new = 0\n",
    "    existing_ids = load_all_existing_lead_ids()\n",
    "    combo_count = 0\n",
    "    total_combos = len(SEARCHES) * len(LOCATIONS)\n",
    "    \n",
    "    for si, search in enumerate(SEARCHES):\n",
    "        for li, location in enumerate(LOCATIONS):\n",
    "            combo_count += 1\n",
    "            print(f\"\\n>>> Combo {combo_count}/{total_combos}: {search['term']} @ {location}\")\n",
    "            save_progress(si, li, 0)\n",
    "            \n",
    "            _, new_count = scrape_search(search[\"term\"], search[\"label\"], location, existing_ids)\n",
    "            total_new += new_count\n",
    "            \n",
    "            # Update global dedup pool\n",
    "            existing_ids = load_all_existing_lead_ids()\n",
    "            \n",
    "            # Delay between combos\n",
    "            if combo_count < total_combos:\n",
    "                delay = random.uniform(20, 40)\n",
    "                print(f\"\\nWaiting {delay:.0f}s before next combo...\\n\")\n",
    "                time.sleep(delay)\n",
    "    \n",
    "    save_progress(len(SEARCHES) - 1, len(LOCATIONS) - 1, 0, \"completed\")\n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"FULL SCRAPE COMPLETE!\")\n",
    "    print(f\"Total new leads: {total_new}\")\n",
    "    print(f\"{'='*70}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "B2B WHOLESALER/WAREHOUSE YELLOW PAGES SCRAPER\n",
      "======================================================================\n",
      "Mode: single\n",
      "Fetch emails: True\n",
      "Headless: False\n",
      "Output directory: exports_b2b_warehouse\n",
      "======================================================================\n",
      "\n",
      "\n",
      "MODE: Single Search\n",
      "Search: wholesale (Wholesale)\n",
      "Location: queens-ny\n",
      "\n",
      "======================================================================\n",
      "SCRAPING: Wholesale\n",
      "Location: queens-ny\n",
      "URL: https://www.yellowpages.com/queens-ny/wholesale\n",
      "Output: exports_b2b_warehouse/yp_b2b_queens-ny_wholesale.xlsx\n",
      "Existing leads in file: 30\n",
      "Global dedup pool: 30 IDs\n",
      "======================================================================\n",
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[16]\u001b[39m\u001b[32m, line 15\u001b[39m\n\u001b[32m     12\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m=\u001b[39m\u001b[33m'\u001b[39m*\u001b[32m70\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m     14\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m MODE == \u001b[33m\"\u001b[39m\u001b[33msingle\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m---> \u001b[39m\u001b[32m15\u001b[39m     \u001b[43mrun_single_search\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     16\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m MODE == \u001b[33m\"\u001b[39m\u001b[33mbatch\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m     17\u001b[39m     run_batch_search()\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[15]\u001b[39m\u001b[32m, line 17\u001b[39m, in \u001b[36mrun_single_search\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m     14\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mLocation: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlocation\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m     16\u001b[39m existing_ids = load_all_existing_lead_ids()\n\u001b[32m---> \u001b[39m\u001b[32m17\u001b[39m \u001b[43mscrape_search\u001b[49m\u001b[43m(\u001b[49m\u001b[43msearch\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mterm\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msearch\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mlabel\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlocation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexisting_ids\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[14]\u001b[39m\u001b[32m, line 34\u001b[39m, in \u001b[36mscrape_search\u001b[39m\u001b[34m(search_term, search_label, location, existing_ids)\u001b[39m\n\u001b[32m     31\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mGlobal dedup pool: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(existing_ids)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m IDs\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     32\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m=\u001b[39m\u001b[33m'\u001b[39m*\u001b[32m70\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m34\u001b[39m driver = \u001b[43mcreate_driver\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     35\u001b[39m all_leads = \u001b[38;5;28mlist\u001b[39m(existing_file_leads)\n\u001b[32m     36\u001b[39m local_ids = {lead[\u001b[33m\"\u001b[39m\u001b[33m_lead_id\u001b[39m\u001b[33m\"\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m lead \u001b[38;5;129;01min\u001b[39;00m all_leads}\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[10]\u001b[39m\u001b[32m, line 89\u001b[39m, in \u001b[36mcreate_driver\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m     86\u001b[39m options.add_experimental_option(\u001b[33m\"\u001b[39m\u001b[33mexcludeSwitches\u001b[39m\u001b[33m\"\u001b[39m, [\u001b[33m\"\u001b[39m\u001b[33menable-automation\u001b[39m\u001b[33m\"\u001b[39m])\n\u001b[32m     87\u001b[39m options.add_experimental_option(\u001b[33m'\u001b[39m\u001b[33museAutomationExtension\u001b[39m\u001b[33m'\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[32m---> \u001b[39m\u001b[32m89\u001b[39m driver = \u001b[43mwebdriver\u001b[49m\u001b[43m.\u001b[49m\u001b[43mChrome\u001b[49m\u001b[43m(\u001b[49m\u001b[43mservice\u001b[49m\u001b[43m=\u001b[49m\u001b[43mService\u001b[49m\u001b[43m(\u001b[49m\u001b[43mChromeDriverManager\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43minstall\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptions\u001b[49m\u001b[43m=\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     90\u001b[39m driver.execute_script(\u001b[33m\"\u001b[39m\u001b[33mObject.defineProperty(navigator, \u001b[39m\u001b[33m'\u001b[39m\u001b[33mwebdriver\u001b[39m\u001b[33m'\u001b[39m\u001b[33m, \u001b[39m\u001b[33m{\u001b[39m\u001b[33mget: () => undefined})\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     92\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m driver\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.projects/scripts/yp/yp-scraper-env/lib/python3.13/site-packages/selenium/webdriver/chrome/webdriver.py:45\u001b[39m, in \u001b[36mWebDriver.__init__\u001b[39m\u001b[34m(self, options, service, keep_alive)\u001b[39m\n\u001b[32m     42\u001b[39m service = service \u001b[38;5;28;01mif\u001b[39;00m service \u001b[38;5;28;01melse\u001b[39;00m Service()\n\u001b[32m     43\u001b[39m options = options \u001b[38;5;28;01mif\u001b[39;00m options \u001b[38;5;28;01melse\u001b[39;00m Options()\n\u001b[32m---> \u001b[39m\u001b[32m45\u001b[39m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[34;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[32m     46\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbrowser_name\u001b[49m\u001b[43m=\u001b[49m\u001b[43mDesiredCapabilities\u001b[49m\u001b[43m.\u001b[49m\u001b[43mCHROME\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mbrowserName\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     47\u001b[39m \u001b[43m    \u001b[49m\u001b[43mvendor_prefix\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mgoog\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     48\u001b[39m \u001b[43m    \u001b[49m\u001b[43moptions\u001b[49m\u001b[43m=\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     49\u001b[39m \u001b[43m    \u001b[49m\u001b[43mservice\u001b[49m\u001b[43m=\u001b[49m\u001b[43mservice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     50\u001b[39m \u001b[43m    \u001b[49m\u001b[43mkeep_alive\u001b[49m\u001b[43m=\u001b[49m\u001b[43mkeep_alive\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     51\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.projects/scripts/yp/yp-scraper-env/lib/python3.13/site-packages/selenium/webdriver/chromium/webdriver.py:66\u001b[39m, in \u001b[36mChromiumDriver.__init__\u001b[39m\u001b[34m(self, browser_name, vendor_prefix, options, service, keep_alive)\u001b[39m\n\u001b[32m     57\u001b[39m executor = ChromiumRemoteConnection(\n\u001b[32m     58\u001b[39m     remote_server_addr=\u001b[38;5;28mself\u001b[39m.service.service_url,\n\u001b[32m     59\u001b[39m     browser_name=browser_name,\n\u001b[32m   (...)\u001b[39m\u001b[32m     62\u001b[39m     ignore_proxy=options._ignore_local_proxy,\n\u001b[32m     63\u001b[39m )\n\u001b[32m     65\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m66\u001b[39m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[34;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcommand_executor\u001b[49m\u001b[43m=\u001b[49m\u001b[43mexecutor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptions\u001b[49m\u001b[43m=\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     67\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[32m     68\u001b[39m     \u001b[38;5;28mself\u001b[39m.quit()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.projects/scripts/yp/yp-scraper-env/lib/python3.13/site-packages/selenium/webdriver/remote/webdriver.py:250\u001b[39m, in \u001b[36mWebDriver.__init__\u001b[39m\u001b[34m(self, command_executor, keep_alive, file_detector, options, locator_converter, web_element_cls, client_config)\u001b[39m\n\u001b[32m    248\u001b[39m \u001b[38;5;28mself\u001b[39m._authenticator_id = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    249\u001b[39m \u001b[38;5;28mself\u001b[39m.start_client()\n\u001b[32m--> \u001b[39m\u001b[32m250\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mstart_session\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcapabilities\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    251\u001b[39m \u001b[38;5;28mself\u001b[39m._fedcm = FedCM(\u001b[38;5;28mself\u001b[39m)\n\u001b[32m    253\u001b[39m \u001b[38;5;28mself\u001b[39m._websocket_connection = \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.projects/scripts/yp/yp-scraper-env/lib/python3.13/site-packages/selenium/webdriver/remote/webdriver.py:342\u001b[39m, in \u001b[36mWebDriver.start_session\u001b[39m\u001b[34m(self, capabilities)\u001b[39m\n\u001b[32m    333\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Creates a new session with the desired capabilities.\u001b[39;00m\n\u001b[32m    334\u001b[39m \n\u001b[32m    335\u001b[39m \u001b[33;03mParameters:\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    338\u001b[39m \u001b[33;03m    - A capabilities dict to start the session with.\u001b[39;00m\n\u001b[32m    339\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    341\u001b[39m caps = _create_caps(capabilities)\n\u001b[32m--> \u001b[39m\u001b[32m342\u001b[39m response = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mCommand\u001b[49m\u001b[43m.\u001b[49m\u001b[43mNEW_SESSION\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcaps\u001b[49m\u001b[43m)\u001b[49m[\u001b[33m\"\u001b[39m\u001b[33mvalue\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m    343\u001b[39m \u001b[38;5;28mself\u001b[39m.session_id = response.get(\u001b[33m\"\u001b[39m\u001b[33msessionId\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    344\u001b[39m \u001b[38;5;28mself\u001b[39m.caps = response.get(\u001b[33m\"\u001b[39m\u001b[33mcapabilities\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.projects/scripts/yp/yp-scraper-env/lib/python3.13/site-packages/selenium/webdriver/remote/webdriver.py:427\u001b[39m, in \u001b[36mWebDriver.execute\u001b[39m\u001b[34m(self, driver_command, params)\u001b[39m\n\u001b[32m    424\u001b[39m     \u001b[38;5;28;01melif\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33msessionId\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m params:\n\u001b[32m    425\u001b[39m         params[\u001b[33m\"\u001b[39m\u001b[33msessionId\u001b[39m\u001b[33m\"\u001b[39m] = \u001b[38;5;28mself\u001b[39m.session_id\n\u001b[32m--> \u001b[39m\u001b[32m427\u001b[39m response = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcommand_executor\u001b[49m\u001b[43m.\u001b[49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdriver_command\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    428\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m response:\n\u001b[32m    429\u001b[39m     \u001b[38;5;28mself\u001b[39m.error_handler.check_response(response)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.projects/scripts/yp/yp-scraper-env/lib/python3.13/site-packages/selenium/webdriver/remote/remote_connection.py:404\u001b[39m, in \u001b[36mRemoteConnection.execute\u001b[39m\u001b[34m(self, command, params)\u001b[39m\n\u001b[32m    402\u001b[39m trimmed = \u001b[38;5;28mself\u001b[39m._trim_large_entries(params)\n\u001b[32m    403\u001b[39m LOGGER.debug(\u001b[33m\"\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[33m \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[33m \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[33m\"\u001b[39m, command_info[\u001b[32m0\u001b[39m], url, \u001b[38;5;28mstr\u001b[39m(trimmed))\n\u001b[32m--> \u001b[39m\u001b[32m404\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcommand_info\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbody\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.projects/scripts/yp/yp-scraper-env/lib/python3.13/site-packages/selenium/webdriver/remote/remote_connection.py:428\u001b[39m, in \u001b[36mRemoteConnection._request\u001b[39m\u001b[34m(self, method, url, body)\u001b[39m\n\u001b[32m    425\u001b[39m     body = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    427\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._client_config.keep_alive:\n\u001b[32m--> \u001b[39m\u001b[32m428\u001b[39m     response = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_conn\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbody\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m=\u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_client_config\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    429\u001b[39m     statuscode = response.status\n\u001b[32m    430\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.projects/scripts/yp/yp-scraper-env/lib/python3.13/site-packages/urllib3/_request_methods.py:143\u001b[39m, in \u001b[36mRequestMethods.request\u001b[39m\u001b[34m(self, method, url, body, fields, headers, json, **urlopen_kw)\u001b[39m\n\u001b[32m    135\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.request_encode_url(\n\u001b[32m    136\u001b[39m         method,\n\u001b[32m    137\u001b[39m         url,\n\u001b[32m   (...)\u001b[39m\u001b[32m    140\u001b[39m         **urlopen_kw,\n\u001b[32m    141\u001b[39m     )\n\u001b[32m    142\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m143\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mrequest_encode_body\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    144\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfields\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfields\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m=\u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43murlopen_kw\u001b[49m\n\u001b[32m    145\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.projects/scripts/yp/yp-scraper-env/lib/python3.13/site-packages/urllib3/_request_methods.py:278\u001b[39m, in \u001b[36mRequestMethods.request_encode_body\u001b[39m\u001b[34m(self, method, url, fields, headers, encode_multipart, multipart_boundary, **urlopen_kw)\u001b[39m\n\u001b[32m    274\u001b[39m     extra_kw[\u001b[33m\"\u001b[39m\u001b[33mheaders\u001b[39m\u001b[33m\"\u001b[39m].setdefault(\u001b[33m\"\u001b[39m\u001b[33mContent-Type\u001b[39m\u001b[33m\"\u001b[39m, content_type)\n\u001b[32m    276\u001b[39m extra_kw.update(urlopen_kw)\n\u001b[32m--> \u001b[39m\u001b[32m278\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43murlopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mextra_kw\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.projects/scripts/yp/yp-scraper-env/lib/python3.13/site-packages/urllib3/poolmanager.py:443\u001b[39m, in \u001b[36mPoolManager.urlopen\u001b[39m\u001b[34m(self, method, url, redirect, **kw)\u001b[39m\n\u001b[32m    441\u001b[39m     response = conn.urlopen(method, url, **kw)\n\u001b[32m    442\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m443\u001b[39m     response = \u001b[43mconn\u001b[49m\u001b[43m.\u001b[49m\u001b[43murlopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mu\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrequest_uri\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    445\u001b[39m redirect_location = redirect \u001b[38;5;129;01mand\u001b[39;00m response.get_redirect_location()\n\u001b[32m    446\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m redirect_location:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.projects/scripts/yp/yp-scraper-env/lib/python3.13/site-packages/urllib3/connectionpool.py:787\u001b[39m, in \u001b[36mHTTPConnectionPool.urlopen\u001b[39m\u001b[34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, preload_content, decode_content, **response_kw)\u001b[39m\n\u001b[32m    784\u001b[39m response_conn = conn \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m release_conn \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    786\u001b[39m \u001b[38;5;66;03m# Make the request on the HTTPConnection object\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m787\u001b[39m response = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_make_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    788\u001b[39m \u001b[43m    \u001b[49m\u001b[43mconn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    789\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    790\u001b[39m \u001b[43m    \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    791\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout_obj\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    792\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbody\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    793\u001b[39m \u001b[43m    \u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m=\u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    794\u001b[39m \u001b[43m    \u001b[49m\u001b[43mchunked\u001b[49m\u001b[43m=\u001b[49m\u001b[43mchunked\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    795\u001b[39m \u001b[43m    \u001b[49m\u001b[43mretries\u001b[49m\u001b[43m=\u001b[49m\u001b[43mretries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    796\u001b[39m \u001b[43m    \u001b[49m\u001b[43mresponse_conn\u001b[49m\u001b[43m=\u001b[49m\u001b[43mresponse_conn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    797\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpreload_content\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpreload_content\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    798\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdecode_content\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdecode_content\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    799\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mresponse_kw\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    800\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    802\u001b[39m \u001b[38;5;66;03m# Everything went great!\u001b[39;00m\n\u001b[32m    803\u001b[39m clean_exit = \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.projects/scripts/yp/yp-scraper-env/lib/python3.13/site-packages/urllib3/connectionpool.py:534\u001b[39m, in \u001b[36mHTTPConnectionPool._make_request\u001b[39m\u001b[34m(self, conn, method, url, body, headers, retries, timeout, chunked, response_conn, preload_content, decode_content, enforce_content_length)\u001b[39m\n\u001b[32m    532\u001b[39m \u001b[38;5;66;03m# Receive the response from the server\u001b[39;00m\n\u001b[32m    533\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m534\u001b[39m     response = \u001b[43mconn\u001b[49m\u001b[43m.\u001b[49m\u001b[43mgetresponse\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    535\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m (BaseSSLError, \u001b[38;5;167;01mOSError\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    536\u001b[39m     \u001b[38;5;28mself\u001b[39m._raise_timeout(err=e, url=url, timeout_value=read_timeout)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.projects/scripts/yp/yp-scraper-env/lib/python3.13/site-packages/urllib3/connection.py:516\u001b[39m, in \u001b[36mHTTPConnection.getresponse\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    513\u001b[39m _shutdown = \u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m.sock, \u001b[33m\"\u001b[39m\u001b[33mshutdown\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[32m    515\u001b[39m \u001b[38;5;66;03m# Get the response from http.client.HTTPConnection\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m516\u001b[39m httplib_response = \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mgetresponse\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    518\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    519\u001b[39m     assert_header_parsing(httplib_response.msg)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/homebrew/Cellar/python@3.13/3.13.1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/http/client.py:1428\u001b[39m, in \u001b[36mHTTPConnection.getresponse\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1426\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m   1427\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1428\u001b[39m         \u001b[43mresponse\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbegin\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1429\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mConnectionError\u001b[39;00m:\n\u001b[32m   1430\u001b[39m         \u001b[38;5;28mself\u001b[39m.close()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/homebrew/Cellar/python@3.13/3.13.1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/http/client.py:331\u001b[39m, in \u001b[36mHTTPResponse.begin\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    329\u001b[39m \u001b[38;5;66;03m# read until we get a non-100 response\u001b[39;00m\n\u001b[32m    330\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m331\u001b[39m     version, status, reason = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_read_status\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    332\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m status != CONTINUE:\n\u001b[32m    333\u001b[39m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/homebrew/Cellar/python@3.13/3.13.1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/http/client.py:292\u001b[39m, in \u001b[36mHTTPResponse._read_status\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    291\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_read_status\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m--> \u001b[39m\u001b[32m292\u001b[39m     line = \u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mfp\u001b[49m\u001b[43m.\u001b[49m\u001b[43mreadline\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_MAXLINE\u001b[49m\u001b[43m \u001b[49m\u001b[43m+\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m)\u001b[49m, \u001b[33m\"\u001b[39m\u001b[33miso-8859-1\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    293\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(line) > _MAXLINE:\n\u001b[32m    294\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m LineTooLong(\u001b[33m\"\u001b[39m\u001b[33mstatus line\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/homebrew/Cellar/python@3.13/3.13.1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/socket.py:719\u001b[39m, in \u001b[36mSocketIO.readinto\u001b[39m\u001b[34m(self, b)\u001b[39m\n\u001b[32m    717\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mcannot read from timed out object\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    718\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m719\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_sock\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrecv_into\u001b[49m\u001b[43m(\u001b[49m\u001b[43mb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    720\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m timeout:\n\u001b[32m    721\u001b[39m     \u001b[38;5;28mself\u001b[39m._timeout_occurred = \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "#                       RUN SCRAPER (Main Cell)\n",
    "# ============================================================================\n",
    "\n",
    "print(f\"\\n{'='*70}\")\n",
    "print(\"B2B WHOLESALER/WAREHOUSE YELLOW PAGES SCRAPER\")\n",
    "print(f\"{'='*70}\")\n",
    "print(f\"Mode: {MODE}\")\n",
    "print(f\"Fetch emails: {FETCH_EMAILS}\")\n",
    "print(f\"Headless: {HEADLESS}\")\n",
    "print(f\"Output directory: {OUTPUT_DIR}\")\n",
    "print(f\"{'='*70}\\n\")\n",
    "\n",
    "if MODE == \"single\":\n",
    "    run_single_search()\n",
    "elif MODE == \"batch\":\n",
    "    run_batch_search()\n",
    "elif MODE == \"all\":\n",
    "    run_all_searches()\n",
    "else:\n",
    "    print(f\"Unknown mode: {MODE}\")\n",
    "    print(\"Valid modes: single, batch, all\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "#                    MERGE ALL FILES (Run after scraping)\n",
    "# ============================================================================\n",
    "\n",
    "import glob\n",
    "\n",
    "def merge_all_files():\n",
    "    \"\"\"Merge all scraped files into one master file\"\"\"\n",
    "    ensure_output_dir()\n",
    "    \n",
    "    files = glob.glob(os.path.join(OUTPUT_DIR, \"yp_b2b_*.xlsx\"))\n",
    "    \n",
    "    if not files:\n",
    "        print(\"No files to merge!\")\n",
    "        return None\n",
    "    \n",
    "    print(f\"Merging {len(files)} files...\")\n",
    "    \n",
    "    all_leads = []\n",
    "    for f in files:\n",
    "        try:\n",
    "            df = pd.read_excel(f)\n",
    "            all_leads.extend(df.to_dict('records'))\n",
    "        except Exception as e:\n",
    "            print(f\"  Error reading {f}: {e}\")\n",
    "    \n",
    "    if not all_leads:\n",
    "        print(\"No leads found!\")\n",
    "        return None\n",
    "    \n",
    "    # Deduplicate by company name + phone\n",
    "    seen = set()\n",
    "    unique_leads = []\n",
    "    for lead in all_leads:\n",
    "        key = generate_lead_id(lead.get(\"Company Name\", \"\"), lead.get(\"Phone Number\", \"\"))\n",
    "        if key not in seen:\n",
    "            seen.add(key)\n",
    "            unique_leads.append(lead)\n",
    "    \n",
    "    # Renumber\n",
    "    for i, lead in enumerate(unique_leads, 1):\n",
    "        lead[\"#\"] = i\n",
    "    \n",
    "    # Save\n",
    "    output_path = os.path.join(OUTPUT_DIR, \"yp_b2b_ALL_LEADS_MERGED.xlsx\")\n",
    "    df = pd.DataFrame(unique_leads)\n",
    "    df.to_excel(output_path, index=False)\n",
    "    add_checkboxes(output_path)\n",
    "    \n",
    "    email_count = sum(1 for lead in unique_leads if lead.get(\"Email Address\"))\n",
    "    \n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"MERGE COMPLETE!\")\n",
    "    print(f\"Files merged: {len(files)}\")\n",
    "    print(f\"Total unique leads: {len(unique_leads)}\")\n",
    "    print(f\"With emails: {email_count}\")\n",
    "    print(f\"Saved to: {output_path}\")\n",
    "    print(f\"{'='*70}\")\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Uncomment to run:\n",
    "# df = merge_all_files()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "#                    EXPORT LEADS WITH EMAILS ONLY\n",
    "# ============================================================================\n",
    "\n",
    "def export_with_emails_only():\n",
    "    \"\"\"Export only leads that have emails\"\"\"\n",
    "    merged_path = os.path.join(OUTPUT_DIR, \"yp_b2b_ALL_LEADS_MERGED.xlsx\")\n",
    "    \n",
    "    if not os.path.exists(merged_path):\n",
    "        print(\"Run merge_all_files() first!\")\n",
    "        return None\n",
    "    \n",
    "    df = pd.read_excel(merged_path)\n",
    "    df_emails = df[df[\"Email Address\"].notna() & (df[\"Email Address\"] != \"\")]\n",
    "    \n",
    "    # Renumber\n",
    "    df_emails = df_emails.copy()\n",
    "    df_emails[\"#\"] = range(1, len(df_emails) + 1)\n",
    "    \n",
    "    output_path = os.path.join(OUTPUT_DIR, \"yp_b2b_LEADS_WITH_EMAILS.xlsx\")\n",
    "    df_emails.to_excel(output_path, index=False)\n",
    "    add_checkboxes(output_path)\n",
    "    \n",
    "    print(f\"Exported {len(df_emails)} leads with emails to: {output_path}\")\n",
    "    return df_emails\n",
    "\n",
    "# Uncomment to run:\n",
    "# df_emails = export_with_emails_only()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "#                         VIEW STATISTICS\n",
    "# ============================================================================\n",
    "\n",
    "def print_stats():\n",
    "    \"\"\"Print statistics about collected leads\"\"\"\n",
    "    merged_path = os.path.join(OUTPUT_DIR, \"yp_b2b_ALL_LEADS_MERGED.xlsx\")\n",
    "    \n",
    "    if not os.path.exists(merged_path):\n",
    "        print(\"Run merge_all_files() first!\")\n",
    "        return\n",
    "    \n",
    "    df = pd.read_excel(merged_path)\n",
    "    \n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(\"LEAD STATISTICS\")\n",
    "    print(f\"{'='*70}\")\n",
    "    print(f\"Total leads: {len(df)}\")\n",
    "    print(f\"With emails: {df['Email Address'].notna().sum()}\")\n",
    "    print(f\"With websites: {df['Website URL'].notna().sum()}\")\n",
    "    print(f\"With phones: {df['Phone Number'].notna().sum()}\")\n",
    "    print(f\"\\nBy Industry:\")\n",
    "    print(df[\"Industry\"].value_counts().to_string())\n",
    "    print(f\"{'='*70}\")\n",
    "\n",
    "# Uncomment to run:\n",
    "# print_stats()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "#                      EXPORT BY INDUSTRY\n",
    "# ============================================================================\n",
    "\n",
    "def export_by_industry():\n",
    "    \"\"\"Export leads grouped by industry type\"\"\"\n",
    "    merged_path = os.path.join(OUTPUT_DIR, \"yp_b2b_ALL_LEADS_MERGED.xlsx\")\n",
    "    \n",
    "    if not os.path.exists(merged_path):\n",
    "        print(\"Run merge_all_files() first!\")\n",
    "        return\n",
    "    \n",
    "    df = pd.read_excel(merged_path)\n",
    "    \n",
    "    for industry in df[\"Industry\"].unique():\n",
    "        industry_df = df[df[\"Industry\"] == industry].copy()\n",
    "        industry_df[\"#\"] = range(1, len(industry_df) + 1)\n",
    "        \n",
    "        safe_name = industry.replace(\"/\", \"-\").replace(\" \", \"_\").lower()\n",
    "        output_path = os.path.join(OUTPUT_DIR, f\"yp_b2b_by_industry_{safe_name}.xlsx\")\n",
    "        industry_df.to_excel(output_path, index=False)\n",
    "        add_checkboxes(output_path)\n",
    "        \n",
    "        print(f\"  {industry}: {len(industry_df)} leads -> {output_path}\")\n",
    "\n",
    "# Uncomment to run:\n",
    "# export_by_industry()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "yp-scraper-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
