{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "291321e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Merged 1 files ‚Üí 89 unique leads\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import re\n",
    "import random\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "from openpyxl import load_workbook\n",
    "from openpyxl.worksheet.datavalidation import DataValidation\n",
    "\n",
    "# ============== CONFIGURATION ==============\n",
    "# Targeting: Delis, corner stores, sandwich shops, restaurants needing digital menu boards\n",
    "\n",
    "# === SEARCH TARGETS ===\n",
    "# Run one at a time or in parallel notebooks\n",
    "SEARCHES = [\n",
    "    {\"term\": \"delis\", \"label\": \"Deli\"},\n",
    "    {\"term\": \"delicatessen\", \"label\": \"Deli\"},\n",
    "    {\"term\": \"sandwich-shops\", \"label\": \"Sandwich Shop\"},\n",
    "    {\"term\": \"bodegas\", \"label\": \"Bodega\"},\n",
    "    {\"term\": \"grocery-stores\", \"label\": \"Grocery/Corner Store\"},\n",
    "    {\"term\": \"convenience-stores\", \"label\": \"Convenience Store\"},\n",
    "    {\"term\": \"pizza\", \"label\": \"Pizza Shop\"},\n",
    "    {\"term\": \"fast-food-restaurants\", \"label\": \"Fast Food\"},\n",
    "    {\"term\": \"take-out-restaurants\", \"label\": \"Takeout\"},\n",
    "    {\"term\": \"cafes\", \"label\": \"Cafe\"},\n",
    "    {\"term\": \"bakeries\", \"label\": \"Bakery\"},\n",
    "    {\"term\": \"bagels\", \"label\": \"Bagel Shop\"},\n",
    "    {\"term\": \"coffee-shops\", \"label\": \"Coffee Shop\"},\n",
    "    {\"term\": \"juice-bars\", \"label\": \"Juice Bar\"},\n",
    "    {\"term\": \"ice-cream-parlors\", \"label\": \"Ice Cream\"},\n",
    "    {\"term\": \"chinese-restaurants\", \"label\": \"Chinese Restaurant\"},\n",
    "    {\"term\": \"mexican-restaurants\", \"label\": \"Mexican Restaurant\"},\n",
    "    {\"term\": \"caribbean-restaurants\", \"label\": \"Caribbean Restaurant\"},\n",
    "]\n",
    "\n",
    "# === CURRENT SEARCH (change index for different category) ===\n",
    "CURRENT_SEARCH_INDEX = 0\n",
    "SEARCH_TERM = SEARCHES[CURRENT_SEARCH_INDEX][\"term\"]\n",
    "INDUSTRY_LABEL = SEARCHES[CURRENT_SEARCH_INDEX][\"label\"]\n",
    "\n",
    "# === NYC NEIGHBORHOODS ===\n",
    "LOCATIONS = [\n",
    "    \"queens-ny\",\n",
    "    \"brooklyn-ny\",\n",
    "    \"bronx-ny\",\n",
    "    \"manhattan-ny\",\n",
    "    \"staten-island-ny\",\n",
    "    # Specific high-density areas\n",
    "    \"astoria-ny\",\n",
    "    \"flushing-ny\",\n",
    "    \"jackson-heights-ny\",\n",
    "    \"jamaica-ny\",\n",
    "    \"williamsburg-brooklyn-ny\",\n",
    "    \"bushwick-brooklyn-ny\",\n",
    "    \"bed-stuy-brooklyn-ny\",\n",
    "    \"crown-heights-brooklyn-ny\",\n",
    "    \"flatbush-brooklyn-ny\",\n",
    "    \"sunset-park-brooklyn-ny\",\n",
    "    \"bay-ridge-brooklyn-ny\",\n",
    "    \"fordham-bronx-ny\",\n",
    "    \"hunts-point-bronx-ny\",\n",
    "    \"washington-heights-ny\",\n",
    "    \"harlem-ny\",\n",
    "    \"east-harlem-ny\",\n",
    "    \"chinatown-ny\",\n",
    "]\n",
    "\n",
    "# === CURRENT LOCATION (change for different area) ===\n",
    "LOCATION = LOCATIONS[0]\n",
    "\n",
    "# === PAGINATION ===\n",
    "START_PAGE = 1\n",
    "END_PAGE = 3\n",
    "\n",
    "OUTPUT_FILE = f\"yp_menus_{LOCATION}_{SEARCH_TERM}_p{START_PAGE}-{END_PAGE}.xlsx\"\n",
    "\n",
    "# === SETTINGS ===\n",
    "FETCH_EMAILS = True\n",
    "DEBUG = False\n",
    "HEADLESS = False\n",
    "MIN_DELAY = 5\n",
    "MAX_DELAY = 10\n",
    "PAGE_DELAY = 15\n",
    "RESTART_DRIVER_EACH_PAGE = True\n",
    "# ===========================================\n",
    "\n",
    "\n",
    "def create_driver():\n",
    "    options = Options()\n",
    "    \n",
    "    if HEADLESS:\n",
    "        options.add_argument(\"--headless=new\")\n",
    "    \n",
    "    options.add_argument(\"--no-sandbox\")\n",
    "    options.add_argument(\"--disable-dev-shm-usage\")\n",
    "    options.add_argument(\"--disable-gpu\")\n",
    "    options.add_argument(\"--window-size=1920,1080\")\n",
    "    options.add_argument(\"--disable-blink-features=AutomationControlled\")\n",
    "    options.add_argument(\"user-agent=Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36\")\n",
    "    options.add_experimental_option(\"excludeSwitches\", [\"enable-automation\"])\n",
    "    options.add_experimental_option('useAutomationExtension', False)\n",
    "    \n",
    "    driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()), options=options)\n",
    "    driver.execute_script(\"Object.defineProperty(navigator, 'webdriver', {get: () => undefined})\")\n",
    "    return driver\n",
    "\n",
    "\n",
    "def extract_email_from_website(driver, website_url):\n",
    "    \"\"\"Try to find email on company's own website\"\"\"\n",
    "    if not website_url or website_url == \"N/A\":\n",
    "        return \"\"\n",
    "    \n",
    "    try:\n",
    "        if not website_url.startswith(\"http\"):\n",
    "            website_url = \"https://\" + website_url\n",
    "        \n",
    "        time.sleep(random.uniform(1, 2))\n",
    "        driver.get(website_url)\n",
    "        time.sleep(2)\n",
    "        \n",
    "        page_source = driver.page_source\n",
    "        \n",
    "        if \"404\" in driver.title or \"not found\" in driver.title.lower():\n",
    "            return \"\"\n",
    "        \n",
    "        mailto_match = re.search(r'href=[\"\\']mailto:([^\"\\'<>?\\s]+)', page_source, re.IGNORECASE)\n",
    "        if mailto_match:\n",
    "            email = mailto_match.group(1).strip()\n",
    "            if '@' in email:\n",
    "                return email\n",
    "        \n",
    "        email_matches = re.findall(r'[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\\.[a-zA-Z]{2,}', page_source)\n",
    "        for email in email_matches:\n",
    "            email_lower = email.lower()\n",
    "            if not any(x in email_lower for x in ['example.com', 'domain.com', 'email.com', 'yoursite', \n",
    "                                                    'sentry.io', 'schema.org', 'json', 'wixpress', \n",
    "                                                    'googleapis', 'facebook', 'twitter', '.png', '.jpg']):\n",
    "                return email\n",
    "        \n",
    "        base_url = website_url.rstrip('/')\n",
    "        contact_pages = ['/contact', '/contact-us', '/about', '/about-us']\n",
    "        \n",
    "        for contact_path in contact_pages:\n",
    "            try:\n",
    "                driver.get(base_url + contact_path)\n",
    "                time.sleep(1.5)\n",
    "                contact_source = driver.page_source\n",
    "                \n",
    "                mailto_match = re.search(r'href=[\"\\']mailto:([^\"\\'<>?\\s]+)', contact_source, re.IGNORECASE)\n",
    "                if mailto_match:\n",
    "                    email = mailto_match.group(1).strip()\n",
    "                    if '@' in email:\n",
    "                        return email\n",
    "                \n",
    "                email_matches = re.findall(r'[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\\.[a-zA-Z]{2,}', contact_source)\n",
    "                for email in email_matches:\n",
    "                    email_lower = email.lower()\n",
    "                    if not any(x in email_lower for x in ['example.com', 'domain.com', 'email.com', 'yoursite',\n",
    "                                                           'sentry.io', 'schema.org', 'json', 'wixpress',\n",
    "                                                           'googleapis', 'facebook', 'twitter', '.png', '.jpg']):\n",
    "                        return email\n",
    "            except:\n",
    "                continue\n",
    "                \n",
    "    except Exception:\n",
    "        pass\n",
    "    \n",
    "    return \"\"\n",
    "\n",
    "\n",
    "def extract_email_from_detail(driver, detail_url, website_url=\"\", debug_save=False):\n",
    "    \"\"\"Try Yellow Pages first, then fall back to company website\"\"\"\n",
    "    try:\n",
    "        time.sleep(random.uniform(MIN_DELAY, MAX_DELAY))\n",
    "        driver.get(detail_url)\n",
    "        \n",
    "        try:\n",
    "            WebDriverWait(driver, 10).until(\n",
    "                EC.presence_of_element_located((By.CSS_SELECTOR, \".business-info, .sales-info, #main-content, #cf-wrapper\"))\n",
    "            )\n",
    "        except:\n",
    "            pass\n",
    "        \n",
    "        time.sleep(2)\n",
    "        page_source = driver.page_source\n",
    "        \n",
    "        is_blocked = \"you have been blocked\" in page_source.lower() or (\"cloudflare\" in page_source.lower() and \"ray id\" in page_source.lower())\n",
    "        \n",
    "        if is_blocked:\n",
    "            if website_url:\n",
    "                print(\" [YP blocked, trying website]\", end=\"\")\n",
    "                email = extract_email_from_website(driver, website_url)\n",
    "                if email:\n",
    "                    return email\n",
    "            return \"__BLOCKED__\"\n",
    "        \n",
    "        if debug_save:\n",
    "            with open(\"debug_page_source.html\", \"w\", encoding=\"utf-8\") as f:\n",
    "                f.write(page_source)\n",
    "            print(f\" [DEBUG: saved]\", end=\"\")\n",
    "        \n",
    "        driver.execute_script(\"window.scrollTo(0, 800);\")\n",
    "        time.sleep(1)\n",
    "        page_source = driver.page_source\n",
    "        \n",
    "        mailto_match = re.search(r'href=[\"\\']mailto:([^\"\\'<>?\\s]+)', page_source, re.IGNORECASE)\n",
    "        if mailto_match:\n",
    "            email = mailto_match.group(1).strip()\n",
    "            if '@' in email and 'yellowpages' not in email.lower():\n",
    "                return email\n",
    "        \n",
    "        try:\n",
    "            email_elements = driver.find_elements(By.CSS_SELECTOR, \"a.email-business, a[class*='email']\")\n",
    "            for el in email_elements:\n",
    "                href = el.get_attribute(\"href\") or \"\"\n",
    "                if \"mailto:\" in href:\n",
    "                    return href.replace(\"mailto:\", \"\").split(\"?\")[0].strip()\n",
    "        except:\n",
    "            pass\n",
    "        \n",
    "        try:\n",
    "            mailto_links = driver.find_elements(By.CSS_SELECTOR, \"a[href*='mailto:']\")\n",
    "            for link in mailto_links:\n",
    "                href = link.get_attribute(\"href\") or \"\"\n",
    "                if \"mailto:\" in href:\n",
    "                    email = href.replace(\"mailto:\", \"\").split(\"?\")[0].strip()\n",
    "                    if '@' in email:\n",
    "                        return email\n",
    "        except:\n",
    "            pass\n",
    "        \n",
    "        soup = BeautifulSoup(page_source, \"html.parser\")\n",
    "        for link in soup.find_all(\"a\", href=True):\n",
    "            href = link[\"href\"]\n",
    "            if \"mailto:\" in href:\n",
    "                email = href.replace(\"mailto:\", \"\").split(\"?\")[0].strip()\n",
    "                if '@' in email and 'yellowpages' not in email.lower():\n",
    "                    return email\n",
    "        \n",
    "        email_pattern = re.search(r'[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\\.[a-zA-Z]{2,}', page_source)\n",
    "        if email_pattern:\n",
    "            email = email_pattern.group()\n",
    "            if not any(x in email.lower() for x in ['example.com', 'domain.com', 'yellowpages', 'schema.org', 'json']):\n",
    "                return email\n",
    "        \n",
    "        if website_url:\n",
    "            print(\" [trying website]\", end=\"\")\n",
    "            email = extract_email_from_website(driver, website_url)\n",
    "            if email:\n",
    "                return email\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\" [err: {e}]\", end=\"\")\n",
    "    return \"\"\n",
    "\n",
    "\n",
    "def parse_listing(listing):\n",
    "    try:\n",
    "        company = listing.select_one(\".business-name span\").text.strip()\n",
    "        phone = listing.select_one(\".phones\").text.strip() if listing.select_one(\".phones\") else \"\"\n",
    "        \n",
    "        address_street = listing.select_one(\".street-address\")\n",
    "        address_locality = listing.select_one(\".locality\")\n",
    "        full_address = \" \".join(filter(None, [\n",
    "            address_street.text.strip() if address_street else \"\",\n",
    "            address_locality.text.strip() if address_locality else \"\"\n",
    "        ]))\n",
    "        \n",
    "        # Extract neighborhood from address for targeting\n",
    "        neighborhood = \"\"\n",
    "        if address_locality:\n",
    "            locality_text = address_locality.text.strip()\n",
    "            # Try to extract neighborhood before city\n",
    "            parts = locality_text.split(\",\")\n",
    "            if len(parts) >= 2:\n",
    "                neighborhood = parts[0].strip()\n",
    "        \n",
    "        website_el = listing.select_one(\".track-visit-website\")\n",
    "        website = website_el[\"href\"] if website_el else \"\"\n",
    "        \n",
    "        detail_link = \"https://www.yellowpages.com\" + listing.select_one(\".business-name\")[\"href\"]\n",
    "\n",
    "        return {\n",
    "            \"#\": None,\n",
    "            \"Company Name\": company,\n",
    "            \"Industry\": INDUSTRY_LABEL,\n",
    "            \"Neighborhood\": neighborhood,\n",
    "            \"Contact Name\": \"\",\n",
    "            \"Email Address\": \"\",\n",
    "            \"Phone Number\": phone,\n",
    "            \"Website URL\": website,\n",
    "            \"Address\": full_address,\n",
    "            \"Date Added\": datetime.now().strftime(\"%-m/%-d/%y\"),\n",
    "            \"Date Contacted\": \"\",\n",
    "            \"Source\": detail_link,\n",
    "            \"Notes\": \"\",\n",
    "            \"Has Menu Board\": \"\",  # For qualifying during outreach\n",
    "            \"Interested\": \"\",\n",
    "            \"Called\": \"\",\n",
    "            \"Followed Up\": \"\",\n",
    "            \"Closed\": \"\"\n",
    "        }\n",
    "    except Exception as e:\n",
    "        print(f\"  Skipping listing: {e}\")\n",
    "        return None\n",
    "\n",
    "\n",
    "def add_checkboxes(filepath):\n",
    "    wb = load_workbook(filepath)\n",
    "    ws = wb.active\n",
    "    \n",
    "    checkbox_validation = DataValidation(type=\"list\", formula1='\"‚òê,‚òë\"', allow_blank=True)\n",
    "    ws.add_data_validation(checkbox_validation)\n",
    "    \n",
    "    headers = {cell.value: cell.column for cell in ws[1]}\n",
    "    \n",
    "    for col_name in [\"Has Menu Board\", \"Interested\", \"Called\", \"Followed Up\", \"Closed\"]:\n",
    "        if col_name in headers:\n",
    "            col_idx = headers[col_name]\n",
    "            for row in range(2, ws.max_row + 1):\n",
    "                cell = ws.cell(row=row, column=col_idx)\n",
    "                cell.value = \"‚òê\"\n",
    "                checkbox_validation.add(cell)\n",
    "    \n",
    "    wb.save(filepath)\n",
    "\n",
    "\n",
    "def get_listings_from_page(driver):\n",
    "    \"\"\"Extract all listing data from current page\"\"\"\n",
    "    driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "    time.sleep(2)\n",
    "    \n",
    "    try:\n",
    "        WebDriverWait(driver, 10).until(\n",
    "            EC.presence_of_element_located((By.CSS_SELECTOR, \".result\"))\n",
    "        )\n",
    "    except:\n",
    "        return []\n",
    "    \n",
    "    listings = driver.find_elements(By.CSS_SELECTOR, \".result\")\n",
    "    page_data = []\n",
    "    \n",
    "    for listing in listings:\n",
    "        try:\n",
    "            soup = BeautifulSoup(listing.get_attribute(\"outerHTML\"), \"html.parser\")\n",
    "            parsed = parse_listing(soup)\n",
    "            if parsed:\n",
    "                page_data.append(parsed)\n",
    "        except:\n",
    "            continue\n",
    "    \n",
    "    return page_data\n",
    "\n",
    "\n",
    "def scrape_yellowpages():\n",
    "    base_url = f\"https://www.yellowpages.com/{LOCATION}/{SEARCH_TERM}\"\n",
    "    print(f\"=\" * 60)\n",
    "    print(f\"üçï Digital Menu Board Lead Scraper\")\n",
    "    print(f\"=\" * 60)\n",
    "    print(f\"Category: {INDUSTRY_LABEL} ({SEARCH_TERM})\")\n",
    "    print(f\"Location: {LOCATION}\")\n",
    "    print(f\"Pages: {START_PAGE} to {END_PAGE}\")\n",
    "    print(f\"Fetch emails: {FETCH_EMAILS}\")\n",
    "    print(f\"Output: {OUTPUT_FILE}\")\n",
    "    print(f\"=\" * 60 + \"\\n\")\n",
    "    \n",
    "    driver = create_driver()\n",
    "    all_data = []\n",
    "    \n",
    "    try:\n",
    "        for page in range(START_PAGE, END_PAGE + 1):\n",
    "            try:\n",
    "                url = base_url if page == 1 else f\"{base_url}?page={page}\"\n",
    "                \n",
    "                print(f\"[Page {page}] Loading {url}\")\n",
    "                driver.get(url)\n",
    "                time.sleep(2)\n",
    "                \n",
    "                page_data = get_listings_from_page(driver)\n",
    "                \n",
    "                if not page_data:\n",
    "                    print(f\"  No listings found - stopping pagination\")\n",
    "                    break\n",
    "                \n",
    "                print(f\"  Found {len(page_data)} listings\")\n",
    "                \n",
    "                if FETCH_EMAILS:\n",
    "                    emails_found = 0\n",
    "                    for i, entry in enumerate(page_data):\n",
    "                        try:\n",
    "                            company_short = entry['Company Name'][:35].ljust(35)\n",
    "                            print(f\"  [{i+1:2}/{len(page_data)}] {company_short}\", end=\"\", flush=True)\n",
    "                            \n",
    "                            debug_save = DEBUG and (page == START_PAGE and i == 0)\n",
    "                            email = extract_email_from_detail(\n",
    "                                driver, \n",
    "                                entry[\"Source\"], \n",
    "                                website_url=entry.get(\"Website URL\", \"\"),\n",
    "                                debug_save=debug_save\n",
    "                            )\n",
    "                            \n",
    "                            if email == \"__BLOCKED__\":\n",
    "                                print(f\" ‚Üí (blocked, no website email found)\")\n",
    "                            elif email:\n",
    "                                entry[\"Email Address\"] = email\n",
    "                                emails_found += 1\n",
    "                                print(f\" ‚Üí {email}\")\n",
    "                            else:\n",
    "                                print(f\" ‚Üí (no email)\")\n",
    "                        except Exception as e:\n",
    "                            print(f\" ‚Üí [error: {e}]\")\n",
    "                            continue\n",
    "                    \n",
    "                    print(f\"  Page {page} complete: {emails_found}/{len(page_data)} emails found\\n\")\n",
    "                \n",
    "                all_data.extend(page_data)\n",
    "                \n",
    "                if all_data:\n",
    "                    temp_df = pd.DataFrame(all_data)\n",
    "                    for idx, row in enumerate(all_data, start=1):\n",
    "                        row[\"#\"] = idx\n",
    "                    temp_df.to_excel(OUTPUT_FILE, index=False)\n",
    "                    print(f\"  üíæ Progress saved to {OUTPUT_FILE} ({len(all_data)} leads)\\n\")\n",
    "                \n",
    "                if page < END_PAGE:\n",
    "                    if RESTART_DRIVER_EACH_PAGE:\n",
    "                        print(f\"  üîÑ Restarting browser for fresh session...\")\n",
    "                        try:\n",
    "                            driver.quit()\n",
    "                        except:\n",
    "                            pass\n",
    "                        time.sleep(3)\n",
    "                        driver = create_driver()\n",
    "                    \n",
    "                    delay = random.uniform(PAGE_DELAY, PAGE_DELAY + 5)\n",
    "                    print(f\"  ‚è≥ Waiting {delay:.1f}s before next page...\\n\")\n",
    "                    time.sleep(delay)\n",
    "                    \n",
    "            except Exception as page_error:\n",
    "                print(f\"\\n  ‚ö†Ô∏è Error on page {page}: {page_error}\")\n",
    "                print(f\"  Saving progress and continuing...\\n\")\n",
    "                try:\n",
    "                    driver.quit()\n",
    "                except:\n",
    "                    pass\n",
    "                driver = create_driver()\n",
    "                continue\n",
    "                \n",
    "    except Exception as e:\n",
    "        print(f\"\\n‚ùå Fatal error: {e}\")\n",
    "    finally:\n",
    "        try:\n",
    "            driver.quit()\n",
    "        except:\n",
    "            pass\n",
    "    \n",
    "    if not all_data:\n",
    "        print(\"No data collected!\")\n",
    "        return pd.DataFrame()\n",
    "    \n",
    "    for i, row in enumerate(all_data, start=1):\n",
    "        row[\"#\"] = i\n",
    "\n",
    "    df = pd.DataFrame(all_data)\n",
    "    df.to_excel(OUTPUT_FILE, index=False)\n",
    "    add_checkboxes(OUTPUT_FILE)\n",
    "    \n",
    "    emails_count = sum(1 for row in all_data if row[\"Email Address\"])\n",
    "    print(f\"=\" * 60)\n",
    "    print(f\"‚úÖ COMPLETE!\")\n",
    "    print(f\"=\" * 60)\n",
    "    print(f\"Total leads: {len(all_data)}\")\n",
    "    print(f\"With emails: {emails_count}\")\n",
    "    print(f\"Saved to: {OUTPUT_FILE}\")\n",
    "    print(f\"=\" * 60)\n",
    "    \n",
    "    return df\n",
    "\n",
    "\n",
    "# === BATCH SCRAPER - Run multiple categories/locations ===\n",
    "def batch_scrape(search_indices=None, location_indices=None):\n",
    "    \"\"\"\n",
    "    Run scraper across multiple categories and locations.\n",
    "    \n",
    "    Example:\n",
    "        batch_scrape(search_indices=[0,1,2], location_indices=[0,1])\n",
    "        # Scrapes: delis, delicatessen, sandwich-shops in queens-ny and brooklyn-ny\n",
    "    \"\"\"\n",
    "    global SEARCH_TERM, INDUSTRY_LABEL, LOCATION, OUTPUT_FILE\n",
    "    \n",
    "    search_indices = search_indices or [0]\n",
    "    location_indices = location_indices or [0]\n",
    "    \n",
    "    all_files = []\n",
    "    \n",
    "    for s_idx in search_indices:\n",
    "        for l_idx in location_indices:\n",
    "            SEARCH_TERM = SEARCHES[s_idx][\"term\"]\n",
    "            INDUSTRY_LABEL = SEARCHES[s_idx][\"label\"]\n",
    "            LOCATION = LOCATIONS[l_idx]\n",
    "            OUTPUT_FILE = f\"yp_menus_{LOCATION}_{SEARCH_TERM}_p{START_PAGE}-{END_PAGE}.xlsx\"\n",
    "            \n",
    "            print(f\"\\n{'='*60}\")\n",
    "            print(f\"üîÑ BATCH: {INDUSTRY_LABEL} in {LOCATION}\")\n",
    "            print(f\"{'='*60}\\n\")\n",
    "            \n",
    "            scrape_yellowpages()\n",
    "            all_files.append(OUTPUT_FILE)\n",
    "            \n",
    "            # Longer delay between different searches\n",
    "            time.sleep(random.uniform(30, 60))\n",
    "    \n",
    "    return all_files\n",
    "\n",
    "\n",
    "# === RUN SINGLE SCRAPE ===\n",
    "if __name__ == \"__main__\":\n",
    "    df = scrape_yellowpages()\n",
    "\n",
    "\n",
    "# === MERGE ALL FILES (run separately after all scrapes complete) ===\n",
    "\"\"\"\n",
    "import pandas as pd\n",
    "import glob\n",
    "\n",
    "files = glob.glob(\"yp_menus_*.xlsx\")\n",
    "dfs = []\n",
    "for f in files:\n",
    "    try:\n",
    "        df = pd.read_excel(f)\n",
    "        dfs.append(df)\n",
    "    except:\n",
    "        continue\n",
    "\n",
    "if dfs:\n",
    "    combined = pd.concat(dfs, ignore_index=True)\n",
    "    combined[\"#\"] = range(1, len(combined) + 1)\n",
    "    # Remove duplicates by phone or company name + address\n",
    "    combined.drop_duplicates(subset=[\"Company Name\", \"Phone Number\"], inplace=True)\n",
    "    combined[\"#\"] = range(1, len(combined) + 1)\n",
    "    combined.to_excel(\"yp_menus_all_leads.xlsx\", index=False)\n",
    "    print(f\"Merged {len(files)} files ‚Üí {len(combined)} unique leads\")\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12645f26",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "yp-scraper-env (3.13.1)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
