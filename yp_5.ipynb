{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "458a494a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "B2B SUPPLIER LEAD SCRAPER\n",
      "Focused: Janitorial | Safety | Promo | Uniforms\n",
      "======================================================================\n",
      "Mode: single\n",
      "Current Niche: janitorial\n",
      "Output: exports_b2b_suppliers\n",
      "======================================================================\n",
      "\n",
      "Created output directory: exports_b2b_suppliers\n",
      "\n",
      "MODE: Single Search\n",
      "Niche: Janitorial/Cleaning Supplier\n",
      "Term: janitorial-supplies\n",
      "Location: queens-ny\n",
      "Pitch: Online ordering portal for recurring cleaning supply orders\n",
      "Loaded 0 existing lead IDs for deduplication\n",
      "\n",
      "======================================================================\n",
      "SCRAPING: Janitorial/Cleaning Supplier\n",
      "Search Term: janitorial-supplies\n",
      "Location: queens-ny\n",
      "URL: https://www.yellowpages.com/queens-ny/janitorial-supplies\n",
      "Existing in file: 0\n",
      "======================================================================\n",
      "\n",
      "[Page 1] Loading...\n",
      "  Found 30 listings, 29 new\n",
      "  [ 1/29] CleanZone Inc                          -> BLOCKED\n",
      "  [ 2/29] Sds Janitorial Supplies                -> BLOCKED\n",
      "  [ 3/29] Kitro Supplies Inc                     [blocked, trying website] -> kitrosupplies@yahoo.com\n",
      "  [ 4/29] ALL CITY SUPPLY                        -> BLOCKED\n",
      "  [ 5/29] National Janitorial Supplies           -> BLOCKED\n",
      "  [ 6/29] MBA Supply Company                     [blocked, trying website] -> BLOCKED\n",
      "\n",
      "  Too many blocks - restarting...\n",
      "  [ 7/29] Ace Janitorial Supply                  [trying website] -> (no email)\n",
      "  [ 8/29] Goldsholle & Garfinkel Inc             [blocked, trying website] -> info@benjaminmoore.com\n",
      "  [ 9/29] Sterling Sanitary Supply               [blocked, trying website] -> BLOCKED\n",
      "  [10/29] Western Cleaning Supplies              -> BLOCKED\n",
      "  [11/29] All One Source Supplies                [blocked, trying website] -> BLOCKED\n",
      "  [12/29] Superior Maintenance Supply            [blocked, trying website] -> info@smssupplies.com\n",
      "  [13/29] Primco Solutions                       [blocked, trying website] -> BLOCKED\n",
      "  [14/29] M & M Franco Disposables Inc           [blocked, trying website] -> BLOCKED\n",
      "\n",
      "  Too many blocks - restarting...\n",
      "  [15/29] Bt Supplies Inc                        [trying website] -> (no email)\n",
      "  [16/29] All One Source                         [blocked, trying website] -> BLOCKED\n",
      "  [17/29] Budget Maintenance Supply              [blocked, trying website] -> sales@budgetms.com\n",
      "  [18/29] 386 Third Ave Partners                 [blocked, trying website] -> BLOCKED\n",
      "  [19/29] Long Island Janitor Supply Corp        [blocked, trying website] -> BLOCKED\n",
      "  [20/29] Bags Galore & Janitorial Supply Corp   [blocked, trying website] -> Bagsgaloreco@verizon.net\n",
      "  [21/29] Kitro Supplies Inc                     [blocked, trying website] -> BLOCKED\n",
      "  [22/29] Armac Industries Inc                   [blocked, trying website] -> sales@hub-4.com\n",
      "  [23/29] Twi Laq                                [blocked, trying website] -> BLOCKED\n",
      "\n",
      "  Too many blocks - restarting...\n",
      "  [24/29] Apple Poly Products Inc                [trying website] -> (no email)\n",
      "  [25/29] M&M Franco Disposables Inc             [blocked, trying website] -> BLOCKED\n",
      "  [26/29] Corp Leon Sales                        [blocked, trying website] -> BLOCKED\n",
      "  [27/29] Anbro Supl Co                          [blocked, trying website] -> BLOCKED\n",
      "  [28/29] UniFirst Uniforms - Metro NY           [blocked, trying website] -> BLOCKED\n",
      "  [29/29] Advantage Wholesale Supply             [blocked, trying website] -> info@awsupply.com\n",
      "\n",
      "  Page 1: 7/29 emails\n",
      "  Saved 29 leads to exports_b2b_suppliers/yp_janitorial_queens-ny_janitorial-supplies.xlsx\n",
      "  Restarting browser...\n",
      "  Waiting 15.0s...\n",
      "\n",
      "[Page 2] Loading...\n",
      "  Found 30 listings, 30 new\n",
      "  [ 1/30] Roosevelt Building Maintenance Co Inc  [blocked, trying website] -> BLOCKED\n",
      "\n",
      "  Too many blocks - restarting...\n",
      "  [ 2/30] Paramount Paper & Plastic Corp         [trying website] -> (no email)\n",
      "  [ 3/30] Ram Industries                         [blocked, trying website] -> BLOCKED\n",
      "  [ 4/30] Quality Supply                         [blocked, trying website] -> BLOCKED\n",
      "  [ 5/30] Continental Building Service Inc       [blocked, trying website] -> BLOCKED\n",
      "  [ 6/30] MBA Supply                             [blocked, trying website] -> BLOCKED\n",
      "  [ 7/30] Quality Supply                         [blocked, trying website] -> BLOCKED\n",
      "\n",
      "  Too many blocks - restarting...\n",
      "  [ 8/30] Kew Forest Maintenance Supply          -> info@kewforest.com\n",
      "  [ 9/30] ABC Paper & Grocery Inc                [blocked, trying website] -> BLOCKED\n",
      "  [10/30] Edmar Janitorial Supplies              [blocked, trying website] -> BLOCKED\n",
      "  [11/30] City Wide Paper & Specialty Products C [blocked, trying website] -> BLOCKED\n",
      "  [12/30] Fordham Supply Co Inc                  [blocked, trying website] -> BLOCKED\n",
      "  [13/30] Imperial Dade                          [blocked, trying website] -> BLOCKED\n",
      "\n",
      "  Too many blocks - restarting...\n",
      "  [14/30] Simplicity Group                       -> (no email)\n",
      "  [15/30] Brag Sales                             [blocked, trying website] -> BLOCKED\n",
      "  [16/30] J Frankel Distributors Inc             [blocked, trying website] -> BLOCKED\n",
      "  [17/30] Advantage Wholesale Supply             [blocked, trying website] -> info@awsupply.com\n",
      "  [18/30] Pacoa                                  [blocked, trying website] -> BLOCKED\n",
      "  [19/30] Primco Solutions Inc                   [blocked, trying website] -> BLOCKED\n",
      "  [20/30] Favor Me                               [blocked, trying website] -> BLOCKED\n",
      "\n",
      "  Too many blocks - restarting...\n",
      "  [21/30] Elmax Builders Supply Co               -> elmaxbuildersupply@msn.com\n",
      "  [22/30] Perkaroma                              [blocked, trying website] -> BLOCKED\n",
      "  [23/30] Clean Eco Maintenance Corp.            [blocked, trying website] -> jperalta02@aol.com\n",
      "  [24/30] Budget Maintenance                     [blocked, trying website] -> BLOCKED\n",
      "  [25/30] Pathe Shipping Supplies Co             [blocked, trying website] -> sales@pathesupplies.com\n",
      "  [26/30] POC Commercial Cleaning Solutions      [blocked, trying website] -> info@poccorp.net\n",
      "  [27/30] Imperial Bag and Paper                 [blocked, trying website] -> BLOCKED\n",
      "  [28/30] Steve Supply Corp                      [blocked, trying website] -> BLOCKED\n",
      "  [29/30] Liberty Paper & Janitorial Spl         [blocked, trying website] -> BLOCKED\n",
      "\n",
      "  Too many blocks - restarting...\n",
      "  [30/30] Osha Industrial Supplies Inc           [trying website] -> (no email)\n",
      "\n",
      "  Page 2: 6/30 emails\n",
      "  Saved 59 leads to exports_b2b_suppliers/yp_janitorial_queens-ny_janitorial-supplies.xlsx\n",
      "  Restarting browser...\n",
      "  Waiting 14.2s...\n",
      "\n",
      "[Page 3] Loading...\n",
      "  Found 30 listings, 30 new\n",
      "  [ 1/30] Brighton USA LTD                       [blocked, trying website] -> BLOCKED\n",
      "  [ 2/30] Jose Martinez                          [blocked, trying website] -> BLOCKED\n",
      "  [ 3/30] Sunrise Inc                            [blocked, trying website] -> BLOCKED\n",
      "  [ 4/30] Wilmon Industrial Supply Corp          [blocked, trying website] -> BLOCKED\n",
      "  [ 5/30] Sparkle Supl Inc                       [blocked, trying website] -> BLOCKED\n",
      "\n",
      "  Too many blocks - restarting...\n",
      "  [ 6/30] Tor-Pro Industries Inc                 [trying website] -> (no email)\n",
      "  [ 7/30] Cleaning Systems & Supply Co Inc       [blocked, trying website] -> BLOCKED\n",
      "  [ 8/30] Janitorial Supply Co                   -> BLOCKED\n",
      "  [ 9/30] Sanitary Supply Specialties            [blocked, trying website] -> BLOCKED\n",
      "  [10/30] Klenzcorp Klenzcorp                    [blocked, trying website] -> BLOCKED\n",
      "  [11/30] Bcb Janitorial Office                  [blocked, trying website] -> BLOCKED\n",
      "\n",
      "  Too many blocks - restarting...\n",
      "  [12/30] Sanaid                                 -> (no email)\n",
      "  [13/30] John Earl Inc                          [blocked, trying website] -> email@johnearl.com\n",
      "  [14/30] Yonkers Hardware Janitorial Supply Inc [blocked, trying website] -> BLOCKED\n",
      "  [15/30] Waldike Co                             [blocked, trying website] -> info@nationalmaintenance.com\n",
      "  [16/30] Cyberclenz                             [blocked, trying website] -> BLOCKED\n",
      "  [17/30] Galician Bar & Restaurant Supplies     [blocked, trying website] -> BLOCKED\n",
      "  [18/30] A BEE Services                         [blocked, trying website] -> BLOCKED\n",
      "  [19/30] Star Supply                            -> BLOCKED\n",
      "\n",
      "  Too many blocks - restarting...\n",
      "  [20/30] Clean Enterprises                      [trying website] -> (no email)\n",
      "  [21/30] Elite Restaurant Equipment, Inc.       [blocked, trying website] -> BLOCKED\n",
      "  [22/30] Mormax Co                              [blocked, trying website] -> sales@mormax.com\n",
      "  [23/30] Amsterdam Supply Co                    [blocked, trying website] -> BLOCKED\n",
      "  [24/30] Every Supply Co Inc                    [blocked, trying website] -> BLOCKED\n",
      "  [25/30] Better Pak Corp                        [blocked, trying website] -> info@betterpak.com\n",
      "  [26/30] Westwood Building Maintenance & Suppli [blocked, trying website] -> BLOCKED\n",
      "  [27/30] Sterling Sani                          -> BLOCKED\n",
      "\n",
      "  Too many blocks - restarting...\n",
      "  [28/30] Rmac Supplies Inc                      [trying website] -> orders@rmacsupplies.com\n",
      "  [29/30] Shield Line                            [blocked, trying website] -> BLOCKED\n",
      "  [30/30] Ace Janitorial Supply                  [blocked, trying website] -> BLOCKED\n",
      "\n",
      "  Page 3: 5/30 emails\n",
      "  Saved 89 leads to exports_b2b_suppliers/yp_janitorial_queens-ny_janitorial-supplies.xlsx\n",
      "  Restarting browser...\n",
      "  Waiting 13.9s...\n",
      "\n",
      "[Page 4] Loading...\n",
      "  Found 21 listings, 21 new\n",
      "  [ 1/21] Ted Supply                             [blocked, trying website] -> BLOCKED\n",
      "  [ 2/21] Walmarc Industrial Wipers              [blocked, trying website] -> BLOCKED\n",
      "  [ 3/21] Industrial Distributors Inc            [blocked, trying website] -> BLOCKED\n",
      "\n",
      "  Too many blocks - restarting...\n",
      "  [ 4/21] Blue DOT Janitorial                    [trying website] -> (no email)\n",
      "  [ 5/21] TEC Products Co Inc                    [blocked, trying website] -> team@techproducts.com\n",
      "  [ 6/21] Spruce Industries                      [blocked, trying website] -> customer.service@spruceindustries.com\n",
      "  [ 7/21] All Island Janitorial Supply           [blocked, trying website] -> BLOCKED\n",
      "  [ 8/21] Gemini Supl Corp                       [blocked, trying website] -> BLOCKED\n",
      "  [ 9/21] Lodging Kits                           [blocked, trying website] -> BLOCKED\n",
      "  [10/21] Alexander Brown Co Inc                 [blocked, trying website] -> BLOCKED\n",
      "  [11/21] Bergo Janitorial Supply Inc            [blocked, trying website] -> bill@bergosupply.com\n",
      "  [12/21] Amityville Feed Supply                 [blocked, trying website] -> BLOCKED\n",
      "\n",
      "  Too many blocks - restarting...\n",
      "  [13/21] Circle D, Janitoral supplies           -> squiles@circlejanitorial.com\n",
      "  [14/21] Cleaning Stop                          [blocked, trying website] -> BLOCKED\n",
      "  [15/21] American Hygiene                       [blocked, trying website] -> BLOCKED\n",
      "  [16/21] New World Linen                        [blocked, trying website] -> newworldlinen@gmail.com\n",
      "  [17/21] Nationwide Janitorial Service          [blocked, trying website] -> BLOCKED\n",
      "  [18/21] E.M. Office Maintenance                -> BLOCKED\n",
      "  [19/21] B & C Vacuum Cleaner's                 [blocked, trying website] -> BLOCKED\n",
      "\n",
      "  Too many blocks - restarting...\n",
      "  [20/21] Crown Janitorial Products              [trying website] -> usa@crown-products.com\n",
      "  [21/21] Kammson Industries                     [blocked, trying website] -> BLOCKED\n",
      "\n",
      "  Page 4: 6/21 emails\n",
      "  Saved 110 leads to exports_b2b_suppliers/yp_janitorial_queens-ny_janitorial-supplies.xlsx\n",
      "  Restarting browser...\n",
      "  Waiting 16.7s...\n",
      "\n",
      "[Page 5] Loading...\n",
      "  No listings - end of results\n",
      "\n",
      "======================================================================\n",
      "COMPLETED: janitorial-supplies in queens-ny\n",
      "New leads: 110\n",
      "Total in file: 110\n",
      "With emails: 24\n",
      "With websites: 99\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "B2B Service Suppliers Yellow Pages Scraper\n",
    "===========================================\n",
    "Target: B2B suppliers that need custom web apps for:\n",
    "- Online ordering portals with customer accounts\n",
    "- Custom quoting/proof approval workflows\n",
    "- Recurring order management\n",
    "- Large catalog management with tiered pricing\n",
    "\n",
    "Focused Niches:\n",
    "1. Janitorial/Cleaning Suppliers\n",
    "2. Industrial Safety Suppliers\n",
    "3. Promotional Products/Print Shops\n",
    "4. Uniform/Workwear Distributors\n",
    "\n",
    "These businesses typically:\n",
    "- Serve other businesses (B2B)\n",
    "- Have complex pricing (volume tiers, customer-specific)\n",
    "- Need recurring order functionality\n",
    "- Often still use phone/fax/email for orders\n",
    "- Have budget for custom solutions\n",
    "\"\"\"\n",
    "\n",
    "import time\n",
    "import re\n",
    "import random\n",
    "import os\n",
    "import json\n",
    "import hashlib\n",
    "from datetime import datetime\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "from openpyxl import load_workbook\n",
    "from openpyxl.worksheet.datavalidation import DataValidation\n",
    "\n",
    "# ============================================================================\n",
    "#                              CONFIGURATION\n",
    "# ============================================================================\n",
    "\n",
    "# === MODE: Choose how to run ===\n",
    "# \"single\"    - Run one search term + one location\n",
    "# \"niche\"     - Run one NICHE (all its search terms) across all locations\n",
    "# \"all\"       - Run everything (comprehensive)\n",
    "MODE = \"single\"\n",
    "\n",
    "# === FOCUSED B2B NICHES ===\n",
    "# Each niche has multiple search terms to maximize coverage\n",
    "\n",
    "NICHES = {\n",
    "    \"janitorial\": {\n",
    "        \"label\": \"Janitorial/Cleaning Supplier\",\n",
    "        \"terms\": [\n",
    "            \"janitorial-supplies\",\n",
    "            \"janitorial-equipment-supplies\",\n",
    "            \"cleaning-supplies\",\n",
    "            \"cleaning-equipment-supplies\",\n",
    "            \"sanitation-supplies\",\n",
    "            \"paper-products-wholesale\",\n",
    "            \"commercial-cleaning-supplies\",\n",
    "            \"floor-care-supplies\",\n",
    "            \"restroom-supplies\",\n",
    "        ],\n",
    "        \"pitch\": \"Online ordering portal for recurring cleaning supply orders\"\n",
    "    },\n",
    "    \"safety\": {\n",
    "        \"label\": \"Industrial Safety Supplier\",\n",
    "        \"terms\": [\n",
    "            \"safety-equipment-supplies\",\n",
    "            \"industrial-safety-equipment\",\n",
    "            \"personal-protective-equipment\",\n",
    "            \"ppe-supplies\",\n",
    "            \"industrial-supplies\",\n",
    "            \"welding-supplies\",\n",
    "            \"industrial-equipment-supplies\",\n",
    "            \"fire-protection-equipment\",\n",
    "            \"first-aid-supplies\",\n",
    "        ],\n",
    "        \"pitch\": \"B2B catalog with compliance docs and account pricing\"\n",
    "    },\n",
    "    \"promo\": {\n",
    "        \"label\": \"Promotional Products/Print\",\n",
    "        \"terms\": [\n",
    "            \"promotional-products\",\n",
    "            \"advertising-specialties\",\n",
    "            \"screen-printing\",\n",
    "            \"custom-printing\",\n",
    "            \"printing-services-commercial\",\n",
    "            \"signs\",\n",
    "            \"banners\",\n",
    "            \"trophies-awards\",\n",
    "            \"embroidery\",\n",
    "            \"business-forms\",\n",
    "            \"custom-t-shirts\",\n",
    "        ],\n",
    "        \"pitch\": \"Custom quote builder with proof approval workflow\"\n",
    "    },\n",
    "    \"uniforms\": {\n",
    "        \"label\": \"Uniform/Workwear Distributor\",\n",
    "        \"terms\": [\n",
    "            \"uniforms\",\n",
    "            \"uniform-supply\",\n",
    "            \"work-clothing\",\n",
    "            \"corporate-apparel\",\n",
    "            \"industrial-uniforms\",\n",
    "            \"medical-scrubs\",\n",
    "            \"restaurant-uniforms\",\n",
    "            \"embroidery-services\",\n",
    "            \"work-boots-shoes\",\n",
    "            \"safety-clothing\",\n",
    "        ],\n",
    "        \"pitch\": \"Company accounts with logo/embroidery options and reordering\"\n",
    "    },\n",
    "}\n",
    "\n",
    "# === CURRENT SELECTION ===\n",
    "# For \"single\" mode: set both niche and term index\n",
    "# For \"niche\" mode: set just the niche key\n",
    "CURRENT_NICHE = \"janitorial\"  # Options: janitorial, safety, promo, uniforms\n",
    "CURRENT_TERM_INDEX = 0        # Which term within the niche (for single mode)\n",
    "CURRENT_LOCATION_INDEX = 0    # Which location (for single mode)\n",
    "\n",
    "# === LOCATIONS ===\n",
    "# Focused on tri-state area + expanded to other industrial regions\n",
    "# These businesses exist everywhere, not just Manhattan\n",
    "LOCATIONS = [\n",
    "    # NYC Boroughs (industrial areas)\n",
    "    \"queens-ny\",\n",
    "    \"brooklyn-ny\",\n",
    "    \"bronx-ny\",\n",
    "    \"staten-island-ny\",\n",
    "\n",
    "    # Queens industrial\n",
    "    \"long-island-city-ny\",\n",
    "    \"maspeth-ny\",\n",
    "    \"jamaica-ny\",\n",
    "    \"college-point-ny\",\n",
    "\n",
    "    # Brooklyn industrial\n",
    "    \"sunset-park-brooklyn-ny\",\n",
    "    \"red-hook-brooklyn-ny\",\n",
    "    \"east-new-york-brooklyn-ny\",\n",
    "\n",
    "    # Bronx industrial\n",
    "    \"hunts-point-bronx-ny\",\n",
    "    \"port-morris-bronx-ny\",\n",
    "    \"south-bronx-ny\",\n",
    "\n",
    "    # Long Island (lots of suppliers here)\n",
    "    \"long-island-ny\",\n",
    "    \"nassau-county-ny\",\n",
    "    \"suffolk-county-ny\",\n",
    "    \"hauppauge-ny\",\n",
    "    \"farmingdale-ny\",\n",
    "    \"hicksville-ny\",\n",
    "    \"westbury-ny\",\n",
    "\n",
    "    # Westchester/Hudson Valley\n",
    "    \"westchester-county-ny\",\n",
    "    \"yonkers-ny\",\n",
    "    \"white-plains-ny\",\n",
    "    \"mount-vernon-ny\",\n",
    "\n",
    "    # New Jersey (huge industrial base)\n",
    "    \"newark-nj\",\n",
    "    \"jersey-city-nj\",\n",
    "    \"elizabeth-nj\",\n",
    "    \"edison-nj\",\n",
    "    \"paterson-nj\",\n",
    "    \"clifton-nj\",\n",
    "    \"passaic-nj\",\n",
    "    \"union-nj\",\n",
    "    \"secaucus-nj\",\n",
    "    \"kearny-nj\",\n",
    "    \"linden-nj\",\n",
    "    \"perth-amboy-nj\",\n",
    "    \"new-brunswick-nj\",\n",
    "    \"middlesex-county-nj\",\n",
    "    \"bergen-county-nj\",\n",
    "    \"essex-county-nj\",\n",
    "    \"hudson-county-nj\",\n",
    "\n",
    "    # Connecticut\n",
    "    \"stamford-ct\",\n",
    "    \"bridgeport-ct\",\n",
    "    \"new-haven-ct\",\n",
    "    \"hartford-ct\",\n",
    "    \"waterbury-ct\",\n",
    "    \"norwalk-ct\",\n",
    "    \"fairfield-county-ct\",\n",
    "]\n",
    "\n",
    "# === PAGINATION ===\n",
    "START_PAGE = 1\n",
    "MAX_PAGES = 5  # Most searches won't have more than this\n",
    "\n",
    "# === OUTPUT ===\n",
    "OUTPUT_DIR = \"exports_b2b_suppliers\"\n",
    "PROGRESS_FILE = \"scrape_progress.json\"\n",
    "\n",
    "# === SCRAPING SETTINGS ===\n",
    "FETCH_EMAILS = True\n",
    "DEBUG = False\n",
    "HEADLESS = False\n",
    "MIN_DELAY = 4\n",
    "MAX_DELAY = 8\n",
    "PAGE_DELAY = 12\n",
    "LISTING_DELAY = 3\n",
    "RESTART_DRIVER_EACH_PAGE = True\n",
    "MAX_RETRIES = 3\n",
    "\n",
    "# === USER AGENTS ===\n",
    "USER_AGENTS = [\n",
    "    \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/121.0.0.0 Safari/537.36\",\n",
    "    \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/121.0.0.0 Safari/537.36\",\n",
    "    \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/17.2 Safari/605.1.15\",\n",
    "    \"Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:122.0) Gecko/20100101 Firefox/122.0\",\n",
    "    \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10.15; rv:122.0) Gecko/20100101 Firefox/122.0\",\n",
    "]\n",
    "\n",
    "# ============================================================================\n",
    "#                              HELPER FUNCTIONS\n",
    "# ============================================================================\n",
    "\n",
    "def ensure_output_dir():\n",
    "    if not os.path.exists(OUTPUT_DIR):\n",
    "        os.makedirs(OUTPUT_DIR)\n",
    "        print(f\"Created output directory: {OUTPUT_DIR}\")\n",
    "\n",
    "\n",
    "def get_output_filename(niche_key, search_term, location):\n",
    "    return os.path.join(OUTPUT_DIR, f\"yp_{niche_key}_{location}_{search_term}.xlsx\")\n",
    "\n",
    "\n",
    "def generate_lead_id(company_name, phone):\n",
    "    key = f\"{company_name.lower().strip()}|{phone.strip()}\"\n",
    "    return hashlib.md5(key.encode()).hexdigest()[:12]\n",
    "\n",
    "\n",
    "def load_existing_leads(filepath):\n",
    "    if os.path.exists(filepath):\n",
    "        try:\n",
    "            df = pd.read_excel(filepath)\n",
    "            return set(\n",
    "                generate_lead_id(str(row.get(\"Company Name\", \"\")), str(row.get(\"Phone Number\", \"\")))\n",
    "                for _, row in df.iterrows()\n",
    "            )\n",
    "        except:\n",
    "            return set()\n",
    "    return set()\n",
    "\n",
    "\n",
    "def load_all_existing_lead_ids():\n",
    "    all_ids = set()\n",
    "    if os.path.exists(OUTPUT_DIR):\n",
    "        for filename in os.listdir(OUTPUT_DIR):\n",
    "            if filename.endswith(\".xlsx\"):\n",
    "                filepath = os.path.join(OUTPUT_DIR, filename)\n",
    "                all_ids.update(load_existing_leads(filepath))\n",
    "    print(f\"Loaded {len(all_ids)} existing lead IDs for deduplication\")\n",
    "    return all_ids\n",
    "\n",
    "\n",
    "def save_progress(niche, term_idx, location_idx, status=\"in_progress\"):\n",
    "    progress = {\n",
    "        \"niche\": niche,\n",
    "        \"term_index\": term_idx,\n",
    "        \"location_index\": location_idx,\n",
    "        \"status\": status,\n",
    "        \"timestamp\": datetime.now().isoformat()\n",
    "    }\n",
    "    with open(PROGRESS_FILE, \"w\") as f:\n",
    "        json.dump(progress, f, indent=2)\n",
    "\n",
    "\n",
    "def load_progress():\n",
    "    if os.path.exists(PROGRESS_FILE):\n",
    "        with open(PROGRESS_FILE, \"r\") as f:\n",
    "            return json.load(f)\n",
    "    return None\n",
    "\n",
    "\n",
    "def create_driver():\n",
    "    options = Options()\n",
    "\n",
    "    if HEADLESS:\n",
    "        options.add_argument(\"--headless=new\")\n",
    "\n",
    "    options.add_argument(\"--no-sandbox\")\n",
    "    options.add_argument(\"--disable-dev-shm-usage\")\n",
    "    options.add_argument(\"--disable-gpu\")\n",
    "    options.add_argument(\"--window-size=1920,1080\")\n",
    "    options.add_argument(\"--disable-blink-features=AutomationControlled\")\n",
    "\n",
    "    user_agent = random.choice(USER_AGENTS)\n",
    "    options.add_argument(f\"user-agent={user_agent}\")\n",
    "\n",
    "    options.add_experimental_option(\"excludeSwitches\", [\"enable-automation\"])\n",
    "    options.add_experimental_option('useAutomationExtension', False)\n",
    "\n",
    "    driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()), options=options)\n",
    "    driver.execute_script(\"Object.defineProperty(navigator, 'webdriver', {get: () => undefined})\")\n",
    "\n",
    "    return driver\n",
    "\n",
    "\n",
    "def random_delay(min_sec=None, max_sec=None):\n",
    "    min_sec = min_sec or MIN_DELAY\n",
    "    max_sec = max_sec or MAX_DELAY\n",
    "    delay = random.uniform(min_sec, max_sec)\n",
    "    time.sleep(delay)\n",
    "    return delay\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "#                           EMAIL EXTRACTION\n",
    "# ============================================================================\n",
    "\n",
    "EMAIL_BLACKLIST = [\n",
    "    'example.com', 'domain.com', 'email.com', 'yoursite', 'yourdomain',\n",
    "    'sentry.io', 'schema.org', 'json', 'wixpress', 'wix.com',\n",
    "    'googleapis', 'google.com', 'facebook', 'twitter', 'instagram',\n",
    "    '.png', '.jpg', '.gif', '.svg', '.css', '.js',\n",
    "    'yellowpages', 'yp.com', 'placeholder', 'test.com',\n",
    "    'wordpress', 'squarespace', 'shopify', 'godaddy', 'wufoo'\n",
    "]\n",
    "\n",
    "\n",
    "def is_valid_email(email):\n",
    "    if not email or '@' not in email:\n",
    "        return False\n",
    "    email_lower = email.lower()\n",
    "    return not any(x in email_lower for x in EMAIL_BLACKLIST)\n",
    "\n",
    "\n",
    "def extract_email_from_website(driver, website_url, timeout=15):\n",
    "    if not website_url or website_url == \"N/A\":\n",
    "        return \"\"\n",
    "\n",
    "    try:\n",
    "        if not website_url.startswith(\"http\"):\n",
    "            website_url = \"https://\" + website_url\n",
    "\n",
    "        random_delay(1, 2)\n",
    "        driver.set_page_load_timeout(timeout)\n",
    "\n",
    "        try:\n",
    "            driver.get(website_url)\n",
    "        except:\n",
    "            return \"\"\n",
    "\n",
    "        time.sleep(2)\n",
    "        page_source = driver.page_source\n",
    "\n",
    "        if any(x in driver.title.lower() for x in [\"404\", \"not found\", \"error\", \"denied\"]):\n",
    "            return \"\"\n",
    "\n",
    "        # Method 1: Mailto links\n",
    "        mailto_match = re.search(r'href=[\"\\']mailto:([^\"\\'<>?\\s]+)', page_source, re.IGNORECASE)\n",
    "        if mailto_match:\n",
    "            email = mailto_match.group(1).strip()\n",
    "            if is_valid_email(email):\n",
    "                return email\n",
    "\n",
    "        # Method 2: Email patterns\n",
    "        email_matches = re.findall(r'[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\\.[a-zA-Z]{2,}', page_source)\n",
    "        for email in email_matches:\n",
    "            if is_valid_email(email):\n",
    "                return email\n",
    "\n",
    "        # Method 3: Contact pages\n",
    "        base_url = website_url.rstrip('/')\n",
    "        contact_paths = ['/contact', '/contact-us', '/about', '/about-us', '/contactus']\n",
    "\n",
    "        for path in contact_paths:\n",
    "            try:\n",
    "                driver.get(base_url + path)\n",
    "                time.sleep(1.5)\n",
    "                contact_source = driver.page_source\n",
    "\n",
    "                mailto_match = re.search(r'href=[\"\\']mailto:([^\"\\'<>?\\s]+)', contact_source, re.IGNORECASE)\n",
    "                if mailto_match:\n",
    "                    email = mailto_match.group(1).strip()\n",
    "                    if is_valid_email(email):\n",
    "                        return email\n",
    "\n",
    "                email_matches = re.findall(r'[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\\.[a-zA-Z]{2,}', contact_source)\n",
    "                for email in email_matches:\n",
    "                    if is_valid_email(email):\n",
    "                        return email\n",
    "            except:\n",
    "                continue\n",
    "\n",
    "    except Exception as e:\n",
    "        if DEBUG:\n",
    "            print(f\" [website error: {e}]\", end=\"\")\n",
    "\n",
    "    return \"\"\n",
    "\n",
    "\n",
    "def extract_email_from_detail(driver, detail_url, website_url=\"\", debug_save=False):\n",
    "    try:\n",
    "        random_delay(LISTING_DELAY, LISTING_DELAY + 2)\n",
    "        driver.get(detail_url)\n",
    "\n",
    "        try:\n",
    "            WebDriverWait(driver, 10).until(\n",
    "                EC.presence_of_element_located((By.CSS_SELECTOR, \".business-info, .sales-info, #main-content, #cf-wrapper\"))\n",
    "            )\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "        time.sleep(2)\n",
    "        page_source = driver.page_source\n",
    "\n",
    "        is_blocked = (\n",
    "            \"you have been blocked\" in page_source.lower() or\n",
    "            (\"cloudflare\" in page_source.lower() and \"ray id\" in page_source.lower())\n",
    "        )\n",
    "\n",
    "        if is_blocked:\n",
    "            if website_url:\n",
    "                print(\" [blocked, trying website]\", end=\"\")\n",
    "                email = extract_email_from_website(driver, website_url)\n",
    "                if email:\n",
    "                    return email\n",
    "            return \"__BLOCKED__\"\n",
    "\n",
    "        if debug_save:\n",
    "            with open(os.path.join(OUTPUT_DIR, \"debug_page.html\"), \"w\", encoding=\"utf-8\") as f:\n",
    "                f.write(page_source)\n",
    "\n",
    "        driver.execute_script(\"window.scrollTo(0, 800);\")\n",
    "        time.sleep(1)\n",
    "        page_source = driver.page_source\n",
    "\n",
    "        # Method 1: Mailto in source\n",
    "        mailto_match = re.search(r'href=[\"\\']mailto:([^\"\\'<>?\\s]+)', page_source, re.IGNORECASE)\n",
    "        if mailto_match:\n",
    "            email = mailto_match.group(1).strip()\n",
    "            if is_valid_email(email):\n",
    "                return email\n",
    "\n",
    "        # Method 2: Email link elements\n",
    "        try:\n",
    "            email_elements = driver.find_elements(By.CSS_SELECTOR, \"a.email-business, a[class*='email']\")\n",
    "            for el in email_elements:\n",
    "                href = el.get_attribute(\"href\") or \"\"\n",
    "                if \"mailto:\" in href:\n",
    "                    email = href.replace(\"mailto:\", \"\").split(\"?\")[0].strip()\n",
    "                    if is_valid_email(email):\n",
    "                        return email\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "        # Method 3: Any mailto\n",
    "        try:\n",
    "            mailto_links = driver.find_elements(By.CSS_SELECTOR, \"a[href*='mailto:']\")\n",
    "            for link in mailto_links:\n",
    "                href = link.get_attribute(\"href\") or \"\"\n",
    "                if \"mailto:\" in href:\n",
    "                    email = href.replace(\"mailto:\", \"\").split(\"?\")[0].strip()\n",
    "                    if is_valid_email(email):\n",
    "                        return email\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "        # Method 4: BeautifulSoup\n",
    "        soup = BeautifulSoup(page_source, \"html.parser\")\n",
    "        for link in soup.find_all(\"a\", href=True):\n",
    "            href = link[\"href\"]\n",
    "            if \"mailto:\" in href:\n",
    "                email = href.replace(\"mailto:\", \"\").split(\"?\")[0].strip()\n",
    "                if is_valid_email(email):\n",
    "                    return email\n",
    "\n",
    "        # Method 5: Regex\n",
    "        email_matches = re.findall(r'[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\\.[a-zA-Z]{2,}', page_source)\n",
    "        for email in email_matches:\n",
    "            if is_valid_email(email):\n",
    "                return email\n",
    "\n",
    "        # Fallback: Company website\n",
    "        if website_url:\n",
    "            print(\" [trying website]\", end=\"\")\n",
    "            email = extract_email_from_website(driver, website_url)\n",
    "            if email:\n",
    "                return email\n",
    "\n",
    "    except Exception as e:\n",
    "        if DEBUG:\n",
    "            print(f\" [error: {e}]\", end=\"\")\n",
    "\n",
    "    return \"\"\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "#                           LISTING PARSING\n",
    "# ============================================================================\n",
    "\n",
    "def parse_listing(listing, niche_label):\n",
    "    try:\n",
    "        name_el = listing.select_one(\".business-name span\")\n",
    "        if not name_el:\n",
    "            name_el = listing.select_one(\".business-name\")\n",
    "        company = name_el.text.strip() if name_el else \"\"\n",
    "\n",
    "        if not company:\n",
    "            return None\n",
    "\n",
    "        phone_el = listing.select_one(\".phones\")\n",
    "        phone = phone_el.text.strip() if phone_el else \"\"\n",
    "\n",
    "        street = listing.select_one(\".street-address\")\n",
    "        locality = listing.select_one(\".locality\")\n",
    "        address = \" \".join(filter(None, [\n",
    "            street.text.strip() if street else \"\",\n",
    "            locality.text.strip() if locality else \"\"\n",
    "        ]))\n",
    "\n",
    "        website_el = listing.select_one(\".track-visit-website\")\n",
    "        website = website_el[\"href\"] if website_el else \"\"\n",
    "\n",
    "        detail_el = listing.select_one(\".business-name\")\n",
    "        detail_link = \"\"\n",
    "        if detail_el and detail_el.get(\"href\"):\n",
    "            detail_link = \"https://www.yellowpages.com\" + detail_el[\"href\"]\n",
    "\n",
    "        categories_el = listing.select_one(\".categories\")\n",
    "        categories = categories_el.text.strip() if categories_el else \"\"\n",
    "\n",
    "        # Check if they have a website (important for qualifying leads)\n",
    "        has_website = \"Yes\" if website else \"No\"\n",
    "\n",
    "        return {\n",
    "            \"#\": None,\n",
    "            \"Company Name\": company,\n",
    "            \"Niche\": niche_label,\n",
    "            \"Category\": categories,\n",
    "            \"Has Website\": has_website,\n",
    "            \"Contact Name\": \"\",\n",
    "            \"Email Address\": \"\",\n",
    "            \"Phone Number\": phone,\n",
    "            \"Website URL\": website,\n",
    "            \"Address\": address,\n",
    "            \"Date Added\": datetime.now().strftime(\"%m/%d/%y\"),\n",
    "            \"Date Contacted\": \"\",\n",
    "            \"Source\": detail_link,\n",
    "            \"Notes\": \"\",\n",
    "            \"Status\": \"\",\n",
    "            \"_lead_id\": generate_lead_id(company, phone)\n",
    "        }\n",
    "    except Exception as e:\n",
    "        if DEBUG:\n",
    "            print(f\"  Parse error: {e}\")\n",
    "        return None\n",
    "\n",
    "\n",
    "def get_listings_from_page(driver, niche_label):\n",
    "    driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "    time.sleep(2)\n",
    "\n",
    "    try:\n",
    "        WebDriverWait(driver, 10).until(\n",
    "            EC.presence_of_element_located((By.CSS_SELECTOR, \".result, .search-results\"))\n",
    "        )\n",
    "    except:\n",
    "        return []\n",
    "\n",
    "    listings = driver.find_elements(By.CSS_SELECTOR, \".result\")\n",
    "    page_data = []\n",
    "\n",
    "    for listing in listings:\n",
    "        try:\n",
    "            html = listing.get_attribute(\"outerHTML\")\n",
    "            soup = BeautifulSoup(html, \"html.parser\")\n",
    "            parsed = parse_listing(soup, niche_label)\n",
    "            if parsed:\n",
    "                page_data.append(parsed)\n",
    "        except:\n",
    "            continue\n",
    "\n",
    "    return page_data\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "#                           EXCEL OUTPUT\n",
    "# ============================================================================\n",
    "\n",
    "def add_checkboxes(filepath):\n",
    "    try:\n",
    "        wb = load_workbook(filepath)\n",
    "        ws = wb.active\n",
    "\n",
    "        # Status dropdown\n",
    "        status_validation = DataValidation(\n",
    "            type=\"list\",\n",
    "            formula1='\"Not Contacted,Contacted,Interested,Not Interested,Closed Won,Closed Lost\"',\n",
    "            allow_blank=True\n",
    "        )\n",
    "        ws.add_data_validation(status_validation)\n",
    "\n",
    "        headers = {cell.value: cell.column for cell in ws[1]}\n",
    "\n",
    "        if \"Status\" in headers:\n",
    "            col_idx = headers[\"Status\"]\n",
    "            for row in range(2, ws.max_row + 1):\n",
    "                cell = ws.cell(row=row, column=col_idx)\n",
    "                if not cell.value:\n",
    "                    cell.value = \"Not Contacted\"\n",
    "                status_validation.add(cell)\n",
    "\n",
    "        wb.save(filepath)\n",
    "    except Exception as e:\n",
    "        print(f\"  Warning: Could not add dropdowns: {e}\")\n",
    "\n",
    "\n",
    "def save_leads_to_excel(leads, filepath):\n",
    "    if not leads:\n",
    "        return\n",
    "\n",
    "    clean_leads = []\n",
    "    for lead in leads:\n",
    "        clean_lead = {k: v for k, v in lead.items() if not k.startswith(\"_\")}\n",
    "        clean_leads.append(clean_lead)\n",
    "\n",
    "    for i, lead in enumerate(clean_leads, 1):\n",
    "        lead[\"#\"] = i\n",
    "\n",
    "    df = pd.DataFrame(clean_leads)\n",
    "    df.to_excel(filepath, index=False)\n",
    "    add_checkboxes(filepath)\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "#                           MAIN SCRAPER\n",
    "# ============================================================================\n",
    "\n",
    "def scrape_search(niche_key, search_term, niche_label, location, existing_ids=None):\n",
    "    existing_ids = existing_ids or set()\n",
    "    base_url = f\"https://www.yellowpages.com/{location}/{search_term}\"\n",
    "    output_file = get_output_filename(niche_key, search_term, location)\n",
    "\n",
    "    # Load existing\n",
    "    existing_file_leads = []\n",
    "    if os.path.exists(output_file):\n",
    "        try:\n",
    "            df = pd.read_excel(output_file)\n",
    "            existing_file_leads = df.to_dict('records')\n",
    "            for lead in existing_file_leads:\n",
    "                lead[\"_lead_id\"] = generate_lead_id(\n",
    "                    str(lead.get(\"Company Name\", \"\")),\n",
    "                    str(lead.get(\"Phone Number\", \"\"))\n",
    "                )\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"SCRAPING: {niche_label}\")\n",
    "    print(f\"Search Term: {search_term}\")\n",
    "    print(f\"Location: {location}\")\n",
    "    print(f\"URL: {base_url}\")\n",
    "    print(f\"Existing in file: {len(existing_file_leads)}\")\n",
    "    print(f\"{'='*70}\\n\")\n",
    "\n",
    "    driver = create_driver()\n",
    "    all_leads = list(existing_file_leads)\n",
    "    local_ids = {lead[\"_lead_id\"] for lead in all_leads}\n",
    "    new_leads_count = 0\n",
    "    blocked_count = 0\n",
    "\n",
    "    try:\n",
    "        for page in range(START_PAGE, MAX_PAGES + 1):\n",
    "            url = base_url if page == 1 else f\"{base_url}?page={page}\"\n",
    "\n",
    "            print(f\"[Page {page}] Loading...\")\n",
    "\n",
    "            for attempt in range(MAX_RETRIES):\n",
    "                try:\n",
    "                    driver.get(url)\n",
    "                    time.sleep(2)\n",
    "                    break\n",
    "                except Exception as e:\n",
    "                    if attempt < MAX_RETRIES - 1:\n",
    "                        print(f\"  Retry {attempt + 1}...\")\n",
    "                        time.sleep(5)\n",
    "                    else:\n",
    "                        print(f\"  Failed: {e}\")\n",
    "                        continue\n",
    "\n",
    "            page_listings = get_listings_from_page(driver, niche_label)\n",
    "\n",
    "            if not page_listings:\n",
    "                print(f\"  No listings - end of results\")\n",
    "                break\n",
    "\n",
    "            # Filter dupes\n",
    "            new_listings = []\n",
    "            for listing in page_listings:\n",
    "                lead_id = listing[\"_lead_id\"]\n",
    "                if lead_id not in existing_ids and lead_id not in local_ids:\n",
    "                    new_listings.append(listing)\n",
    "                    local_ids.add(lead_id)\n",
    "\n",
    "            print(f\"  Found {len(page_listings)} listings, {len(new_listings)} new\")\n",
    "\n",
    "            if not new_listings:\n",
    "                print(f\"  All duplicates - skipping\")\n",
    "                if page < MAX_PAGES:\n",
    "                    time.sleep(random.uniform(PAGE_DELAY/2, PAGE_DELAY))\n",
    "                continue\n",
    "\n",
    "            # Fetch emails\n",
    "            if FETCH_EMAILS:\n",
    "                emails_found = 0\n",
    "                for i, lead in enumerate(new_listings):\n",
    "                    company_short = lead['Company Name'][:38].ljust(38)\n",
    "                    print(f\"  [{i+1:2}/{len(new_listings)}] {company_short}\", end=\"\", flush=True)\n",
    "\n",
    "                    email = extract_email_from_detail(\n",
    "                        driver,\n",
    "                        lead[\"Source\"],\n",
    "                        website_url=lead.get(\"Website URL\", \"\"),\n",
    "                        debug_save=(DEBUG and page == 1 and i == 0)\n",
    "                    )\n",
    "\n",
    "                    if email == \"__BLOCKED__\":\n",
    "                        print(f\" -> BLOCKED\")\n",
    "                        blocked_count += 1\n",
    "                        if blocked_count >= 5:\n",
    "                            print(\"\\n  Too many blocks - restarting...\")\n",
    "                            try:\n",
    "                                driver.quit()\n",
    "                            except:\n",
    "                                pass\n",
    "                            time.sleep(10)\n",
    "                            driver = create_driver()\n",
    "                            blocked_count = 0\n",
    "                    elif email:\n",
    "                        lead[\"Email Address\"] = email\n",
    "                        emails_found += 1\n",
    "                        print(f\" -> {email}\")\n",
    "                    else:\n",
    "                        print(f\" -> (no email)\")\n",
    "\n",
    "                print(f\"\\n  Page {page}: {emails_found}/{len(new_listings)} emails\")\n",
    "\n",
    "            all_leads.extend(new_listings)\n",
    "            new_leads_count += len(new_listings)\n",
    "\n",
    "            # Save progress\n",
    "            save_leads_to_excel(all_leads, output_file)\n",
    "            print(f\"  Saved {len(all_leads)} leads to {output_file}\")\n",
    "\n",
    "            if page < MAX_PAGES:\n",
    "                if RESTART_DRIVER_EACH_PAGE:\n",
    "                    print(f\"  Restarting browser...\")\n",
    "                    try:\n",
    "                        driver.quit()\n",
    "                    except:\n",
    "                        pass\n",
    "                    time.sleep(3)\n",
    "                    driver = create_driver()\n",
    "\n",
    "                delay = random.uniform(PAGE_DELAY, PAGE_DELAY + 5)\n",
    "                print(f\"  Waiting {delay:.1f}s...\\n\")\n",
    "                time.sleep(delay)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"\\nError: {e}\")\n",
    "        if all_leads:\n",
    "            save_leads_to_excel(all_leads, output_file)\n",
    "\n",
    "    finally:\n",
    "        try:\n",
    "            driver.quit()\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "    email_count = sum(1 for lead in all_leads if lead.get(\"Email Address\"))\n",
    "    website_count = sum(1 for lead in all_leads if lead.get(\"Has Website\") == \"Yes\")\n",
    "\n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"COMPLETED: {search_term} in {location}\")\n",
    "    print(f\"New leads: {new_leads_count}\")\n",
    "    print(f\"Total in file: {len(all_leads)}\")\n",
    "    print(f\"With emails: {email_count}\")\n",
    "    print(f\"With websites: {website_count}\")\n",
    "    print(f\"{'='*70}\")\n",
    "\n",
    "    return all_leads, new_leads_count\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "#                           RUN MODES\n",
    "# ============================================================================\n",
    "\n",
    "def run_single_search():\n",
    "    \"\"\"Run a single search term + location\"\"\"\n",
    "    ensure_output_dir()\n",
    "\n",
    "    niche = NICHES[CURRENT_NICHE]\n",
    "    term = niche[\"terms\"][CURRENT_TERM_INDEX]\n",
    "    location = LOCATIONS[CURRENT_LOCATION_INDEX]\n",
    "\n",
    "    print(f\"\\nMODE: Single Search\")\n",
    "    print(f\"Niche: {niche['label']}\")\n",
    "    print(f\"Term: {term}\")\n",
    "    print(f\"Location: {location}\")\n",
    "    print(f\"Pitch: {niche['pitch']}\")\n",
    "\n",
    "    existing_ids = load_all_existing_lead_ids()\n",
    "    scrape_search(CURRENT_NICHE, term, niche[\"label\"], location, existing_ids)\n",
    "\n",
    "\n",
    "def run_niche_search():\n",
    "    \"\"\"Run all terms for one niche across all locations\"\"\"\n",
    "    ensure_output_dir()\n",
    "\n",
    "    niche_key = CURRENT_NICHE\n",
    "    niche = NICHES[niche_key]\n",
    "\n",
    "    print(f\"\\nMODE: Full Niche Search\")\n",
    "    print(f\"Niche: {niche['label']}\")\n",
    "    print(f\"Terms: {len(niche['terms'])}\")\n",
    "    print(f\"Locations: {len(LOCATIONS)}\")\n",
    "    print(f\"Total combinations: {len(niche['terms']) * len(LOCATIONS)}\")\n",
    "    print(f\"Pitch: {niche['pitch']}\")\n",
    "\n",
    "    total_new = 0\n",
    "    existing_ids = load_all_existing_lead_ids()\n",
    "    combo = 0\n",
    "    total = len(niche['terms']) * len(LOCATIONS)\n",
    "\n",
    "    for ti, term in enumerate(niche['terms']):\n",
    "        for li, location in enumerate(LOCATIONS):\n",
    "            combo += 1\n",
    "            print(f\"\\n>>> [{combo}/{total}] {term} @ {location}\")\n",
    "            save_progress(niche_key, ti, li)\n",
    "\n",
    "            _, new_count = scrape_search(niche_key, term, niche[\"label\"], location, existing_ids)\n",
    "            total_new += new_count\n",
    "            existing_ids = load_all_existing_lead_ids()\n",
    "\n",
    "            if combo < total:\n",
    "                delay = random.uniform(20, 40)\n",
    "                print(f\"\\nWaiting {delay:.0f}s...\\n\")\n",
    "                time.sleep(delay)\n",
    "\n",
    "    save_progress(niche_key, len(niche['terms'])-1, len(LOCATIONS)-1, \"completed\")\n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"NICHE COMPLETE: {niche['label']}\")\n",
    "    print(f\"Total new leads: {total_new}\")\n",
    "    print(f\"{'='*70}\")\n",
    "\n",
    "\n",
    "def run_all_niches():\n",
    "    \"\"\"Run everything\"\"\"\n",
    "    ensure_output_dir()\n",
    "\n",
    "    total_combos = sum(len(n['terms']) * len(LOCATIONS) for n in NICHES.values())\n",
    "\n",
    "    print(f\"\\nMODE: Full Scrape (all niches)\")\n",
    "    print(f\"Niches: {len(NICHES)}\")\n",
    "    print(f\"Total combinations: {total_combos}\")\n",
    "\n",
    "    total_new = 0\n",
    "    existing_ids = load_all_existing_lead_ids()\n",
    "    combo = 0\n",
    "\n",
    "    for niche_key, niche in NICHES.items():\n",
    "        for ti, term in enumerate(niche['terms']):\n",
    "            for li, location in enumerate(LOCATIONS):\n",
    "                combo += 1\n",
    "                print(f\"\\n>>> [{combo}/{total_combos}] {niche['label']}: {term} @ {location}\")\n",
    "                save_progress(niche_key, ti, li)\n",
    "\n",
    "                _, new_count = scrape_search(niche_key, term, niche[\"label\"], location, existing_ids)\n",
    "                total_new += new_count\n",
    "                existing_ids = load_all_existing_lead_ids()\n",
    "\n",
    "                if combo < total_combos:\n",
    "                    delay = random.uniform(20, 40)\n",
    "                    time.sleep(delay)\n",
    "\n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"ALL NICHES COMPLETE!\")\n",
    "    print(f\"Total new leads: {total_new}\")\n",
    "    print(f\"{'='*70}\")\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "#                           UTILITIES\n",
    "# ============================================================================\n",
    "\n",
    "def merge_all_files():\n",
    "    \"\"\"Merge all files into master list\"\"\"\n",
    "    import glob\n",
    "    ensure_output_dir()\n",
    "\n",
    "    files = glob.glob(os.path.join(OUTPUT_DIR, \"yp_*.xlsx\"))\n",
    "    files = [f for f in files if \"MERGED\" not in f and \"EMAILS\" not in f]\n",
    "\n",
    "    if not files:\n",
    "        print(\"No files to merge!\")\n",
    "        return None\n",
    "\n",
    "    print(f\"Merging {len(files)} files...\")\n",
    "\n",
    "    all_leads = []\n",
    "    for f in files:\n",
    "        try:\n",
    "            df = pd.read_excel(f)\n",
    "            all_leads.extend(df.to_dict('records'))\n",
    "        except Exception as e:\n",
    "            print(f\"  Error reading {f}: {e}\")\n",
    "\n",
    "    if not all_leads:\n",
    "        print(\"No leads found!\")\n",
    "        return None\n",
    "\n",
    "    # Dedupe\n",
    "    seen = set()\n",
    "    unique = []\n",
    "    for lead in all_leads:\n",
    "        key = generate_lead_id(str(lead.get(\"Company Name\", \"\")), str(lead.get(\"Phone Number\", \"\")))\n",
    "        if key not in seen:\n",
    "            seen.add(key)\n",
    "            unique.append(lead)\n",
    "\n",
    "    for i, lead in enumerate(unique, 1):\n",
    "        lead[\"#\"] = i\n",
    "\n",
    "    output_path = os.path.join(OUTPUT_DIR, \"ALL_LEADS_MERGED.xlsx\")\n",
    "    df = pd.DataFrame(unique)\n",
    "    df.to_excel(output_path, index=False)\n",
    "    add_checkboxes(output_path)\n",
    "\n",
    "    email_count = sum(1 for l in unique if l.get(\"Email Address\"))\n",
    "    website_count = sum(1 for l in unique if l.get(\"Has Website\") == \"Yes\")\n",
    "\n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"MERGE COMPLETE!\")\n",
    "    print(f\"Files merged: {len(files)}\")\n",
    "    print(f\"Total unique leads: {len(unique)}\")\n",
    "    print(f\"With emails: {email_count}\")\n",
    "    print(f\"With websites: {website_count}\")\n",
    "    print(f\"Saved to: {output_path}\")\n",
    "    print(f\"{'='*70}\")\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "def export_hot_leads():\n",
    "    \"\"\"Export leads that have BOTH email AND website (hottest leads)\"\"\"\n",
    "    merged_path = os.path.join(OUTPUT_DIR, \"ALL_LEADS_MERGED.xlsx\")\n",
    "\n",
    "    if not os.path.exists(merged_path):\n",
    "        print(\"Run merge_all_files() first!\")\n",
    "        return None\n",
    "\n",
    "    df = pd.read_excel(merged_path)\n",
    "\n",
    "    # Hot leads: have email\n",
    "    hot = df[df[\"Email Address\"].notna() & (df[\"Email Address\"] != \"\")]\n",
    "    hot = hot.copy()\n",
    "    hot[\"#\"] = range(1, len(hot) + 1)\n",
    "\n",
    "    output_path = os.path.join(OUTPUT_DIR, \"HOT_LEADS_WITH_EMAILS.xlsx\")\n",
    "    hot.to_excel(output_path, index=False)\n",
    "    add_checkboxes(output_path)\n",
    "\n",
    "    print(f\"Exported {len(hot)} hot leads (with emails) to: {output_path}\")\n",
    "\n",
    "    # Also export leads WITHOUT websites (need your services most!)\n",
    "    no_website = df[df[\"Has Website\"] == \"No\"]\n",
    "    no_website = no_website.copy()\n",
    "    no_website[\"#\"] = range(1, len(no_website) + 1)\n",
    "\n",
    "    output_path2 = os.path.join(OUTPUT_DIR, \"LEADS_NO_WEBSITE.xlsx\")\n",
    "    no_website.to_excel(output_path2, index=False)\n",
    "    add_checkboxes(output_path2)\n",
    "\n",
    "    print(f\"Exported {len(no_website)} leads WITHOUT websites to: {output_path2}\")\n",
    "\n",
    "    return hot\n",
    "\n",
    "\n",
    "def export_by_niche():\n",
    "    \"\"\"Export by niche type\"\"\"\n",
    "    merged_path = os.path.join(OUTPUT_DIR, \"ALL_LEADS_MERGED.xlsx\")\n",
    "\n",
    "    if not os.path.exists(merged_path):\n",
    "        print(\"Run merge_all_files() first!\")\n",
    "        return\n",
    "\n",
    "    df = pd.read_excel(merged_path)\n",
    "\n",
    "    for niche in df[\"Niche\"].unique():\n",
    "        niche_df = df[df[\"Niche\"] == niche].copy()\n",
    "        niche_df[\"#\"] = range(1, len(niche_df) + 1)\n",
    "\n",
    "        safe_name = niche.replace(\"/\", \"-\").replace(\" \", \"_\").lower()\n",
    "        output_path = os.path.join(OUTPUT_DIR, f\"LEADS_{safe_name}.xlsx\")\n",
    "        niche_df.to_excel(output_path, index=False)\n",
    "        add_checkboxes(output_path)\n",
    "\n",
    "        email_ct = niche_df[\"Email Address\"].notna().sum()\n",
    "        print(f\"  {niche}: {len(niche_df)} leads ({email_ct} with email) -> {output_path}\")\n",
    "\n",
    "\n",
    "def print_stats():\n",
    "    \"\"\"Show statistics\"\"\"\n",
    "    merged_path = os.path.join(OUTPUT_DIR, \"ALL_LEADS_MERGED.xlsx\")\n",
    "\n",
    "    if not os.path.exists(merged_path):\n",
    "        print(\"Run merge_all_files() first!\")\n",
    "        return\n",
    "\n",
    "    df = pd.read_excel(merged_path)\n",
    "\n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(\"LEAD STATISTICS\")\n",
    "    print(f\"{'='*70}\")\n",
    "    print(f\"Total leads: {len(df)}\")\n",
    "    print(f\"With emails: {df['Email Address'].notna().sum()}\")\n",
    "    print(f\"With websites: {(df['Has Website'] == 'Yes').sum()}\")\n",
    "    print(f\"WITHOUT websites: {(df['Has Website'] == 'No').sum()} <- BEST PROSPECTS!\")\n",
    "    print(f\"\\nBy Niche:\")\n",
    "    for niche in df[\"Niche\"].unique():\n",
    "        niche_df = df[df[\"Niche\"] == niche]\n",
    "        emails = niche_df[\"Email Address\"].notna().sum()\n",
    "        no_web = (niche_df[\"Has Website\"] == \"No\").sum()\n",
    "        print(f\"  {niche}: {len(niche_df)} total, {emails} emails, {no_web} no website\")\n",
    "    print(f\"{'='*70}\")\n",
    "\n",
    "\n",
    "def print_niches():\n",
    "    \"\"\"Show available niches and their search terms\"\"\"\n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(\"AVAILABLE NICHES\")\n",
    "    print(f\"{'='*70}\")\n",
    "    for key, niche in NICHES.items():\n",
    "        print(f\"\\n{key}: {niche['label']}\")\n",
    "        print(f\"  Pitch: {niche['pitch']}\")\n",
    "        print(f\"  Terms ({len(niche['terms'])}):\")\n",
    "        for i, term in enumerate(niche['terms']):\n",
    "            print(f\"    [{i}] {term}\")\n",
    "    print(f\"\\n{'='*70}\")\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "#                           MAIN\n",
    "# ============================================================================\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(\"B2B SUPPLIER LEAD SCRAPER\")\n",
    "    print(\"Focused: Janitorial | Safety | Promo | Uniforms\")\n",
    "    print(f\"{'='*70}\")\n",
    "    print(f\"Mode: {MODE}\")\n",
    "    print(f\"Current Niche: {CURRENT_NICHE}\")\n",
    "    print(f\"Output: {OUTPUT_DIR}\")\n",
    "    print(f\"{'='*70}\\n\")\n",
    "\n",
    "    if MODE == \"single\":\n",
    "        run_single_search()\n",
    "    elif MODE == \"niche\":\n",
    "        run_niche_search()\n",
    "    elif MODE == \"all\":\n",
    "        run_all_niches()\n",
    "    else:\n",
    "        print(f\"Unknown mode: {MODE}\")\n",
    "        print(\"Valid: single, niche, all\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03fcba68",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "yp-scraper-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
