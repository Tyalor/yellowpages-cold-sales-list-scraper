{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "291321e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting scrape: https://www.yellowpages.com/brooklyn-ny/general-contractors\n",
      "Max pages: 5 | Fetch emails: True\n",
      "\n",
      "[Page 1] https://www.yellowpages.com/brooklyn-ny/general-contractors\n",
      "  Found 31 listings\n",
      "  Skipping listing: 'NoneType' object has no attribute 'text'\n",
      "  [1/30] SAS Roofing & Waterproofing\n",
      "  [2/30] Total Renovation Contractors\n",
      "  [3/30] City & County Paving Corp\n",
      "  [4/30] Glenwood Construction\n",
      "  [5/30] M. Bhuiyan Construction Company Inc.\n",
      "  [6/30] Alam General Contracting Inc\n",
      "  [7/30] M&K Construction Company\n",
      "  [8/30] Yellow Construction Inc.\n",
      "  [9/30] Rosul Contracting Corporation\n",
      "  [10/30] Miller General Contracting, Ltd.\n",
      "  [11/30] J.K. Construction N.Y. Inc.\n",
      "  [12/30] Big Rose 1 Construction\n",
      "  [13/30] Dynamax Construction Corp\n",
      "  [14/30] Multi Construction Company\n",
      "  [15/30] Green View Construction Inc.\n",
      "  [16/30] N J General Contracting Inc\n",
      "  [17/30] A Howard Construction Inc\n",
      "  [18/30] Donofrio General Contractors Corp\n",
      "  [19/30] Z H N Contracting Corp\n",
      "  [20/30] Cheever Development Corp\n",
      "  [21/30] Bobby's Twins Contracting\n",
      "  [22/30] Volmar Construction Inc\n",
      "  [23/30] DiSalvo Contracting Co. Inc.\n",
      "  [24/30] Hillsborough Construction\n",
      "  [25/30] North East Contraction & MNG Service\n",
      "  [26/30] Plaza Construction Group Inc\n",
      "  [27/30] Alamin Construction Inc\n",
      "  [28/30] United Contracting\n",
      "  [29/30] Apex Contracting & Renovation Inc\n",
      "  [30/30] Dineen Construction Corp\n",
      "[Page 2] https://www.yellowpages.com/brooklyn-ny/general-contractors?page=2\n",
      "  No results found, stopping.\n",
      "\n",
      "==================================================\n",
      "Done! Saved 30 leads to yellow_pages_leads.xlsx\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import re\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "from openpyxl import load_workbook\n",
    "from openpyxl.worksheet.datavalidation import DataValidation\n",
    "\n",
    "# ============== CONFIGURATION ==============\n",
    "SEARCH_TERM = \"general-contractors\"   # e.g., \"general-contractors\", \"real-estate-agents\", \"plumbers\", \"lawyers\"\n",
    "LOCATION = \"brooklyn-ny\"               # e.g., \"brooklyn-ny\", \"queens-ny\", \"manhattan-ny\", \"los-angeles-ca\"\n",
    "INDUSTRY_LABEL = \"Construction\"        # Label for the Industry column in output\n",
    "OUTPUT_FILE = \"yellow_pages_leads.xlsx\"\n",
    "MAX_PAGES = 5                          # Number of pages to scrape (set to None for all pages)\n",
    "FETCH_EMAILS = True                    # Set to False to skip visiting detail pages (faster but no emails)\n",
    "# ===========================================\n",
    "\n",
    "\n",
    "def create_driver():\n",
    "    options = Options()\n",
    "    options.add_argument(\"--headless\")\n",
    "    options.add_argument(\"--no-sandbox\")\n",
    "    options.add_argument(\"--disable-dev-shm-usage\")\n",
    "    options.add_argument(\"--disable-gpu\")\n",
    "    options.add_argument(\"--window-size=1920,1080\")\n",
    "    options.add_argument(\"user-agent=Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36\")\n",
    "    return webdriver.Chrome(service=Service(ChromeDriverManager().install()), options=options)\n",
    "\n",
    "\n",
    "def extract_email_from_detail(driver, detail_url):\n",
    "    try:\n",
    "        driver.get(detail_url)\n",
    "        time.sleep(1.5)\n",
    "        \n",
    "        # Try clicking \"Email Business\" button first to reveal email\n",
    "        try:\n",
    "            email_btn = driver.find_element(By.CSS_SELECTOR, \"a.email-business\")\n",
    "            href = email_btn.get_attribute(\"href\")\n",
    "            if href and href.startswith(\"mailto:\"):\n",
    "                return href.replace(\"mailto:\", \"\").split(\"?\")[0]\n",
    "        except:\n",
    "            pass\n",
    "        \n",
    "        soup = BeautifulSoup(driver.page_source, \"html.parser\")\n",
    "        \n",
    "        # Method 1: Look for email-business class link\n",
    "        email_link = soup.select_one(\"a.email-business[href^='mailto:']\")\n",
    "        if email_link:\n",
    "            return email_link[\"href\"].replace(\"mailto:\", \"\").split(\"?\")[0]\n",
    "        \n",
    "        # Method 2: Any mailto link\n",
    "        email_link = soup.select_one(\"a[href^='mailto:']\")\n",
    "        if email_link:\n",
    "            return email_link[\"href\"].replace(\"mailto:\", \"\").split(\"?\")[0]\n",
    "        \n",
    "        # Method 3: Check all dd elements for email pattern\n",
    "        for dd in soup.select(\"dd\"):\n",
    "            text = dd.get_text()\n",
    "            match = re.search(r'[\\w\\.-]+@[\\w\\.-]+\\.\\w+', text)\n",
    "            if match:\n",
    "                return match.group()\n",
    "        \n",
    "        # Method 4: Search entire page\n",
    "        page_text = soup.get_text()\n",
    "        match = re.search(r'[\\w\\.-]+@[\\w\\.-]+\\.\\w+', page_text)\n",
    "        if match:\n",
    "            return match.group()\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"    Error: {e}\")\n",
    "    return \"\"\n",
    "\n",
    "\n",
    "def parse_listing(listing):\n",
    "    try:\n",
    "        company = listing.select_one(\".business-name span\").text.strip()\n",
    "        phone = listing.select_one(\".phones\").text.strip() if listing.select_one(\".phones\") else \"\"\n",
    "        \n",
    "        address_street = listing.select_one(\".street-address\")\n",
    "        address_locality = listing.select_one(\".locality\")\n",
    "        full_address = \" \".join(filter(None, [\n",
    "            address_street.text.strip() if address_street else \"\",\n",
    "            address_locality.text.strip() if address_locality else \"\"\n",
    "        ]))\n",
    "        \n",
    "        website_el = listing.select_one(\".track-visit-website\")\n",
    "        website = website_el[\"href\"] if website_el else \"\"\n",
    "        \n",
    "        detail_link = \"https://www.yellowpages.com\" + listing.select_one(\".business-name\")[\"href\"]\n",
    "\n",
    "        return {\n",
    "            \"#\": None,\n",
    "            \"Company Name\": company,\n",
    "            \"Industry\": INDUSTRY_LABEL,\n",
    "            \"Contact Name\": \"\",\n",
    "            \"Email Address\": \"\",\n",
    "            \"Phone Number\": phone,\n",
    "            \"Website URL\": website,\n",
    "            \"Address\": full_address,\n",
    "            \"Date Added\": datetime.now().strftime(\"%-m/%-d/%y\"),\n",
    "            \"Date Contacted\": \"\",\n",
    "            \"Source\": detail_link,\n",
    "            \"Notes\": \"\",\n",
    "            \"Called\": \"\",\n",
    "            \"Followed Up\": \"\",\n",
    "            \"Closed\": \"\"\n",
    "        }\n",
    "    except Exception as e:\n",
    "        print(f\"  Skipping listing: {e}\")\n",
    "        return None\n",
    "\n",
    "\n",
    "def add_checkboxes(filepath):\n",
    "    wb = load_workbook(filepath)\n",
    "    ws = wb.active\n",
    "    \n",
    "    checkbox_validation = DataValidation(type=\"list\", formula1='\"☐,☑\"', allow_blank=True)\n",
    "    ws.add_data_validation(checkbox_validation)\n",
    "    \n",
    "    headers = {cell.value: cell.column for cell in ws[1]}\n",
    "    \n",
    "    for col_name in [\"Called\", \"Followed Up\", \"Closed\"]:\n",
    "        if col_name in headers:\n",
    "            col_idx = headers[col_name]\n",
    "            for row in range(2, ws.max_row + 1):\n",
    "                cell = ws.cell(row=row, column=col_idx)\n",
    "                cell.value = \"☐\"\n",
    "                checkbox_validation.add(cell)\n",
    "    \n",
    "    wb.save(filepath)\n",
    "\n",
    "\n",
    "def scrape_yellowpages():\n",
    "    base_url = f\"https://www.yellowpages.com/{LOCATION}/{SEARCH_TERM}\"\n",
    "    print(f\"Starting scrape: {base_url}\")\n",
    "    print(f\"Max pages: {MAX_PAGES or 'All'} | Fetch emails: {FETCH_EMAILS}\\n\")\n",
    "    \n",
    "    driver = create_driver()\n",
    "    all_data = []\n",
    "    page = 1\n",
    "    \n",
    "    try:\n",
    "        while MAX_PAGES is None or page <= MAX_PAGES:\n",
    "            url = base_url if page == 1 else f\"{base_url}?page={page}\"\n",
    "            print(f\"[Page {page}] {url}\")\n",
    "            \n",
    "            driver.get(url)\n",
    "            driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "            time.sleep(2)\n",
    "\n",
    "            try:\n",
    "                WebDriverWait(driver, 10).until(\n",
    "                    EC.presence_of_element_located((By.CLASS_NAME, \"result\"))\n",
    "                )\n",
    "            except:\n",
    "                print(f\"  No results found, stopping.\")\n",
    "                break\n",
    "\n",
    "            listings = driver.find_elements(By.CLASS_NAME, \"result\")\n",
    "            if not listings:\n",
    "                print(f\"  No listings, stopping.\")\n",
    "                break\n",
    "                \n",
    "            print(f\"  Found {len(listings)} listings\")\n",
    "\n",
    "            # Parse all listings on this page\n",
    "            page_data = []\n",
    "            for listing in listings:\n",
    "                soup = BeautifulSoup(listing.get_attribute(\"outerHTML\"), \"html.parser\")\n",
    "                parsed = parse_listing(soup)\n",
    "                if parsed:\n",
    "                    page_data.append(parsed)\n",
    "            \n",
    "            # Fetch emails from detail pages\n",
    "            if FETCH_EMAILS:\n",
    "                for i, entry in enumerate(page_data):\n",
    "                    print(f\"  [{i+1}/{len(page_data)}] {entry['Company Name'][:40]}\", end=\"\")\n",
    "                    email = extract_email_from_detail(driver, entry[\"Source\"])\n",
    "                    if email:\n",
    "                        entry[\"Email Address\"] = email\n",
    "                        print(f\" → {email}\")\n",
    "                    else:\n",
    "                        print()\n",
    "            \n",
    "            all_data.extend(page_data)\n",
    "            \n",
    "            # Check for next page - just increment and try\n",
    "            page += 1\n",
    "            \n",
    "    finally:\n",
    "        driver.quit()\n",
    "    \n",
    "    if not all_data:\n",
    "        print(\"No data collected!\")\n",
    "        return pd.DataFrame()\n",
    "    \n",
    "    # Add row numbers\n",
    "    for i, row in enumerate(all_data, start=1):\n",
    "        row[\"#\"] = i\n",
    "\n",
    "    # Save to single Excel file\n",
    "    df = pd.DataFrame(all_data)\n",
    "    df.to_excel(OUTPUT_FILE, index=False)\n",
    "    add_checkboxes(OUTPUT_FILE)\n",
    "    \n",
    "    print(f\"\\n{'='*50}\")\n",
    "    print(f\"Done! Saved {len(all_data)} leads to {OUTPUT_FILE}\")\n",
    "    print(f\"{'='*50}\")\n",
    "    \n",
    "    return df\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    scrape_yellowpages()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "565f4154",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "yp-scraper-env (3.13.1)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
