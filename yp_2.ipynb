{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "291321e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping: https://www.yellowpages.com/brooklyn-ny/general-contractors\n",
      "Scraping page 1: https://www.yellowpages.com/brooklyn-ny/general-contractors\n",
      "Found 31 listings on page 1\n",
      "Skipping listing due to parse error: 'NoneType' object has no attribute 'text'\n",
      "  [1/30] Checking for email: SAS Roofing & Waterproofing\n",
      "  [2/30] Checking for email: Total Renovation Contractors\n",
      "  [3/30] Checking for email: City & County Paving Corp\n",
      "  [4/30] Checking for email: Glenwood Construction\n",
      "  [5/30] Checking for email: M. Bhuiyan Construction Company Inc.\n",
      "  [6/30] Checking for email: Alam General Contracting Inc\n",
      "  [7/30] Checking for email: M&K Construction Company\n",
      "  [8/30] Checking for email: Yellow Construction Inc.\n",
      "  [9/30] Checking for email: Rosul Contracting Corporation\n",
      "  [10/30] Checking for email: Miller General Contracting, Ltd.\n",
      "  [11/30] Checking for email: J.K. Construction N.Y. Inc.\n",
      "  [12/30] Checking for email: Big Rose 1 Construction\n",
      "  [13/30] Checking for email: Dynamax Construction Corp\n",
      "  [14/30] Checking for email: Multi Construction Company\n",
      "  [15/30] Checking for email: Green View Construction Inc.\n",
      "  [16/30] Checking for email: N J General Contracting Inc\n",
      "  [17/30] Checking for email: A Howard Construction Inc\n",
      "  [18/30] Checking for email: Donofrio General Contractors Corp\n",
      "  [19/30] Checking for email: Z H N Contracting Corp\n",
      "  [20/30] Checking for email: Cheever Development Corp\n",
      "  [21/30] Checking for email: Bobby's Twins Contracting\n",
      "  [22/30] Checking for email: Volmar Construction Inc\n",
      "  [23/30] Checking for email: DiSalvo Contracting Co. Inc.\n",
      "  [24/30] Checking for email: Hillsborough Construction\n",
      "  [25/30] Checking for email: North East Contraction & MNG Service\n",
      "  [26/30] Checking for email: Plaza Construction Group Inc\n",
      "  [27/30] Checking for email: Alamin Construction Inc\n",
      "  [28/30] Checking for email: United Contracting\n",
      "  [29/30] Checking for email: Apex Contracting & Renovation Inc\n",
      "  [30/30] Checking for email: Dineen Construction Corp\n",
      "No next page link found, finished pagination\n",
      "\n",
      "Saved 30 leads to yellow_pages_leads.xlsx\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "from openpyxl import load_workbook\n",
    "from openpyxl.worksheet.datavalidation import DataValidation\n",
    "import re\n",
    "\n",
    "# ============== CONFIGURATION ==============\n",
    "SEARCH_TERM = \"general-contractors\"  # e.g., \"general-contractors\", \"real-estate-agents\", \"plumbers\"\n",
    "LOCATION = \"brooklyn-ny\"              # e.g., \"brooklyn-ny\", \"queens-ny\", \"manhattan-ny\"\n",
    "INDUSTRY_LABEL = \"Construction\"       # Label for the Industry column in output\n",
    "OUTPUT_FILE = \"yellow_pages_leads.xlsx\"\n",
    "MAX_PAGES = 5                         # Number of pages to scrape (set to None for all pages)\n",
    "# ===========================================\n",
    "\n",
    "def extract_email_from_detail(driver, detail_url):\n",
    "    \"\"\"Visit detail page and extract email if available\"\"\"\n",
    "    try:\n",
    "        driver.get(detail_url)\n",
    "        time.sleep(1)\n",
    "        soup = BeautifulSoup(driver.page_source, \"html.parser\")\n",
    "        \n",
    "        # Look for email in various places\n",
    "        email_link = soup.select_one(\"a.email-business\")\n",
    "        if email_link and email_link.get(\"href\", \"\").startswith(\"mailto:\"):\n",
    "            return email_link[\"href\"].replace(\"mailto:\", \"\").split(\"?\")[0]\n",
    "        \n",
    "        # Check Brands field or other sections for email\n",
    "        brands_section = soup.find(\"dt\", string=re.compile(\"Brands|Email\", re.I))\n",
    "        if brands_section:\n",
    "            dd = brands_section.find_next_sibling(\"dd\")\n",
    "            if dd:\n",
    "                text = dd.get_text()\n",
    "                email_match = re.search(r'[\\w\\.-]+@[\\w\\.-]+\\.\\w+', text)\n",
    "                if email_match:\n",
    "                    return email_match.group()\n",
    "        \n",
    "        # Search entire page for email pattern\n",
    "        page_text = soup.get_text()\n",
    "        email_match = re.search(r'[\\w\\.-]+@[\\w\\.-]+\\.\\w+', page_text)\n",
    "        if email_match:\n",
    "            return email_match.group()\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"Error extracting email from {detail_url}: {e}\")\n",
    "    return \"\"\n",
    "\n",
    "def parse_listing(listing):\n",
    "    try:\n",
    "        company = listing.select_one(\".business-name span\").text.strip()\n",
    "        phone = listing.select_one(\".phones\").text.strip() if listing.select_one(\".phones\") else \"\"\n",
    "        address_street = listing.select_one(\".street-address\")\n",
    "        address_locality = listing.select_one(\".locality\")\n",
    "        full_address = \" \".join(filter(None, [\n",
    "            address_street.text.strip() if address_street else \"\",\n",
    "            address_locality.text.strip() if address_locality else \"\"\n",
    "        ]))\n",
    "        website = listing.select_one(\".track-visit-website\")[\"href\"] if listing.select_one(\".track-visit-website\") else \"N/A\"\n",
    "        detail_link = \"https://www.yellowpages.com\" + listing.select_one(\".business-name\")[\"href\"]\n",
    "\n",
    "        return {\n",
    "            \"#\": None,\n",
    "            \"Company Name\": company,\n",
    "            \"Industry\": INDUSTRY_LABEL,\n",
    "            \"Contact Name\": \"\",\n",
    "            \"Email Address\": \"\",\n",
    "            \"Phone Number\": phone,\n",
    "            \"Website URL\": website,\n",
    "            \"Address\": full_address,\n",
    "            \"Date Added\": datetime.now().strftime(\"%-m/%-d/%y\"),\n",
    "            \"Date Contacted\": \"\",\n",
    "            \"Source\": detail_link,\n",
    "            \"Notes\": \"\",\n",
    "            \"Called\": \"\",\n",
    "            \"Followed Up\": \"\",\n",
    "            \"Closed\": \"\"\n",
    "        }\n",
    "    except Exception as e:\n",
    "        print(\"Skipping listing due to parse error:\", e)\n",
    "        return None\n",
    "\n",
    "def add_checkboxes(filepath):\n",
    "    wb = load_workbook(filepath)\n",
    "    ws = wb.active\n",
    "    \n",
    "    checkbox_validation = DataValidation(type=\"list\", formula1='\"☐,☑\"', allow_blank=True)\n",
    "    ws.add_data_validation(checkbox_validation)\n",
    "    \n",
    "    # Find checkbox columns (Called, Followed Up, Closed)\n",
    "    headers = {cell.value: cell.column for cell in ws[1]}\n",
    "    checkbox_cols = [\"Called\", \"Followed Up\", \"Closed\"]\n",
    "    \n",
    "    for col_name in checkbox_cols:\n",
    "        if col_name in headers:\n",
    "            col_idx = headers[col_name]\n",
    "            col_letter = ws.cell(row=1, column=col_idx).column_letter\n",
    "            for row in range(2, ws.max_row + 1):\n",
    "                cell = ws.cell(row=row, column=col_idx)\n",
    "                cell.value = \"☐\"\n",
    "                checkbox_validation.add(cell)\n",
    "    \n",
    "    wb.save(filepath)\n",
    "\n",
    "def scrape_yellowpages_rendered(base_url):\n",
    "    options = Options()\n",
    "    options.add_argument(\"--headless\")\n",
    "    options.add_argument(\"--no-sandbox\")\n",
    "    options.add_argument(\"--disable-dev-shm-usage\")\n",
    "    options.add_argument(\"user-agent=Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36\")\n",
    "\n",
    "    driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()), options=options)\n",
    "    \n",
    "    all_data = []\n",
    "    page = 1\n",
    "    \n",
    "    while True:\n",
    "        if MAX_PAGES and page > MAX_PAGES:\n",
    "            break\n",
    "            \n",
    "        url = base_url if page == 1 else f\"{base_url}?page={page}\"\n",
    "        print(f\"Scraping page {page}: {url}\")\n",
    "        \n",
    "        driver.get(url)\n",
    "        driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "        time.sleep(2)\n",
    "\n",
    "        try:\n",
    "            WebDriverWait(driver, 15).until(\n",
    "                EC.presence_of_element_located((By.CLASS_NAME, \"result\"))\n",
    "            )\n",
    "        except:\n",
    "            print(f\"No more results found on page {page}\")\n",
    "            break\n",
    "\n",
    "        listings = driver.find_elements(By.CLASS_NAME, \"result\")\n",
    "        if not listings:\n",
    "            print(\"No listings found, stopping pagination\")\n",
    "            break\n",
    "            \n",
    "        print(f\"Found {len(listings)} listings on page {page}\")\n",
    "\n",
    "        # First pass: collect basic data and detail URLs\n",
    "        page_data = []\n",
    "        for listing in listings:\n",
    "            soup = BeautifulSoup(listing.get_attribute(\"outerHTML\"), \"html.parser\")\n",
    "            parsed = parse_listing(soup)\n",
    "            if parsed:\n",
    "                page_data.append(parsed)\n",
    "        \n",
    "        # Second pass: visit detail pages for emails\n",
    "        for i, entry in enumerate(page_data):\n",
    "            detail_url = entry[\"Source\"]\n",
    "            print(f\"  [{i+1}/{len(page_data)}] Checking for email: {entry['Company Name']}\")\n",
    "            email = extract_email_from_detail(driver, detail_url)\n",
    "            if email:\n",
    "                entry[\"Email Address\"] = email\n",
    "                print(f\"    Found email: {email}\")\n",
    "        \n",
    "        all_data.extend(page_data)\n",
    "        \n",
    "        # Check for next page\n",
    "        driver.get(url)\n",
    "        time.sleep(1)\n",
    "        soup = BeautifulSoup(driver.page_source, \"html.parser\")\n",
    "        next_link = soup.select_one(\"a.next\")\n",
    "        if not next_link:\n",
    "            print(\"No next page link found, finished pagination\")\n",
    "            break\n",
    "        \n",
    "        page += 1\n",
    "    \n",
    "    driver.quit()\n",
    "    \n",
    "    # Add row numbers\n",
    "    for i, row in enumerate(all_data, start=1):\n",
    "        row[\"#\"] = i\n",
    "\n",
    "    df = pd.DataFrame(all_data)\n",
    "    output_file = OUTPUT_FILE\n",
    "    df.to_excel(output_file, index=False)\n",
    "    \n",
    "    # Add checkbox validation\n",
    "    add_checkboxes(output_file)\n",
    "    \n",
    "    print(f\"\\nSaved {len(all_data)} leads to {output_file}\")\n",
    "    return df\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    url = f\"https://www.yellowpages.com/{LOCATION}/{SEARCH_TERM}\"\n",
    "    print(f\"Scraping: {url}\")\n",
    "    scrape_yellowpages_rendered(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "565f4154",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "yp-scraper-env (3.13.1)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
